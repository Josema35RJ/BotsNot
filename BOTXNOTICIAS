import collections
from email.utils import parsedate_to_datetime
import logging
from typing import Dict, Optional
import colorlog
from cachetools import TTLCache
from logging import handlers
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from functools import partial
import feedparser
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import tweepy
import sqlite3
import hashlib
from datetime import datetime, timedelta
import os
from dotenv import load_dotenv
import asyncio
from concurrent.futures import ThreadPoolExecutor
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.triggers.cron import CronTrigger
from PIL import Image
import io
import aiofiles
import discord
from discord.ext import commands
import re
from collections import deque, defaultdict
import aiohttp
import time
import sys
from transformers import MarianTokenizer, MarianMTModel
import torch
from feedgen.feed import FeedGenerator
import urllib
import json

# Configuración de eventos para Windows
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# Cargar variables de entorno
load_dotenv(dotenv_path="datos.env")

queue = []  # Cola global para reintentos

# Configuración centralizada
CONFIG = {
    "API_LIMITS": {
        "MONTHLY_READS": 10000,
        "REQUESTS_PER_WINDOW": 180, # Mantener este valor, Tweepy lo usa internamente
        "RATE_LIMIT_WINDOW_SECONDS": 900,  # Mantener este valor (15 minutos)
        "TWEETS_PER_DAY": 80, # Reducir el límite diario para no agotar el mensual rápido
        "MONTHLY_POSTS_TOTAL": 1200,  # Reducir el límite mensual total (1500 es el máximo, mejor dejar margen)
    },
    "INTERVALS": {
        "PROCESS_NEWS_MINUTES": 60,  # Procesar noticias de RSS/Discord cada hora (no usa API de X para leer)
        "CHECK_HEALTH_MINUTES": 60, # Mantener la verificación de salud
        # *** MODIFICACIÓN SUGERIDA 1: Aumentar este intervalo ***
        # Aumenta significativamente el tiempo entre actualizaciones de feeds de Twitter/X.
        # Prueba con 1440 (24 horas) o incluso más si sigues teniendo problemas.
        "UPDATE_TWITTER_FEEDS_MINUTES": 2880, # Ejemplo: Cambiado de 720 a 1440 (24 horas)
        "HEARTBEAT_SECONDS": 300, # Mantener el heartbeat
        "TWEET_COOLDOWN_SECONDS": 60,  # 45 minutos entre tweets (ayuda a espaciar publicaciones)
        "DISCORD_MESSAGE_COOLDOWN_SECONDS": 10, # Mantener el cooldown de mensajes de Discord
        "DISCORD_PROCESSING_SECONDS": 30,  # Mantener el procesamiento frecuente de Discord
        # *** MODIFICACIÓN SUGERIDA 2: Aumentar este retardo ***
        # Incrementa la pausa base entre procesar diferentes cuentas de Twitter en update_twitter_feeds.
        # Prueba con 120 (2 minutos) o más.
        "API_REQUEST_DELAY_SECONDS": 300, # Ejemplo: Cambiado de 90 a 120
        "QUEUE_PROCESSING_MINUTES": 60,  # Procesar cola de publicación cada 20 minutos (ajustar si es necesario)
    },
    "PATHS": {
        "RSS_CACHE_DIR": "rss_cache",
        "TEMP_IMAGE_DIR": "optimized_images",
        "CACHE_DIR": "cache",
        "DB_FILE": "bot.db",
        "DETAILED_LOG": "bot_detailed.log",
        "ERROR_LOG": "bot_errors.log",
    },
    "TWEET": {
        "MAX_LENGTH": 280,
    },
    "WEATHER": {
        "POST_HOUR": 23,  # Hora de publicación (22:00)
        "MAP_URL": "https://www.eltiempo.es/lluvia",
        "SOURCE_URL": "https://www.eltiempo.es",
        "MAX_IMAGE_SIZE": 5 * 1024 * 1024,
    }
}

# Configuración de logging
handler = colorlog.StreamHandler(stream=sys.stdout)
handler.setFormatter(colorlog.ColoredFormatter(
    '%(log_color)s%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S',
    log_colors={
        'DEBUG': 'cyan',
        'INFO': 'green',
        'WARNING': 'yellow',
        'ERROR': 'red',
        'CRITICAL': 'bold_red',
    }
))
handler.stream.reconfigure(encoding='utf-8')

logging.basicConfig(
    level=logging.INFO,
    handlers=[
        handlers.RotatingFileHandler(CONFIG["PATHS"]["DETAILED_LOG"], maxBytes=512*1024, backupCount=30, encoding='utf-8'),
        handler
    ]
)
error_handler = handlers.RotatingFileHandler(CONFIG["PATHS"]["ERROR_LOG"], maxBytes=512*1024, backupCount=10, encoding='utf-8')
error_handler.setLevel(logging.ERROR)
logging.getLogger().addHandler(error_handler)
logging.getLogger('apscheduler').setLevel(logging.WARNING)

# Variables de entorno para las APIs
TWITTER_API_KEY_EN = os.getenv("TWITTER_API_KEY_EN")
TWITTER_API_SECRET_EN = os.getenv("TWITTER_API_SECRET_EN")
TWITTER_ACCESS_TOKEN_EN = os.getenv("TWITTER_ACCESS_TOKEN_EN")
TWITTER_ACCESS_SECRET_EN = os.getenv("TWITTER_ACCESS_SECRET_EN")
TWITTER_BEARER_TOKEN_EN = os.getenv("TWITTER_BEARER_TOKEN_EN")

TWITTER_API_KEY_ES = os.getenv("TWITTER_API_KEY_ES")
TWITTER_API_SECRET_ES = os.getenv("TWITTER_API_SECRET_ES")
TWITTER_ACCESS_TOKEN_ES = os.getenv("TWITTER_ACCESS_TOKEN_ES")
TWITTER_ACCESS_SECRET_ES = os.getenv("TWITTER_ACCESS_SECRET_ES")

DISCORD_TOKEN = os.getenv("DISCORD_TOKEN")
DISCORD_CHANNEL_1 = int(os.getenv("DISCORD_CHANNEL_1"))
DISCORD_CHANNEL_2 = int(os.getenv("DISCORD_CHANNEL_2"))

duplicate_cache = TTLCache(maxsize=1000, ttl=3600)  # Cachear por 1 hora

# Lista de fuentes RSS
RSS_FEEDS = [
    "https://www.gameinformer.com/rss.xml",
    "https://www.engadget.com/gaming/rss.xml",
    "https://www.gamespot.com/feeds/news/",
    "https://blog.playstation.com/feed/",
    "https://www.engadget.com/tech/rss.xml",
    "https://kotaku.com/rss",
    "https://www.polygon.com/rss/index.xml",
    "https://www.engadget.com/rss.xml",
    "https://www.gematsu.com/feed",
    "https://www.pcgamer.com/rss",
    "https://www.gameranx.com/feed/",
    "https://www.ubisoft.com/en-us/company/newsroom/rss",
    "https://www.steampowered.com/news/feed",
    "https://www.gog.com/news/feed",
]

# Lista de cuentas de Twitter/X
TWITTER_ACCOUNTS = [
    "NoMansSky",
    "Jorge_Most_",
    "ControlMision",
    "RockstarGames",
    "NVIDIAAI",
    "PlayStation",
    "Xbox",
    "NintendoAmerica",
    "Bethesda",
    "Steam",
    "CallofDuty",
    "XboxGamePass",
    "PlayStationES",
]

# Palabras clave en inglés para puntuación y detección de noticias
KEYWORDS_EN = {
    "gaming": {"game": 0.8, "nintendo": 1.0, "playstation": 1.0, "xbox": 1.0, "pc": 0.9, "console": 0.9},
    "tech": {"technology": 0.8, "apple": 1.0, "google": 1.0, "ai": 1.2, "hardware": 0.9, "software": 0.9},
    "news_indicators": ["release", "update", "patch", "announcement", "news", "breaking", "rotation", "free", "available"]
}

# Estructuras de datos
trending_keywords = deque(maxlen=50)
source_usage = defaultdict(int)
image_cache = {}
url_cache = {}
translation_cache = {}
discord_processing_lock = asyncio.Lock()
user_id_cache = {}  # Caché para IDs de usuarios de Twitter
bot_status = {
    "twitter_connected_en": False,
    "twitter_connected_es": False,
    "sqlite_connected": False,
    "discord_connected": False,
    "tasks_running": 0,
    "last_task": "Idle",
    "processed_news": 0,
    "recent_processed_news": 0,
    "posted_tweets_en": 0,
    "posted_tweets_es": 0,
    "errors": 0,
    "uptime": datetime.now(),
    "daily_tweets_total": 0,
    "monthly_posts_en": 0,
    "monthly_posts_es": 0,
    "last_reset": datetime.now().replace(hour=0, minute=0, second=0, microsecond=0),
    "monthly_reset": datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0),
    "last_tweet_time_en": None,
    "last_tweet_time_es": None,
    "x_api_reads_remaining": CONFIG["API_LIMITS"]["MONTHLY_READS"],
    "last_x_api_reset": datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0),
    "api_request_count": 0,
    "api_window_start": datetime.now(),
    "twitter_wait_until": {"read": 0, "write_en": 0, "write_es": 0}  # Tiempos de espera por categoría e idioma
}

# Crear directorios
for dir_path in [CONFIG["PATHS"]["RSS_CACHE_DIR"], CONFIG["PATHS"]["TEMP_IMAGE_DIR"], CONFIG["PATHS"]["CACHE_DIR"]]:
    os.makedirs(dir_path, exist_ok=True)

# Inicializar modelo de traducción local
translation_tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-es")
translation_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-es")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
translation_model.to(device)

# Conexión a Twitter
auth_en = tweepy.OAuthHandler(TWITTER_API_KEY_EN, TWITTER_API_SECRET_EN)
auth_en.set_access_token(TWITTER_ACCESS_TOKEN_EN, TWITTER_ACCESS_SECRET_EN)
twitter_api_en = tweepy.API(auth_en, wait_on_rate_limit=True)
twitter_client_en = tweepy.Client(
    bearer_token=TWITTER_BEARER_TOKEN_EN,
    consumer_key=TWITTER_API_KEY_EN,
    consumer_secret=TWITTER_API_SECRET_EN,
    access_token=TWITTER_ACCESS_TOKEN_EN,
    access_token_secret=TWITTER_ACCESS_SECRET_EN
)

auth_es = tweepy.OAuthHandler(TWITTER_API_KEY_ES, TWITTER_API_SECRET_ES)
auth_es.set_access_token(TWITTER_ACCESS_TOKEN_ES, TWITTER_ACCESS_SECRET_ES)
twitter_api_es = tweepy.API(auth_es, wait_on_rate_limit=True)
twitter_client_es = tweepy.Client(
    consumer_key=TWITTER_API_KEY_ES,
    consumer_secret=TWITTER_API_SECRET_ES,
    access_token=TWITTER_ACCESS_TOKEN_ES,
    access_token_secret=TWITTER_ACCESS_SECRET_ES
)

# Verificar conexión a Twitter
try:
    twitter_client_en.get_me()
    logging.info("Twitter EN conectado exitosamente")
    bot_status["twitter_connected_en"] = True
except Exception as e:
    logging.error(f"Error conectando a Twitter EN: {e}")
    bot_status["twitter_connected_en"] = False

try:
    twitter_client_es.get_me()
    logging.info("Twitter ES conectado exitosamente")
    bot_status["twitter_connected_es"] = True
except Exception as e:
    logging.error(f"Error conectando a Twitter ES: {e}")
    bot_status["twitter_connected_es"] = False

# Semáforos y eventos para control de concurrencia y límites de tasa
read_semaphore = asyncio.Semaphore(3)
write_semaphore_en = asyncio.Semaphore(1)
write_semaphore_es = asyncio.Semaphore(1)
read_rate_limit_event = asyncio.Event()
write_rate_limit_event_en = asyncio.Event()
write_rate_limit_event_es = asyncio.Event()
read_rate_limit_event.set()  # Inicialmente no pausado
write_rate_limit_event_en.set()
write_rate_limit_event_es.set()

def print_section_header(title, color="blue"):
    """Imprime un encabezado de sección con estilo futurista."""
    color_codes = {
        "blue": "34",
        "green": "32",
        "red": "31",
        "cyan": "36",
        "magenta": "35"
    }
    color_code = color_codes.get(color, "34")  # Default a azul si el color no está definido
    print(f"\033[1;{color_code}m{'═' * 60}\033[0m")
    print(f"\033[1;{color_code}m🚀 {title:^56} 🚀\033[0m")
    print(f"\033[1;{color_code}m{'═' * 60}\033[0m")

def print_progress_bar(current, total, label, critical_threshold=80):
    """Imprime una barra de progreso con colores según el porcentaje."""
    percent = (current / total) * 100 if total > 0 else 0
    bar_length = 40
    filled = int(bar_length * percent / 100)
    bar = '█' * filled + '─' * (bar_length - filled)
    color = "32" if percent < critical_threshold else "31"  # Verde si <80%, rojo si ≥80%
    print(f"\033[1m{label:<20}\033[0m \033[{color}m[{bar}] {percent:>5.1f}%\033[0m")

def get_dynamic_update_interval(account=None):
    """
    Calcula un intervalo dinámico para actualizar feeds de Twitter/X basado en lecturas restantes y prioridad de la cuenta.
    
    Args:
        account: Nombre de la cuenta de Twitter/X (opcional).
    
    Returns:
        float: Intervalo en minutos.
    """
    base_interval = CONFIG["INTERVALS"]["UPDATE_TWITTER_FEEDS_MINUTES"]
    remaining_reads = bot_status.get("x_api_reads_remaining", CONFIG["API_LIMITS"]["MONTHLY_READS"])
    total_reads = CONFIG["API_LIMITS"]["MONTHLY_READS"]
    adjustment_factor = max(1, total_reads / max(remaining_reads, 1))
    
    # Conservar lecturas cuando son bajas
    if remaining_reads < 1000:
        adjustment_factor *= 4  # Cuadruplicar intervalo
    elif remaining_reads < 5000:
        adjustment_factor *= 2  # Duplicar intervalo
    
    dynamic_interval = base_interval * adjustment_factor
    max_interval = 720  # Máximo de 12 horas
    
    # Priorizar cuentas importantes
    priority_accounts = [   "NoMansSky",
    "Jorge_Most_",
    "ControlMision",
    "RockstarGames",
    "NVIDIAAI",
    "PlayStation",
    "Xbox",
    "NintendoAmerica",
    "Bethesda",
    "Steam",
    "CallofDuty",
    "XboxGamePass",
    "PlayStationES"]
    if account in priority_accounts and remaining_reads > 1000:
        dynamic_interval *= 0.5  # Reducir intervalo
    
    return min(dynamic_interval, max_interval)


def normalize_text_for_duplicate_check(text: str) -> str:
    """
    Normaliza el texto (título/resumen) para la verificación de duplicados:
    minúsculas, elimina puntuación y espacios extra.
    """
    if not isinstance(text, str):
        return ""
    # Eliminar puntuación y convertir a minúsculas
    text = re.sub(r'[^\w\s]', '', text).lower()
    # Eliminar espacios extra
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Conexión a SQLite3 y persistencia de estado
def connect_sqlite():
    """
    Establece la conexión a la base de datos SQLite.
    Modificada para ser llamada al inicio del script.
    """
    global bot_status, conn, cursor
    try:
        # check_same_thread=False permite acceder desde diferentes hilos,
        # pero requiere que cada hilo use su propio cursor y no comparta conexiones/cursores.
        # Con run_in_executor, cada llamada a la función de DB se ejecuta en un hilo del pool,
        # por lo que es más seguro si cada función de DB obtiene su propia conexión/cursor temporal,
        # o si la conexión global solo se usa DENTRO de las funciones ejecutadas por run_in_executor.
        conn = sqlite3.connect(CONFIG["PATHS"]["DB_FILE"], check_same_thread=False)
        cursor = conn.cursor()

        # Crear tablas si no existen
        cursor.execute('''CREATE TABLE IF NOT EXISTS historial (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            hash TEXT,
                            title TEXT,
                            url TEXT,
                            tweet TEXT,
                            relevance REAL,
                            source TEXT,
                            date TEXT,
                            engagement INTEGER,
                            summary TEXT,
                            language TEXT,
                            link TEXT,
                            UNIQUE(hash, language)
                        )''')
        cursor.execute('''CREATE INDEX IF NOT EXISTS idx_hash ON historial (hash)''')

        cursor.execute("PRAGMA table_info(historial)")
        columns = [column[1] for column in cursor.fetchall()]
        if "language" not in columns:
            logging.info("La columna 'language' no existe en 'historial'. Agregándola...")
            cursor.execute("ALTER TABLE historial ADD COLUMN language TEXT")
            conn.commit()
            logging.info("Columna 'language' agregada exitosamente a 'historial'.")

        cursor.execute('''CREATE TABLE IF NOT EXISTS bot_state (
                            key TEXT PRIMARY KEY,
                            value TEXT
                        )''')

        cursor.execute('''CREATE TABLE IF NOT EXISTS cola_publicacion (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            title TEXT,
                            summary TEXT,
                            url TEXT,
                            image_data BLOB,
                            news_hash TEXT,
                            score REAL,
                            source TEXT,
                            language TEXT,
                            trends TEXT,
                            added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )''')

        conn.commit()
        bot_status["sqlite_connected"] = True
        return conn, cursor
    except Exception as e:
        logging.error(f"Error conectando a SQLite3: {e}", exc_info=True)
        bot_status["sqlite_connected"] = False
        bot_status["errors"] += 1
        raise
    
async def run_db_sync(func, *args):
    """
    Helper para ejecutar funciones síncronas de base de datos en un ThreadPoolExecutor.
    """
    loop = asyncio.get_event_loop()
    # Ejecuta la función síncrona 'func' con argumentos '*args' en un hilo del pool por defecto.
    return await loop.run_in_executor(None, func, *args)

async def save_bot_state():
    """
    Función asíncrona para guardar el estado del bot usando el helper run_db_sync.
    """
    await run_db_sync(save_bot_state_sync, cursor, conn)

def load_bot_state_sync(cursor):
    """
    Carga el estado del bot desde la base de datos (versión síncrona).
    Diseñada para ejecutarse en un hilo separado o al inicio del script.
    """
    global bot_status
    try:
        cursor.execute("SELECT key, value FROM bot_state")
        state = dict(cursor.fetchall())

        bot_status["posted_tweets_en"] = int(state.get("posted_tweets_en", 0))
        bot_status["posted_tweets_es"] = int(state.get("posted_tweets_es", 0))
        bot_status["daily_tweets_total"] = int(state.get("daily_tweets_total", 0))
        bot_status["monthly_posts_en"] = int(state.get("monthly_posts_en", 0))
        bot_status["monthly_posts_es"] = int(state.get("monthly_posts_es", 0))
        bot_status["errors"] = int(state.get("errors", 0))
        bot_status["x_api_reads_remaining"] = int(state.get("x_api_reads_remaining", CONFIG["API_LIMITS"]["MONTHLY_READS"]))
        bot_status["api_request_count"] = int(state.get("api_request_count", 0))

        def parse_date(value, default):
            if not value:
                return default
            try:
                if isinstance(value, (int, float)):
                    return datetime.fromtimestamp(value)
                return datetime.fromisoformat(value)
            except (ValueError, TypeError):
                logging.warning(f"Valor de fecha inválido: {value}. Usando predeterminado.")
                return default

        bot_status["last_reset"] = parse_date(
            state.get("last_reset"),
            datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        )
        bot_status["monthly_reset"] = parse_date(
            state.get("monthly_reset"),
            datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        )
        bot_status["last_x_api_reset"] = parse_date(
            state.get("last_x_api_reset"),
            datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        )
        bot_status["api_window_start"] = parse_date(
            state.get("api_window_start"),
            datetime.now()
        )

        bot_status["last_tweet_time_en"] = parse_date(state.get("last_tweet_time_en"), None) if state.get("last_tweet_time_en") else None
        bot_status["last_tweet_time_es"] = parse_date(state.get("last_tweet_time_es"), None) if state.get("last_tweet_time_es") else None

        bot_status["twitter_wait_until"]["read"] = float(state.get("twitter_read_wait_until", 0))
        bot_status["twitter_wait_until"]["write_en"] = float(state.get("twitter_write_wait_en", 0))
        bot_status["twitter_wait_until"]["write_es"] = float(state.get("twitter_write_wait_es", 0))

        logging.info("Estado del bot cargado correctamente")
    except Exception as e:
        logging.error(f"Error cargando estado del bot: {e}", exc_info=True)
        bot_status["last_reset"] = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        bot_status["monthly_reset"] = datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        bot_status["last_x_api_reset"] = datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        bot_status["api_window_start"] = datetime.now()
        bot_status["last_tweet_time_en"] = None
        bot_status["last_tweet_time_es"] = None

def save_bot_state_sync(cursor, conn):
    """
    Guarda el estado del bot en la base de datos (versión síncrona).
    Diseñada para ejecutarse en un hilo separado.
    """
    global bot_status
    try:
        state = {
            "posted_tweets_en": str(bot_status["posted_tweets_en"]),
            "posted_tweets_es": str(bot_status["posted_tweets_es"]),
            "daily_tweets_total": str(bot_status["daily_tweets_total"]),
            "monthly_posts_en": str(bot_status["monthly_posts_en"]),
            "monthly_posts_es": str(bot_status["monthly_posts_es"]),
            "errors": str(bot_status["errors"]),
            "x_api_reads_remaining": str(bot_status["x_api_reads_remaining"]),
            "api_request_count": str(bot_status["api_request_count"]),
            "last_reset": bot_status["last_reset"].isoformat() if isinstance(bot_status["last_reset"], datetime) else datetime.now().replace(hour=0, minute=0, second=0, microsecond=0).isoformat(),
            "monthly_reset": bot_status["monthly_reset"].isoformat() if isinstance(bot_status["monthly_reset"], datetime) else datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0).isoformat(),
            "last_x_api_reset": bot_status["last_x_api_reset"].isoformat() if isinstance(bot_status["last_x_api_reset"], datetime) else datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0).isoformat(),
            "api_window_start": bot_status["api_window_start"].isoformat() if isinstance(bot_status["api_window_start"], datetime) else datetime.now().isoformat(),
            "last_tweet_time_en": bot_status["last_tweet_time_en"].isoformat() if isinstance(bot_status["last_tweet_time_en"], datetime) else "",
            "last_tweet_time_es": bot_status["last_tweet_time_es"].isoformat() if isinstance(bot_status["last_tweet_time_es"], datetime) else "",
            "twitter_read_wait_until": str(bot_status["twitter_wait_until"]["read"]),
            "twitter_write_wait_en": str(bot_status["twitter_wait_until"]["write_en"]),
            "twitter_write_wait_es": str(bot_status["twitter_wait_until"]["write_es"]),
        }
        for key, value in state.items():
            cursor.execute("INSERT OR REPLACE INTO bot_state (key, value) VALUES (?, ?)", (key, value))
        conn.commit()
        logging.debug("Estado del bot guardado")
    except Exception as e:
        logging.error(f"Error guardando estado del bot: {e}", exc_info=True)

conn, cursor = connect_sqlite()
load_bot_state_sync(cursor)

# Configuración de Discord
intents = discord.Intents.default()
intents.messages = True
intents.message_content = True
discord_bot = commands.Bot(command_prefix="recuperar", intents=intents)
discord_news = {DISCORD_CHANNEL_1: deque(maxlen=200), DISCORD_CHANNEL_2: deque(maxlen=200)}
last_message_time = 0

@discord_bot.event
async def on_ready():
    global bot_status
    logging.info(f"Discord conectado como {discord_bot.user}")
    bot_status["discord_connected"] = True

@discord_bot.event
async def on_message(message):
    """
    Procesa los mensajes de Discord para identificar y añadir noticias.
    Modificada para usar la función asíncrona is_duplicate y await save_bot_state.
    """
    global last_message_time, bot_status, discord_news
    bot_status["last_task"] = f"Procesando mensaje de Discord (canal {message.channel.id})"

    try:
        if message.author == discord_bot.user:
            return

        await discord_bot.process_commands(message)

        if message.channel.id not in [DISCORD_CHANNEL_1, DISCORD_CHANNEL_2]:
            return

        current_time = time.time()
        if current_time - last_message_time < CONFIG["INTERVALS"]["DISCORD_MESSAGE_COOLDOWN_SECONDS"]:
            return
        last_message_time = current_time

        is_from_bot = message.author.bot
        source = f"discord_bot_{message.author.name}" if is_from_bot else f"discord_{message.author.name}"

        urls = re.findall(
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
            message.content
        )

        # Procesar embeds
        if message.embeds:
            for embed in message.embeds:
                title = embed.title or "Sin título"
                summary = embed.description or message.content or "Sin descripción"
                url = embed.url or None

                if not url and embed.description:
                    embed_urls = re.findall(
                        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
                        embed.description
                    )
                    url = embed_urls[0] if embed_urls else None

                if not url:
                    url = f"discord://{message.channel.id}/{message.id}"

                # Usar la función asíncrona is_duplicate
                if await is_duplicate(title, summary, url, source):
                    logging.info(f"Duplicado detectado en embed: {title[:50]}")
                    continue

                image_url = embed.image.url if embed.image else None
                # La validación de imagen se puede hacer más tarde si es necesario para publicación,
                # no es estrictamente necesario bloquear aquí para cada mensaje de Discord.


                news_hash = hashlib.sha256((title + summary + url + source).encode()).hexdigest()
                discord_news[message.channel.id].append({
                    "title": title,
                    "link": url,
                    "summary": summary,
                    "source": source,
                    "date": datetime.now(),
                    "hash": news_hash,
                    "image_url": image_url,
                    "is_discord": True
                })
                logging.info(f"Noticia añadida desde {source}: {title[:50]} - {url}")
        
        # Procesar URLs en texto plano
        elif urls:
            content = " ".join(message.content.lower().split())
            title = message.content[:100] or "Mensaje con enlace"
            summary = message.content

            for url in urls:
                # Usar la función asíncrona is_duplicate
                if await is_duplicate(title, summary, url, source):
                    logging.info(f"Duplicado detectado en URL: {title[:50]}")
                    continue

                # Validación de URL básica asíncrona
                async with aiohttp.ClientSession() as session:
                    try:
                        async with session.head(url, timeout=aiohttp.ClientTimeout(total=5)) as response:
                            if response.status != 200:
                                logging.warning(f"URL no válida para '{title}': {url}")
                                continue
                    except Exception as e:
                        logging.warning(f"Error validando URL para '{title}': {e}")
                        continue

                news_hash = hashlib.sha256((title + summary + url + source).encode()).hexdigest()
                # No hay image_url evidente en texto plano
                image_url = None
                discord_news[message.channel.id].append({
                    "title": title,
                    "link": url,
                    "summary": summary,
                    "source": source,
                    "date": datetime.now(),
                    "hash": news_hash,
                    "image_url": image_url,
                    "is_discord": True
                })
                logging.info(f"Noticia añadida desde {source} (URL): {title[:50]} - {url}")

        # Procesar mensajes que contienen palabras clave indicadoras de noticias sin URL ni embed
        elif any(keyword.lower() in message.content.lower() for keyword in KEYWORDS_EN["news_indicators"]):
            title = message.content[:100]
            summary = message.content
            url = f"discord://{message.channel.id}/{message.id}"

            # Usar la función asíncrona is_duplicate
            if await is_duplicate(title, summary, url, source):
                logging.info(f"Duplicado detectado en texto plano: {title[:50]}")
                return

            news_hash = hashlib.sha256((title + summary + url + source).encode()).hexdigest()
            # Obtener image_url de adjuntos si existen
            image_url = message.attachments[0].url if message.attachments else None
            discord_news[message.channel.id].append({
                "title": title,
                "link": url,
                "summary": summary,
                "source": source,
                "date": datetime.now(),
                "hash": news_hash,
                "image_url": image_url,
                "is_discord": True
            })
            logging.info(f"Noticia de texto añadida desde {source}: {title[:50]}")

    except Exception as e:
        logging.error(f"Error procesando mensaje de Discord (canal {message.channel.id}): {e}", exc_info=True)
        bot_status["errors"] += 1
        await save_bot_state() # Usar la función asíncrona para guardar estado

@discord_bot.command(name="noticias")
async def fetch_news(ctx, number: int):
    global bot_status
    bot_status["last_task"] = f"Ejecutando 'recuperarnoticias' en canal {ctx.channel.id}"
    if number <= 0 or number > 50:
        await ctx.send("❌ El número debe estar entre 1 y 50. Ejemplo: `recuperarnoticias 5`")
        return

    log_message = await ctx.send("📋 **Procesando noticias...**")
    log_content = "📋 **Procesando noticias...**\n"
    await log_message.edit(content=log_content)
    
    try:
        async with aiohttp.ClientSession() as session:
            trends = await get_trending_keywords()
            news_items = []
            
            async for message in ctx.channel.history(limit=100):
                if len(news_items) >= number:
                    break
                
                if message.embeds:
                    for embed in message.embeds:
                        title = embed.title or "Sin título"
                        summary = embed.description or message.content or "Sin descripción"
                        url = embed.url or None
                        image_url = embed.image.url if embed.image else None
                        
                        if not url and embed.description:
                            embed_urls = re.findall(
                                r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
                                embed.description
                            )
                            url = embed_urls[0] if embed_urls else None
                        
                        if not url:
                            url = f"discord://{message.channel.id}/{message.id}"
                        
                        if url:
                            short_url = await shorten_url(url)
                            news_hash = hashlib.sha256((title + summary + short_url + f"discord_channel_{ctx.channel.id}").encode()).hexdigest()
                            news_items.append({
                                "title": title,
                                "link": short_url,
                                "summary": summary,
                                "source": f"discord_channel_{ctx.channel.id}",
                                "date": message.created_at,
                                "hash": news_hash,
                                "image_url": image_url,
                                "is_discord": True
                            })
                
                else:
                    urls = re.findall(
                        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
                        message.content
                    )
                    if urls:
                        for url in urls:
                            if len(news_items) < number:
                                short_url = await shorten_url(url)
                                news_hash = hashlib.sha256((message.content + summary + short_url + f"discord_channel_{ctx.channel.id}").encode()).hexdigest()
                                news_items.append({
                                    "title": message.content[:100],
                                    "link": short_url,
                                    "summary": message.content,
                                    "source": f"discord_channel_{ctx.channel.id}",
                                    "date": message.created_at,
                                    "hash": news_hash,
                                    "image_url": None,
                                    "is_discord": True
                                })
                    elif any(keyword.lower() in message.content.lower() for keyword in KEYWORDS_EN["news_indicators"]):
                        if len(news_items) < number:
                            title = message.content[:100]
                            summary = message.content
                            url = f"discord://{message.channel.id}/{message.id}"
                            image_url = message.attachments[0].url if message.attachments else None
                            news_hash = hashlib.sha256((title + summary + url + f"discord_channel_{ctx.channel.id}").encode()).hexdigest()
                            news_items.append({
                                "title": title,
                                "link": url,
                                "summary": summary,
                                "source": f"discord_channel_{ctx.channel.id}",
                                "date": message.created_at,
                                "hash": news_hash,
                                "image_url": image_url,
                                "is_discord": True
                            })

            processed = 0
            for i, entry in enumerate(news_items, 1):
                log_content += f"\n**Noticia {i}/{number}:** {entry['title'][:50]}...\n"
                log_content += f"**Fuente:** {entry['source']}\n"
                log_content += "Calculando puntuación...\n"
                await log_message.edit(content=log_content)
                
                score = await score_news(entry, trends)
                log_content += f"**Puntuación:** {score:.1f}\n"
                log_content += "🔝 **Prioridad máxima (origen Discord)**\n"
                await log_message.edit(content=log_content)
                
                total_daily_tweets = bot_status["daily_tweets_total"]
                if total_daily_tweets >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
                    log_content += f"❌ **Límite diario alcanzado:** {CONFIG['API_LIMITS']['TWEETS_PER_DAY']} tweets\n"
                    await log_message.edit(content=log_content)
                    break

                languages = ["en", "es"]
                for language in languages:
                    client = twitter_client_en if language == "en" else twitter_client_es
                    api = twitter_api_en if language == "en" else twitter_api_es
                    daily_key = "daily_tweets_total"
                    monthly_key = f"monthly_posts_{language}"
                    posted_key = f"posted_tweets_{language}"
                    last_tweet_key = f"last_tweet_time_{language}"
                    category = f"write_{language}"
                    semaphore = write_semaphore_en if language == "en" else write_semaphore_es
                    rate_limit_event = write_rate_limit_event_en if language == "en" else write_rate_limit_event_es

                    current_time = time.time()
                    if current_time < bot_status["twitter_wait_until"].get(category, 0):
                        log_content += f"⏳ **En espera para {language.upper()} hasta {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%H:%M:%S')}**\n"
                        continue

                    if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
                        log_content += f"❌ **Límite mensual total alcanzado**\n"
                        continue

                    last_tweet_time = bot_status[last_tweet_key]
                    if last_tweet_time and (datetime.now() - last_tweet_time).total_seconds() < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
                        log_content += f"⏳ **Espera requerida para {language}**\n"
                        continue

                    if await is_published_in_language(entry["title"], entry["summary"], entry["link"], entry["source"], language):
                        log_content += f"✅ **Ya publicado en {language.upper()}**\n"
                        continue

                    log_content += f"**Publicando en {language.upper()}...**\n"
                    await log_message.edit(content=log_content)

                    async def make_twitter_request(func, *args, **kwargs):
                        async with semaphore:
                            await rate_limit_event.wait()
                            for attempt in range(5):
                                try:
                                    result = await asyncio.to_thread(func, *args, **kwargs)
                                    return result
                                except tweepy.TweepyException as e:
                                    if e.response and e.response.status_code == 429:
                                        retry_after = int(e.response.headers.get("Retry-After", 900))
                                        reset_time = time.time() + retry_after
                                        bot_status["twitter_wait_until"][category] = reset_time
                                        rate_limit_event.clear()
                                        logging.warning(f"429 detectado en {language}. Pausando hasta {datetime.fromtimestamp(reset_time)}")
                                        await asyncio.sleep(retry_after)
                                        rate_limit_event.set()
                                    else:
                                        logging.error(f"Error en intento {attempt + 1}/5 para {language}: {e}")
                                        if attempt == 4:
                                            raise
                                        await asyncio.sleep(2 ** attempt)
                                except Exception as e:
                                    logging.error(f"Error inesperado en intento {attempt + 1}/5 para {language}: {e}")
                                    if attempt == 4:
                                        raise
                                    await asyncio.sleep(2 ** attempt)

                    try:
                        tweet_match = re.search(r'https://(nitter\.net|twitter\.com|x\.com)/(\w+)/status/(\d+)', entry["link"])
                        if tweet_match:
                            success = await make_twitter_request(
                                repost_tweet_from_url, entry["link"], entry["summary"], language
                            )
                            if success:
                                log_content += f"✅ **Reposteado en {language.upper()}** (ID: {tweet_match.group(3)})\n"
                                bot_status[daily_key] += 1
                                bot_status[monthly_key] += 1
                                bot_status[posted_key] += 1
                                bot_status[last_tweet_key] = datetime.now()
                                processed += 1
                        else:
                            tweet = await generate_detailed_tweet(entry["title"], entry["summary"], entry["link"], trends, language)
                            image_url = entry.get("image_url")
                            if image_url:
                                img_data = await download_image(session, image_url)
                                if img_data:
                                    optimized_img = await optimize_image(img_data)
                                    if optimized_img:
                                        temp_image_path = os.path.join(CONFIG["PATHS"]["TEMP_IMAGE_DIR"], f"{entry['hash']}_{language}.jpg")
                                        async with aiofiles.open(temp_image_path, "wb") as f:
                                            await f.write(optimized_img)
                                        media = await make_twitter_request(api.media_upload, temp_image_path)
                                        tweet_response = await make_twitter_request(
                                            client.create_tweet, text=tweet, media_ids=[media.media_id]
                                        )
                                    else:
                                        tweet_response = await make_twitter_request(client.create_tweet, text=tweet)
                                else:
                                    tweet_response = await make_twitter_request(client.create_tweet, text=tweet)
                            else:
                                tweet_response = await make_twitter_request(client.create_tweet, text=tweet)
                            
                            log_content += f"✅ **Publicado en {language.upper()}** (ID: {tweet_response.data['id']})\n"
                            bot_status[daily_key] += 1
                            bot_status[monthly_key] += 1
                            bot_status[posted_key] += 1
                            bot_status[last_tweet_key] = datetime.now()
                            processed += 1

                        news_hash = hashlib.sha256((entry["title"] + entry["summary"] + entry["link"] + entry["source"]).encode()).hexdigest()
                        cursor.execute('''INSERT OR IGNORE INTO historial (hash, title, url, tweet, relevance, source, date, engagement, summary, language)
                                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                                      (news_hash, entry["title"], entry["link"], tweet, score, entry["source"], datetime.now().isoformat(), 0, entry["summary"], language))
                        conn.commit()
                        save_bot_state_sync(cursor, conn)
                    except tweepy.TweepyException as e:
                        log_content += f"❌ **Error en {language.upper()}:** {str(e)}\n"
                        if e.response and e.response.status_code == 429:
                            log_content += f"⏳ **Límite de tasa, esperando hasta {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%H:%M:%S')}**\n"
                        bot_status["errors"] += 1
                        save_bot_state_sync(cursor, conn)
                    except Exception as e:
                        log_content += f"❌ **Error inesperado en {language.upper()}:** {str(e)}\n"
                        bot_status["errors"] += 1
                        save_bot_state_sync(cursor, conn)

                await log_message.edit(content=log_content)
                await asyncio.sleep(1)

            log_content += f"\n🏁 **Finalizado:** {processed} noticias publicadas."
            await log_message.edit(content=log_content)

            await asyncio.sleep(5)
            await ctx.message.delete()
            await log_message.delete()

        logging.info(f"Comando 'recuperarnoticias' completado: {processed}/{number} noticias procesadas")
        bot_status["processed_news"] += processed
        bot_status["recent_processed_news"] += processed
        save_bot_state_sync(cursor, conn)

    except Exception as e:
        logging.error(f"Error en 'recuperarnoticias' (canal {ctx.channel.id}): {e}", exc_info=True)
        log_content += f"\n❌ **Error general:** {str(e)}"
        await log_message.edit(content=log_content)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        await asyncio.sleep(5)
        await ctx.message.delete()
        await log_message.delete()

executor = ThreadPoolExecutor(max_workers=2)

async def check_api_rate_limit(response_headers=None, category="read", endpoint=None, exception=None):
    current_time_ts = time.time()
    
    logging.debug(f"Encabezados recibidos para {category}: {response_headers}, Tipo: {type(response_headers)}")
    
    if response_headers and isinstance(response_headers, collections.abc.Mapping):
        try:
            logging.debug(f"Procesando encabezados para {category}: {response_headers}")
            # Manejo de Retry-After
            if "Retry-After" in response_headers:
                retry_after = response_headers["Retry-After"]
                try:
                    wait_time = min(int(retry_after) + 5, 3600)  # Máximo 1 hora
                    logging.info(f"Retry-After detectado. Esperando {wait_time}s")
                    return wait_time
                except ValueError:
                    retry_date = parsedate_to_datetime(retry_after)
                    wait_time = min((retry_date.timestamp() - current_time_ts) + 5, 3600) if retry_date else 3600
                    logging.info(f"Retry-After (fecha) detectado. Esperando {wait_time}s")
                    return wait_time
            
            # Manejo de límites de tasa
            remaining = int(response_headers.get("x-rate-limit-remaining", float('inf')))
            reset_ts = int(response_headers.get("x-rate-limit-reset", 0))
            if remaining <= 0 and reset_ts > current_time_ts:
                wait_time = min((reset_ts - current_time_ts) + 5, 10800)  # Máximo 3 horas
                logging.warning(f"Límite alcanzado para {category}. Esperando {wait_time:.1f}s")
                return wait_time
        except Exception as e:
            logging.error(f"Error procesando encabezados: {e}")
    
    # Respaldo según el tipo de límite
    if exception and isinstance(exception, tweepy.TooManyRequests):
        if category.startswith("write"):
            # Suponer límite diario (50 tweets/día en plan gratuito) si no hay más info
            wait_time = 86400  # 24 horas
            logging.warning(f"Posible límite diario alcanzado para {category}. Esperando {wait_time}s por defecto.")
        else:
            wait_time = 900  # 15 minutos para otros límites
            logging.warning(f"No se encontraron encabezados útiles para {category}. Esperando {wait_time}s por defecto.")
        return wait_time
    
    return 0

async def make_twitter_request(func, *args, category="read", semaphore=None, rate_limit_event=None, endpoint=None, **kwargs):
    global bot_status
    semaphore = semaphore or (read_semaphore if category == "read" else (write_semaphore_en if category == "write_en" else write_semaphore_es))
    rate_limit_event = rate_limit_event or (read_rate_limit_event if category == "read" else (write_rate_limit_event_en if category == "write_en" else write_rate_limit_event_es))
    
    async with semaphore:
        for attempt in range(5):
            # Verificar límite antes del intento
            wait_time = await check_api_rate_limit(category=category, endpoint=endpoint)
            if wait_time > 0:
                logging.debug(f"Esperando {wait_time:.1f}s antes del intento {attempt + 1} para {category}")
                await asyncio.sleep(wait_time)
            
            if not rate_limit_event.is_set():
                await rate_limit_event.wait()
            
            try:
                bot_status["api_request_count"] += 1
                result = await asyncio.to_thread(func, *args, **kwargs)
                # Extraer encabezados de la respuesta
                response_headers = getattr(result, 'response', None).headers if hasattr(result, 'response') else None
                logging.debug(f"Encabezados en intento exitoso para {category}: {response_headers}")
                wait_time = await check_api_rate_limit(response_headers=response_headers, category=category, endpoint=endpoint)
                if wait_time > 0:
                    rate_limit_event.clear()
                    logging.debug(f"Pausando {wait_time:.1f}s tras respuesta para {category}")
                    await asyncio.sleep(wait_time)
                    rate_limit_event.set()
                return result
            except tweepy.TooManyRequests as e:
                # Extraer encabezados del error
                response_headers = getattr(e, 'response', None).headers if hasattr(e, 'response') else None
                logging.debug(f"Encabezados en error 429 para {category}: {response_headers}")
                wait_time = await check_api_rate_limit(response_headers=response_headers, category=category, endpoint=endpoint, exception=e)
                if wait_time > 0:
                    rate_limit_event.clear()
                    logging.warning(f"Error 429 en intento {attempt + 1}/5 para {category}: {e}. Esperando {wait_time}s")
                    await asyncio.sleep(wait_time)
                    rate_limit_event.set()
                elif attempt == 4:
                    logging.error(f"Fallo tras 5 intentos por 429 para {category}: {e}")
                    raise
            except Exception as e:
                logging.error(f"Error inesperado en intento {attempt + 1}/5 para {category}: {e}")
                if attempt == 4:
                    raise
                await asyncio.sleep(2 ** attempt)
                
async def generate_rss_feed(username: str, num_tweets: int = 5) -> str | None:
    """
    Genera un feed RSS para un usuario de Twitter/X.
    Utiliza make_twitter_request para manejar límites de API.
    """
    global bot_status
    bot_status["last_task"] = f"Generando feed RSS para @{username}"
    logging.debug(f"Intentando generar feed RSS para @{username}")

    category = "read"
    current_time_ts = time.time()

    # Verificar si las lecturas están pausadas antes de intentar generar el feed
    if current_time_ts < bot_status["twitter_wait_until"].get(category, 0):
        logging.warning(f"Lecturas de Twitter/X pausadas hasta {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%Y-%m-%d %H:%M:%S')}. Omitiendo generación de feed para @{username}")
        return None

    # make_twitter_request ya maneja el límite mensual y por ventana,
    # así que no necesitamos verificaciones explícitas aquí, solo confiar en él.

    try:
        # Obtener ID de usuario (usando caché y make_twitter_request)
        if username in user_id_cache:
            user_id = user_id_cache[username]
            logging.debug(f"Usuario @{username} encontrado en caché con ID: {user_id}")
        else:
            logging.debug(f"Buscando ID de usuario para @{username} via API.")
            user_response = await make_twitter_request(
                twitter_client_en.get_user,
                username=username,
                category=category
            )
            if not user_response or not user_response.data:
                logging.warning(f"No se encontró el usuario @{username} o error al obtenerlo.")
                return None
            user_id = user_response.data.id
            user_id_cache[username] = user_id
            logging.debug(f"ID de usuario para @{username} obtenido: {user_id}")
            save_bot_state_sync(cursor, conn) # Guardar user_id_cache si es necesario persistirlo

        # Obtener tweets del usuario (usando make_twitter_request)
        logging.debug(f"Obteniendo tweets para el usuario {user_id} (@{username}).")
        tweets_response = await make_twitter_request(
            twitter_client_en.get_users_tweets,
            id=user_id,
            max_results=num_tweets,
            tweet_fields=["created_at"], # Solicitar solo los campos necesarios
            category=category
        )

        if not tweets_response or not tweets_response.data:
            logging.info(f"No se encontraron tweets recientes para @{username} o error al obtenerlos.")
            return None

        fg = FeedGenerator()
        fg.title(f"Tweets from @{username}")
        fg.link(href=f"https://x.com/{username}", rel="alternate")
        fg.description(f"Latest tweets from @{username}")

        for tweet in tweets_response.data:
            fe = fg.add_entry()
            fe.title(tweet.text[:100] + "..." if len(tweet.text) > 100 else tweet.text)
            fe.link(href=f"https://x.com/{username}/status/{tweet.id}")
            fe.description(tweet.text)
            # Asegurarse de que created_at es un objeto datetime antes de formatear
            if isinstance(tweet.created_at, datetime):
                 fe.pubDate(tweet.created_at.isoformat())
            else:
                 logging.warning(f"Formato de fecha inesperado para tweet {tweet.id}: {tweet.created_at}")
                 fe.pubDate(datetime.now().isoformat()) # Usar fecha actual como fallback


        rss_file = os.path.join(CONFIG["PATHS"]["RSS_CACHE_DIR"], f"{username}.rss")
        fg.rss_file(rss_file)
        logging.info(f"Feed RSS generado exitosamente para @{username}: {rss_file}")
        return rss_file

    except Exception as e:
        logging.error(f"Error generando RSS para @{username}: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return None

async def update_twitter_feeds():
    """
    Actualiza los feeds de Twitter/X obteniendo los últimos tweets de las cuentas configuradas.
    
    Returns:
        list: Lista de noticias/tweets obtenidos.
    """
    twitter_news = []
    async with aiohttp.ClientSession() as session:
        for account in TWITTER_ACCOUNTS:
            interval = get_dynamic_update_interval(account)
            retry_delay = 1  # Retraso inicial para retroceso exponencial
            for attempt in range(3):
                try:
                    # Verificar existencia de la cuenta
                    user = await make_twitter_request(
                        twitter_client_en.get_user,
                        username=account,
                        category="read",
                        endpoint=f"users/by/username/{account}"
                    )
                    if not user.data:
                        logging.warning(f"Cuenta {account} no encontrada o inaccesible")
                        break
                    
                    tweets = await make_twitter_request(
                        twitter_client_en.get_users_tweets,
                        id=user.data.id,
                        max_results=10,
                        exclude=["retweets", "replies"],
                        category="read",
                        endpoint=f"users/{user.data.id}/tweets"
                    )
                    if tweets.data:
                        for tweet in tweets.data:
                            news_entry = {
                                "title": tweet.text[:50],
                                "summary": tweet.text,
                                "link": f"https://twitter.com/{account}/status/{tweet.id}",
                                "source": f"Twitter: {account}",
                                "is_tweet": True
                            }
                            twitter_news.append(news_entry)
                    break  # Salir si la solicitud es exitosa
                except tweepy.TooManyRequests as e:
                    logging.warning(f"Error 429 obteniendo tweets de {account} en intento {attempt + 1}/3: {e}")
                    if attempt == 2:
                        logging.error(f"No se pudieron obtener tweets de {account} tras 3 intentos por 429")
                        break
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2  # Retroceso exponencial
                except tweepy.TweepyException as e:
                    logging.warning(f"Error obteniendo tweets de {account} en intento {attempt + 1}/3: {e}")
                    if attempt == 2:
                        logging.error(f"No se pudieron obtener tweets de {account} tras 3 intentos")
                        break
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                except Exception as e:
                    logging.error(f"Error inesperado procesando {account}: {e}")
                    break
    return twitter_news

def clean_text(text: str) -> str:
    if not text or not isinstance(text, str):
        return ""
    text = re.sub(r'<.*?>', '', text)  # Eliminar etiquetas HTML
    text = re.sub(r'[^\w\s.,!?\'"-]', '', text)  # Mantener comillas y guiones
    return text.strip()

def post_process_translation(translated_text: str) -> str:
    corrections = {
        "interruptor": "Switch",
        "deja caer": "no incluirá",
        "preordenes": "preventas",
        "carro": "cartucho",
        "añade el": "añade soporte para"
    }
    for wrong, correct in corrections.items():
        translated_text = translated_text.replace(wrong, correct)
    return translated_text

def translate_text(text: str) -> str:
    cleaned_text = clean_text(text)
    if not cleaned_text:
        return text
    if cleaned_text in translation_cache:
        return translation_cache[cleaned_text]
    try:
        inputs = translation_tokenizer(cleaned_text, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
        with torch.inference_mode():
            translated = translation_model.generate(**inputs)
        translated_text = translation_tokenizer.decode(translated[0], skip_special_tokens=True)
        translation_cache[cleaned_text] = translated_text
        return translated_text
    except Exception as e:
        logging.error(f"Error en MarianMT para texto '{cleaned_text[:50]}...': {e}")
        return cleaned_text

async def fetch_url(session: aiohttp.ClientSession, url: str) -> bytes | None:
    """
    Obtiene el contenido binario de una URL.
    """
    global bot_status
    # No actualizamos last_task aquí ya que es una función auxiliar llamada por otras
    try:
        # Aumentar un poco el timeout para descargas de imágenes potencialmente más grandes
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=15)) as response:
            if response.status != 200:
                logging.warning(f"Error obteniendo {url}: Código de estado {response.status}")
                return None
            # Verificar el tipo de contenido para asegurar que es una imagen o HTML (para scraping)
            content_type = response.headers.get("Content-Type", "").lower()
            if not (content_type.startswith("image/") or "text/html" in content_type):
                 logging.warning(f"Contenido no es imagen ni HTML en {url}: {content_type}")
                 return None
            return await response.read()
    except Exception as e:
        # Loguear como debug o warning si es un error común de red, error si es inesperado
        logging.debug(f"Error obteniendo {url}: {e}")
        # No incrementar contador de errores aquí para errores de red comunes, solo para errores lógicos.
        return None

async def get_image_from_url(session: aiohttp.ClientSession, url: str, max_attempts: int = 5) -> str | None:
    """
    Intenta encontrar y validar URLs de imágenes en una página web.
    Prioriza imágenes Open Graph y Twitter Card, y luego busca en etiquetas <img>.
    Intenta validar las URLs encontradas.
    """
    logging.debug(f"Attempting to get image URL from: {url}")
    try:
        # Use fetch_url to get the page content
        html_content_bytes = await fetch_url(session, url)
        if not html_content_bytes:
            logging.debug(f"Could not get content from {url} to search for images.")
            return None

        # Decode HTML content, trying different encodings
        try:
            html_content = html_content_bytes.decode('utf-8')
        except UnicodeDecodeError:
            try:
                html_content = html_content_bytes.decode('latin-1')
            except Exception as e:
                logging.warning(f"Could not decode content from {url}: {e}")
                return None

        soup = BeautifulSoup(html_content, "html.parser")
        image_urls = []

        # Prioritize meta tags (Open Graph and Twitter Card)
        og_image = soup.find("meta", attrs={"property": "og:image"})
        if og_image and og_image.get("content"):
            image_urls.append(og_image.get("content"))
            logging.debug(f"Found Open Graph image: {og_image.get('content')}")

        twitter_image = soup.find("meta", attrs={"name": "twitter:image"})
        if twitter_image and twitter_image.get("content"):
             image_urls.append(twitter_image.get("content"))
             logging.debug(f"Found Twitter Card image: {twitter_image.get('content')}")

        # Search for <img> tags. Try to find URLs that look high-resolution
        img_tags = soup.find_all("img", attrs={"src": True})
        for tag in img_tags:
            img_url = tag.get("src")
            if img_url and img_url.startswith(("http://", "https://")):
                 # Simple heuristic: check for common high-res indicators in URL
                 if any(indicator in img_url.lower() for indicator in ["large", "full", "original"]):
                     image_urls.insert(0, img_url) # Add to the beginning to prioritize
                 else:
                    image_urls.append(img_url)

        # Remove duplicates while preserving order
        image_urls = list(dict.fromkeys(image_urls))

        logging.debug(f"Image URLs found (including meta and img): {image_urls}")

        # Validate the found URLs and return the first valid one
        for img_url in image_urls[:max_attempts]: # Limit the number of validations
            if await validate_image_url(session, img_url):
                logging.debug(f"Validated image URL: {img_url}")
                return img_url
            else:
                logging.debug(f"Image URL not valid or inaccessible: {img_url}")


        logging.info(f"No valid images found on {url} after {max_attempts} validation attempts.")
        return None

    except Exception as e:
        logging.error(f"Error getting image URL from {url}: {e}", exc_info=True)
        # Decide if this is a critical error for the bot
        # bot_status["errors"] += 1
        # save_bot_state_sync(cursor, conn)
        return None

async def validate_image_url(session: aiohttp.ClientSession, img_url: str) -> bool:
    """
    Verifica si una URL apunta a una imagen válida y accesible.
    """
    try:
        # Usar método HEAD para verificar sin descargar todo el contenido
        async with session.head(img_url, timeout=5) as response:
            if response.status != 200:
                logging.debug(f"Validación HEAD fallida para {img_url}: {response.status}")
                return False
            content_type = response.headers.get("Content-Type", "").lower()
            # Verificar si el tipo de contenido es una imagen
            if not content_type.startswith("image/"):
                 logging.debug(f"Validación HEAD fallida para {img_url}: Content-Type no es imagen ({content_type})")
                 return False
            # Opcional: Verificar tamaño mínimo/máximo si es relevante
            content_length = int(response.headers.get("Content-Length", 0))
            if content_length > 0 and content_length < 1024: # Ejemplo: ignorar imágenes muy pequeñas (<1KB)
                 logging.debug(f"Validación HEAD fallida para {img_url}: Imagen demasiado pequeña ({content_length} bytes)")
                 return False

            logging.debug(f"Validación HEAD exitosa para {img_url}")
            return True
    except Exception as e:
        logging.debug(f"Error durante validación HEAD para {img_url}: {e}")
        return False
    
async def download_image(session: aiohttp.ClientSession, image_url: str) -> bytes | None:
    """
    Descarga el contenido binario de una URL de imagen.
    Verifica si los datos descargados son una imagen válida.
    Manejo más robusto de errores y validación.
    """
    logging.debug(f"Attempting to download image from: {image_url}")
    try:
        # Use fetch_url to download binary content
        # fetch_url should already handle basic status code checks
        image_data = await fetch_url(session, image_url)
        if not image_data:
            logging.debug(f"Failed to download image data from {image_url}.")
            return None

        # Verify if the downloaded data is a valid image using PIL
        try:
            # Use a context manager for the image to ensure it's closed
            with Image.open(io.BytesIO(image_data)) as img:
                img.verify() # Verify if it's a valid image without loading fully
                # Check image format if needed, e.g., exclude GIFs if not supported by Twitter
                if img.format not in ['JPEG', 'PNG', 'WEBP']: # Add/remove formats as needed
                     logging.warning(f"Downloaded data from {image_url} is in unsupported format: {img.format}")
                     return None
                logging.debug(f"Downloaded data from {image_url} verified as valid image.")
            # Return the original downloaded data if verification is successful
            return image_data
        except Exception as e:
            # Catch specific PIL errors if possible, or a general Exception
            logging.warning(f"Downloaded data from {image_url} is not a valid image or verification failed: {e}")
            return None

    except Exception as e:
        # Log as error for unexpected issues during download
        logging.error(f"Error downloading image from {image_url}: {e}", exc_info=True)
        # Do not increment error counter for common network issues, only critical logic errors.
        return None

async def shorten_url(url: str) -> str:
    global bot_status
    bot_status["last_task"] = f"Acortando URL: {url}"
    try:
        if url in url_cache:
            return url_cache[url]
        cache_path = os.path.join(CONFIG["PATHS"]["CACHE_DIR"], f"{hashlib.sha256(url.encode()).hexdigest()}.url")
        if os.path.exists(cache_path):
            async with aiofiles.open(cache_path, "r") as f:
                short_url = await f.read()
                url_cache[url] = short_url
                return short_url
        async with aiohttp.ClientSession() as session:
            async with session.get(f"http://tinyurl.com/api-create.php?url={url}") as response:
                if response.status != 200:
                    logging.error(f"Error acortando {url}: Código de estado {response.status}")
                    return url
                short_url = await response.text()
                async with aiofiles.open(cache_path, "w") as f:
                    await f.write(short_url)
                url_cache[url] = short_url
                return short_url
    except Exception as e:
        logging.error(f"Error acortando {url}: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return url

async def get_trending_keywords() -> list[str]:
    global trending_keywords, bot_status
    bot_status["last_task"] = "Obteniendo tendencias"
    try:
        return list(KEYWORDS_EN["gaming"].keys()) + list(KEYWORDS_EN["tech"].keys())
    except Exception as e:
        logging.error(f"Error obteniendo tendencias: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return list(KEYWORDS_EN["gaming"].keys()) + list(KEYWORDS_EN["tech"].keys())

def calculate_relevance(title: str, summary: str) -> float:
    content = (title + " " + summary).lower()
    keyword_score = 0
    for category in ["gaming", "tech"]:
        kw_dict = KEYWORDS_EN[category]
        for kw, weight in kw_dict.items():
            if kw in content:
                keyword_score += weight * 1.5
    for kw in KEYWORDS_EN["news_indicators"]:
        if kw in content:
            keyword_score += 0.75
    return keyword_score

def calculate_freshness(news_date: datetime) -> float:
    try:
        age_hours = (datetime.now() - news_date).total_seconds() / 3600
        return max(0, 1 - 0.1 * age_hours)
    except Exception as e:
        logging.error(f"Error calculando frescura: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return 0

async def is_duplicate(title: str, summary: str, url: str, source: str, entry_date: datetime = None) -> bool:
    """
    Verifica si una noticia es un duplicado basándose en hash.

    Args:
        title (str): Título de la noticia.
        summary (str): Resumen de la noticia.
        url (str): URL de la noticia.
        source (str): Fuente de la noticia.
        entry_date (datetime, optional): Fecha de publicación de la noticia.

    Returns:
        bool: True si la noticia es un duplicado, False si es nueva.
    """
    news_hash = hashlib.sha256((title + summary + url + source).encode()).hexdigest()
    cache_key = f"{news_hash}_duplicate"

    # Verificar si es reciente (dentro de 24 horas) y está en caché
    if entry_date and (datetime.now() - entry_date).total_seconds() < 86400:  # 24 horas
        if cache_key in duplicate_cache:
            logging.debug(f"✅ Duplicado encontrado en caché: {title[:50]}")
            return duplicate_cache[cache_key]
    
    try:
        # Crear un cursor local
        local_cursor = conn.cursor()
        try:
            local_cursor.execute('''SELECT 1 FROM historial WHERE hash = ?''', (news_hash,))
            result = await run_db_sync(local_cursor.fetchone)
            is_duplicate = result is not None
        finally:
            local_cursor.close()  # Cerrar el cursor explícitamente

        # Almacenar en caché si es reciente
        if entry_date and (datetime.now() - entry_date).total_seconds() < 86400:
            duplicate_cache[cache_key] = is_duplicate

        # Registrar resultado
        if is_duplicate:
            logging.debug(f"❌ Noticia duplicada: {title[:50]}")
        else:
            logging.debug(f"✅ Noticia nueva: {title[:50]}")

        return is_duplicate
    except Exception as e:
        logging.error(f"❌ Error verificando duplicado: {e}", exc_info=True)
        bot_status["errors"] += 1
        await save_bot_state()
        return False
    
async def is_published_in_language(title: str, summary: str, url: str, source: str, language: str) -> bool:
     """
     Checks if a news item has already been published in a specific language.
     Uses robust criteria, including the language.
     Executes the database query in a separate thread using run_db_sync.
     """
     # Add checks for None or empty strings for inputs
     if not title or not url or not source or not language:
         return False # Cannot check with missing info

     try:
         # Use normalized text for title and summary if needed for language-specific check
         # normalized_title = normalize_text_for_duplicate_check(title)
         # normalized_summary = normalize_text_for_duplicate_check(summary)

         # Query includes language
         result = await run_db_sync(
             cursor.execute,
             "SELECT 1 FROM historial WHERE title = ? AND summary = ? AND url = ? AND source = ? AND language = ?",
             (title, summary, url, source, language)
         )
         fetch_result = await run_db_sync(result.fetchone)
         is_published = fetch_result is not None
         # logging.debug(f"Published check for '{title[:50]}' in {language}: {is_published}") # Too verbose
         return is_published
     except Exception as e:
         logging.error(f"Error in is_published_in_language: {e}", exc_info=True)
         # Decide if this is a critical error
         # bot_status["errors"] += 1
         # await save_bot_state()
         return False # Assume not published in case of DB error
     
try:
    tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
    bert_model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
    bert_model.eval()
    if torch.cuda.is_available():
        bert_model.to("cuda")
    logging.info("Modelo DistilBERT y tokenizer inicializados correctamente.")
except Exception as e:
    logging.error(f"Error al inicializar DistilBERT: {e}", exc_info=True)
    tokenizer = None
    bert_model = None

async def score_news(entry, trends):
    if tokenizer is None or bert_model is None:
        logging.error("Modelo DistilBERT o tokenizer no inicializados.")
        return 0.0

    try:
        text = entry.get('title', '')
        if not text:
            logging.warning("Título de noticia vacío.")
            return 0.0
    
        # Filtrar noticias de gaming/tecnología
        text_lower = text.lower()
        if any(keyword in text_lower for keyword in KEYWORDS_EN):
            logging.debug(f"Noticia descartada por tema de gaming/tecnología: {text}")
            return 0.0  # Puntuación 0 para descartar

        def run_bert(inputs):
            with torch.no_grad():
                outputs = bert_model(**inputs)
                probs = torch.softmax(outputs.logits, dim=-1)
                return probs[0][1].item()  # Probabilidad de la clase positiva

        inputs = tokenizer(
            text,
            return_tensors="pt",
            max_length=128,
            truncation=True,
            padding=True
        )
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}

        loop = asyncio.get_running_loop()
        score = await loop.run_in_executor(None, partial(run_bert, inputs))

        # Ajustar con tendencias
        trend_score = 0.0
        for trend in trends:
            if trend.lower() in text_lower:
                trend_score += 20.0  # Aumentar puntaje en escala 0-100
        final_score = min(score * 100 + trend_score, 100.0)  # Escala 0-100

        logging.debug(f"Noticia puntuada: {text} con score {final_score} (base: {score*100}, tendencias: {trend_score})")
        return final_score

    except Exception as e:
        logging.error(f"Error en score_news: {e}", exc_info=True)
        return 0.0

async def optimize_image(image_data: bytes) -> bytes | None:
    """
    Optimiza una imagen para reducir su tamaño manteniendo una calidad aceptable.
    Utiliza Pillow para redimensionar y ajustar la calidad JPEG.
    Ejecuta operaciones de Pillow en un hilo separado.
    Manejo de errores mejorado.
    """
    loop = asyncio.get_event_loop()
    # Ensure image_data is not None before proceeding
    if image_data is None:
        logging.warning("optimize_image received None data.")
        return None

    logging.debug(f"Starting image optimization. Original size: {len(image_data)} bytes.")

    try:
        # Define a synchronous function for Pillow operations
        def optimize_image_sync(img_data_bytes):
            try:
                # Use a context manager for the image
                with Image.open(io.BytesIO(img_data_bytes)) as img:
                    # Ensure image is in RGB mode for JPEG saving
                    if img.mode != 'RGB':
                        img = img.convert("RGB")

                    max_long_dim = 1280
                    width, height = img.size
                    # Resize if necessary
                    if max(width, height) > max_long_dim:
                        if width > height:
                            new_width = max_long_dim
                            new_height = int(height * (new_width / width))
                        else:
                            new_height = max_long_dim
                            new_width = int(width * (new_height / height))
                        # Use LANCZOS for better quality resizing
                        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)

                    output = io.BytesIO()
                    jpeg_quality = 85
                    # Try saving with initial quality
                    img.save(output, format="JPEG", quality=jpeg_quality, optimize=True)
                    optimized_data = output.getvalue()

                    # Re-optimize with lower quality if size reduction is not significant
                    # Check if optimized size is still close to original AND size is large
                    if len(optimized_data) >= len(img_data_bytes) * 0.95 and len(optimized_data) > 100 * 1024 and jpeg_quality > 80: # Added size check > 100KB
                         logging.debug("Optimized image size not significantly smaller, attempting lower quality.")
                         output = io.BytesIO() # Reset buffer
                         img.save(output, format="JPEG", quality=80, optimize=True)
                         optimized_data = output.getvalue()

                    # Final check on size, Twitter limit is 5MB for most image types
                    if len(optimized_data) > 5 * 1024 * 1024:
                         logging.warning(f"Optimized image is still too large: {len(optimized_data)} bytes.")
                         return None # Return None if still too large

                    return optimized_data

            except Exception as e:
                # Log the error within the synchronous function
                logging.error(f"Error optimizing image in thread: {e}", exc_info=True)
                return None

        # Execute the synchronous function in the executor
        optimized_data = await loop.run_in_executor(None, optimize_image_sync, image_data)

        if optimized_data:
            logging.debug(f"Image optimized successfully. Final size: {len(optimized_data)} bytes.")
        else:
            logging.warning("Image optimization failed or resulted in invalid data.")

        return optimized_data

    except Exception as e:
        # Log general errors from the async wrapper
        logging.error(f"General error in optimize_image async wrapper: {e}", exc_info=True)
        # bot_status["errors"] += 1 # Decide if this is a critical bot error
        # await save_bot_state()
        return None

async def translate_to_spanish(text: str) -> str:
    cleaned_text = clean_text(text)
    if not cleaned_text:
        return text
    loop = asyncio.get_event_loop()
    translated = await loop.run_in_executor(executor, translate_text, cleaned_text)
    translated = post_process_translation(translated)
    return translated

def get_summary_sentences(summary: str, max_chars: int) -> str:
    """
    Divide el resumen en oraciones y selecciona las que caben en max_chars.
    """
    sentences = re.split(r'(?<=[.!?])\s+', summary)
    selected_sentences = []
    current_length = 0
    for sentence in sentences:
        sentence_length = len(sentence) + 1  # +1 para el espacio
        if current_length + sentence_length > max_chars:
            break
        selected_sentences.append(sentence)
        current_length += sentence_length
    return " ".join(selected_sentences).strip()


async def generate_detailed_tweet(title: str, summary: str, url: str, trends: list[str], language: str) -> str:
    is_gaming = any(kw in title.lower() for kw in ["game", "nintendo", "playstation", "xbox"])
    emojis = "🎮" if is_gaming else "💻"

    if language == 'es':
        title_text = await translate_to_spanish(title)
        summary_text = await translate_to_spanish(clean_text(summary))
    else:
        title_text = title
        summary_text = clean_text(summary)

    # Hashtags basados en palabras clave
    keywords = re.findall(r'\b\w+\b', (title + " " + summary).lower())
    relevant_keywords = [kw for kw in keywords if len(kw) > 3 and kw not in ["the", "and"]]
    hashtags = [f"#{kw.capitalize()}" for kw in relevant_keywords[:2]]
    hashtags_str = " ".join(hashtags)

    short_url = await shorten_url(url) if url else ""
    fixed_length = len(emojis) + 1 + len(title_text) + 2 + len(hashtags_str) + (len(short_url) + 1 if short_url else 0)
    available_chars = 280 - fixed_length - len(" ¡Descubre más! ") - 5

    summary_to_include = summary_text[:available_chars]
    if len(summary_text) > available_chars:
        summary_to_include = summary_to_include.rsplit(' ', 1)[0] + "..."

    cta = "¡Descubre más!" if language == 'es' else "Find out more!"
    tweet = f"{emojis} {title_text}. {summary_to_include} {cta} {hashtags_str} {short_url}".strip()
    if len(tweet) > 280:
        tweet = tweet[:277] + "..."

    return tweet

async def generate_weather_tweet(language: str, source_url: str) -> str:
    global bot_status
    bot_status["last_task"] = f"Generando tweet del mapa del tiempo en {language}"
    try:
        tomorrow = (datetime.now() + timedelta(days=1)).strftime("%d/%m/%Y")
        short_url = await shorten_url(source_url)
        if language == "es":
            tweet = f"🌤️ Pronóstico del tiempo en España para mañana ({tomorrow}). #Tiempo #Meteorología {short_url}"
        else:
            tweet = f"🌤️ Weather forecast for Spain for tomorrow ({tomorrow}). #Weather #Meteorology {short_url}"
        
        if len(tweet) > CONFIG["TWEET"]["MAX_LENGTH"]:
            tweet = tweet[:CONFIG["TWEET"]["MAX_LENGTH"]-3] + "..."
        
        logging.info(f"Tweet del tiempo generado ({len(tweet)} chars): {tweet}")
        return tweet
    except Exception as e:
        logging.error(f"Error generando tweet del tiempo en {language}: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return ""

import aiohttp
import os
import hashlib
import logging
import colorlog
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urljoin

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time

def capture_map_screenshot(url, output_path):
    """
    Captura una imagen del mapa meteorológico desde la URL dada usando Selenium.
    
    Args:
        url (str): URL de la página con el mapa (ej. "https://www.eltiempo.es/lluvia").
        output_path (str): Ruta donde guardar la captura (formato PNG).
    """
    options = Options()
    options.headless = True  # Ejecutar Chrome sin interfaz gráfica
    driver = webdriver.Chrome(options=options)
    
    # Ajustar el tamaño de la ventana para capturar el mapa completo
    driver.set_window_size(1200, 800)
    
    # Abrir la URL
    driver.get(url)
    
    # Esperar a que el mapa se cargue completamente
    time.sleep(5)
    
    # Guardar la captura de pantalla
    driver.save_screenshot(output_path)
    
    # Cerrar el navegador
    driver.quit()
    
    logging.info(f"✅ Captura guardada en {output_path}")

async def post_weather_map(session):
    global bot_status
    print_section_header("Publicando Pronóstico del Tiempo", color="magenta")
    
    try:
        # Configuración
        map_url = CONFIG["WEATHER"]["MAP_URL"]  # "https://www.eltiempo.es/lluvia"
        tomorrow = (datetime.now() + timedelta(days=1)).strftime("%d/%m/%Y")
        
        # Obtener resumen del pronóstico
        forecast_text = await get_weather_forecast(session, tomorrow)
        tweet_text = f"{forecast_text} #TiempoEspaña"
        
        logging.debug(f"📝 Preparando tweet: {tweet_text}")
        print(f"\033[1;36m📅 Pronóstico para: {tomorrow}\033[0m")
        print(f"\033[1;36m📝 Tweet: {tweet_text[:50]}...\033[0m")

        # Capturar imagen del mapa con Selenium
        loop = asyncio.get_event_loop()
        output_path = os.path.join(CONFIG["PATHS"]["TEMP_IMAGE_DIR"], f"weather_map_{tomorrow.replace('/', '-')}.png")
        await loop.run_in_executor(None, capture_map_screenshot, map_url, output_path)
        
        # Leer la imagen capturada
        async with aiofiles.open(output_path, "rb") as f:
            media_data = await f.read()
        media_type = "image"

        # Optimizar la imagen si es necesario
        optimized_data = await optimize_image(media_data)
        if optimized_data:
            media_data = optimized_data
            logging.debug("Imagen optimizada exitosamente")
        else:
            logging.warning("No se pudo optimizar la imagen, usando original")

        # Publicar el tweet
        success = await post_tweet(
            session=session,
            title=tweet_text,
            summary=tweet_text,
            url="",
            image_data=media_data,
            news_hash=hashlib.sha256(tweet_text.encode()).hexdigest(),
            score=1.0,
            source="weather_map",
            language="es",
            trends=["#TiempoEspaña", "#Pronostico"]
        )

        if success:
            logging.info(f"✅ Pronóstico del tiempo publicado exitosamente")
            print(f"\033[1;32m✅ Publicado: {tweet_text[:50]}...\033[0m")
        else:
            logging.error(f"❌ Error publicando el pronóstico del tiempo")
            print(f"\033[1;31m❌ Error al publicar el pronóstico\033[0m")
            bot_status["errors"] += 1

    except Exception as e:
        logging.error(f"❌ Error en post_weather_map: {e}", exc_info=True)
        print(f"\033[1;31m❌ Error: {e}\033[0m")
        bot_status["errors"] += 1
    finally:
        # Limpiar archivo temporal
        if os.path.exists(output_path):
            await asyncio.to_thread(os.remove, output_path)
        print(f"\033[1;34m{'─' * 60}\033[0m\n")

async def get_weather_forecast(session, date):
    """Obtiene un resumen del pronóstico para el día especificado usando OpenWeatherMap."""
    try:
        api_key = CONFIG["WEATHER"].get("OPENWEATHER_API_KEY")
        if not api_key:
            logging.warning("⚠️ Clave de OpenWeatherMap no configurada. Usando texto genérico.")
            return CONFIG["WEATHER"].get("FORECAST_TEMPLATE", "Pronóstico para {date}: 🌧️ Lluvias moderadas.").format(date=date)
        
        url = f"http://api.openweathermap.org/data/2.5/forecast?q=Madrid,ES&appid={api_key}&units=metric"
        async with session.get(url) as response:
            if response.status != 200:
                logging.error(f"❌ Error en API OpenWeatherMap: Código {response.status}")
                return CONFIG["WEATHER"]["FORECAST_TEMPLATE"].format(date=date)
            
            data = await response.json()
            tomorrow = (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")
            forecast = next((item for item in data["list"] if tomorrow in item["dt_txt"]), None)
            if forecast:
                rain = forecast.get("rain", {}).get("3h", 0)
                wind = forecast["wind"]["speed"]
                temp = forecast["main"]["temp"]
                return f"Pronóstico para {date}: 🌧️ Lluvias: {rain}mm, 💨 Viento: {wind}m/s, 🌡️ Temp: {temp}°C."
            return CONFIG["WEATHER"]["FORECAST_TEMPLATE"].format(date=date)
    except Exception as e:
        logging.error(f"❌ Error obteniendo pronóstico: {e}", exc_info=True)
        return CONFIG["WEATHER"]["FORECAST_TEMPLATE"].format(date=date)

async def scrape_weather_media(session, url):
    """Busca y descarga un GIF real desde la página."""
    try:
        async with session.get(url, timeout=10) as response:
            if response.status != 200:
                return None, None
            html = await response.text()
            soup = BeautifulSoup(html, "html.parser")
            # Busca un elemento con el GIF (ajusta según la estructura de la página)
            gif_tag = soup.find("img", src=lambda src: src and src.lower().endswith(".gif"))
            if gif_tag:
                gif_url = urljoin(url, gif_tag["src"])
                async with session.get(gif_url, timeout=10) as gif_response:
                    if gif_response.status == 200 and gif_response.headers.get("Content-Type", "").startswith("image/gif"):
                        return await gif_response.read(), "gif"
            return None, None
    except Exception as e:
        print(f"Error al scrapear {url}: {e}")
        return None, None

async def repost_tweet_from_url(tweet_url: str, summary: str, language: str = 'en') -> bool:
    global bot_status
    category = f"write_{language}"
    current_time = time.time()
    if current_time < bot_status["twitter_wait_until"].get(category, 0):
        logging.warning(f"En período de espera para {category} hasta {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%H:%M:%S')}")
        return False

    try:
        tweet_match = re.search(r'https://(nitter\.net|twitter\.com|x\.com)/(\w+)/status/(\d+)', tweet_url)
        if not tweet_match:
            logging.warning(f"URL no válida para republicación: {tweet_url}")
            return False
        
        username = tweet_match.group(2)
        tweet_id = tweet_match.group(3)
        title = f"Tweet from @{username}"
        source = f"twitter_{username}"
        client = twitter_client_en if language == 'en' else twitter_client_es
        daily_key = "daily_tweets_total"
        monthly_key = f"monthly_posts_{language}"
        posted_key = f"posted_tweets_{language}"
        last_tweet_key = f"last_tweet_time_{language}"
        semaphore = write_semaphore_en if language == "en" else write_semaphore_es
        rate_limit_event = write_rate_limit_event_en if language == "en" else write_rate_limit_event_es

        if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"] or bot_status[daily_key] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
            logging.info(f"Límite alcanzado para {language} (diario: {bot_status[daily_key]}/{CONFIG['API_LIMITS']['TWEETS_PER_DAY']})")
            return False
        
        last_tweet_time = bot_status[last_tweet_key]
        if last_tweet_time and (time.time() - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
            logging.info(f"Espera requerida para {language}")
            return False
        
        if await is_published_in_language(title, summary, tweet_url, source, language):
            logging.info(f"Tweet ya publicado en {language}: {tweet_url}")
            return False
        
        tweet_response = await make_twitter_request(
            client.get_tweet,
            id=tweet_id,
            tweet_fields=["text"],
            category="read"
        )
        if not tweet_response.data:
            logging.warning(f"No se pudo obtener el tweet {tweet_id}")
            return False

        tweet_text = tweet_response.data.text
        comment = summary.strip() if summary else ""
        if comment.startswith(f"RT @{username}:"):
            comment = comment[len(f"RT @{username}:"):].strip()
        if language == 'es' and comment:
            comment = await translate_to_spanish(comment)

        if comment and len(comment) > CONFIG["TWEET"]["MAX_LENGTH"] - 50:
            comment = comment[:CONFIG["TWEET"]["MAX_LENGTH"]-53] + "..."

        if comment:
            tweet_response = await make_twitter_request(
                client.create_tweet,
                text=comment,
                quote_tweet_id=tweet_id,
                category=category,
                semaphore=semaphore,
                rate_limit_event=rate_limit_event
            )
            logging.info(f"Quote Tweet publicado en {language}: {comment[:50]}... ID: {tweet_response.data['id']}")
        else:
            tweet_response = await make_twitter_request(
                client.retweet,
                tweet_id=tweet_id,
                category=category,
                semaphore=semaphore,
                rate_limit_event=rate_limit_event
            )
            logging.info(f"Retweet simple publicado en {language}: Tweet ID {tweet_id}")

        bot_status[daily_key] += 1
        bot_status[monthly_key] += 1
        bot_status[posted_key] += 1
        bot_status[last_tweet_key] = datetime.now()
        
        news_hash = hashlib.sha256((title + summary + tweet_url + source).encode()).hexdigest()
        cursor.execute('''INSERT OR IGNORE INTO historial (hash, title, url, tweet, relevance, source, date, engagement, summary, language)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                      (news_hash, title, tweet_url, comment or "Retweet", 50.0, source, datetime.now().isoformat(), 0, tweet_text, language))
        conn.commit()
        save_bot_state_sync(cursor, conn)
        return True
        
    except Exception as e:
        logging.error(f"Error al citar tweet {tweet_url} en {language}: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return False

async def post_tweet(session: aiohttp.ClientSession, title: str, summary: str, url: str, image_data: bytes | None, news_hash: str, score: float, source: str, language: str, trends: list[str]) -> bool:
    """
    Attempts to post a tweet. Handles limits, cooldown, duplicates, image upload
    and adds to the queue if it fails due to recoverable limits.
    Uses CONFIG["API_LIMITS"] and CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"] values.
    Benefits from dynamic limit handling in make_twitter_request.
    Modified to handle image_data being None and improved error handling.
    """
    global bot_status
    category = f"write_{language}"
    current_time = time.time()

    # Check if writing for this language is paused
    if current_time < bot_status["twitter_wait_until"].get(category, 0):
        logging.warning(f"In waiting period for {category} until {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%H:%M:%S')}. Skipping post attempt.")
        # Add to queue if not already published and not in queue?
        # This logic is better handled in process_single_news or the caller
        return False

    client = twitter_client_en if language == "en" else twitter_client_es
    api = twitter_api_en if language == "en" else twitter_api_es
    daily_key = "daily_tweets_total"
    monthly_key = f"monthly_posts_{language}"
    posted_key = f"posted_tweets_{language}"
    last_tweet_key = f"last_tweet_time_{language}"
    semaphore = write_semaphore_en if language == "en" else write_semaphore_es
    rate_limit_event = write_rate_limit_event_en if language == "en" else write_rate_limit_event_es

    try:
        # Check monthly and daily limits using CONFIG["API_LIMITS"] values
        if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
            logging.info(f"Total monthly limit reached: {bot_status['monthly_posts_en'] + bot_status['monthly_posts_es']}/{CONFIG['API_LIMITS']['MONTHLY_POSTS_TOTAL']}. Skipping post attempt.")
            return False

        if bot_status[daily_key] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
            logging.info(f"Daily limit reached: {bot_status[daily_key]}/{CONFIG['API_LIMITS']['TWEETS_PER_DAY']}. Skipping post attempt.")
            return False

        # Check tweet cooldown using CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]
        last_tweet_time = bot_status.get(last_tweet_key) # Use .get() for safety
        if last_tweet_time and (current_time - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
            remaining = CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"] - (current_time - last_tweet_time.timestamp())
            logging.info(f"Cooldown required for {language}: {remaining/60:.1f} minutes remaining. Skipping post attempt.")
            return False

        # Check if already published using the async function is_published_in_language
        # This check is critical before attempting to post
        if await is_published_in_language(title, summary, url, source, language):
            logging.info(f"News already published in {language}: {title[:50]}. Skipping post attempt.")
            return False

        logging.info(f"Intentando publicar en {language}: {title[:50]}...")
        tweet = await generate_detailed_tweet(title, summary, url, trends, language) # generate_detailed_tweet must be defined
        temp_image_path = os.path.join(CONFIG["PATHS"]["TEMP_IMAGE_DIR"], f"{news_hash}_{language}.jpg")

        media = None
        # --- MODIFICATION: Check if image_data is valid before attempting to optimize/upload ---
        # image_data is already bytes | None from previous steps
        if image_data and isinstance(image_data, bytes) and len(image_data) > 0:
            # Use aiofiles for asynchronous file operations
            try:
                # Save original downloaded image data temporarily
                async with aiofiles.open(temp_image_path, "wb") as f:
                    await f.write(image_data)

                # optimize_image now handles None input and returns None on failure
                optimized_image_data = await optimize_image(image_data)

                if optimized_image_data:
                    # Save optimized data to the temporary file for Tweepy
                    async with aiofiles.open(temp_image_path, "wb") as f:
                        await f.write(optimized_image_data)

                    try:
                        # make_twitter_request handles Tweepy's sync call and rate limits
                        media = await make_twitter_request(
                            api.media_upload,
                            temp_image_path,
                            category=category,
                            semaphore=semaphore,
                            rate_limit_event=rate_limit_event
                        )
                        logging.debug(f"Image uploaded successfully for tweet in {language}.")
                    except Exception as e:
                         logging.error(f"Error uploading image for tweet in {language}: {e}")
                         media = None # Continue without image if upload fails
                else:
                    logging.warning(f"Image optimization failed for tweet in {language}. Posting without image.")
                    media = None # Post without image if optimization fails
            except Exception as e:
                logging.error(f"Error processing image for tweet in {language}: {e}", exc_info=True)
                media = None # Ensure media is None if any image processing error occurs
        else:
            logging.debug(f"No valid image data provided for tweet in {language}. Posting without image.")
            media = None # No image data to process

        tweet_response = None
        try:
            # make_twitter_request handles the Tweepy call, rate limits, and retries
            tweet_response = await make_twitter_request(
                client.create_tweet,
                text=tweet,
                media_ids=[media.media_id] if media else None, # Pass media_ids only if media exists
                category=category,
                semaphore=semaphore,
                rate_limit_event=rate_limit_event
            )
            logging.info(f"Tweet in {language} posted (score {score:.1f}): {tweet[:50]}... ID: {tweet_response.data['id']}")

            # Update status counters
            bot_status[daily_key] += 1
            bot_status[monthly_key] += 1
            bot_status[posted_key] += 1
            bot_status[last_tweet_key] = datetime.now()

            # Record in history using run_db_sync
            await run_db_sync(
                cursor.execute,
                '''INSERT OR IGNORE INTO historial (hash, title, url, tweet, relevance, source, date, engagement, summary, language)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                (news_hash, title, url, tweet, score, source, datetime.now().isoformat(), 0, summary, language)
            )
            await run_db_sync(conn.commit)
            await save_bot_state() # Save state after successful post and history update

            return True

        except Exception as e:
            logging.error(f"Error posting tweet in {language}: {e}", exc_info=True)
            # Check if the error is a recoverable rate limit error (429)
            is_rate_limit_error = isinstance(e, tweepy.TweepyException) and e.response and e.response.status_code == 429

            # Add to queue ONLY if it's a recoverable error AND it hasn't been published yet in this language
            # The check `await is_published_in_language(...)` is crucial here to avoid queuing items that were
            # successfully posted by another process/attempt but failed to be removed from a previous queue run.
            if is_rate_limit_error and not await is_published_in_language(title, summary, url, source, language):
                 trends_json = json.dumps(trends)
                 # Add to queue using run_db_sync
                 # Store the ORIGINAL image_data (bytes or None) in the queue
                 await run_db_sync(
                     cursor.execute,
                     '''INSERT INTO cola_publicacion (title, summary, url, image_data, news_hash, score, source, language, trends)
                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                     (title, summary, url, image_data, news_hash, score, source, language, trends_json)
                 )
                 await run_db_sync(conn.commit)
                 logging.info(f"News '{title[:50]}' added to queue for {language} due to rate limit.")
            else:
                 # If it's not a rate limit error, or if it's already published, just log the error.
                 logging.warning(f"News '{title[:50]}' failed to post in {language} due to non-recoverable error or already published. Not adding to queue.")


            bot_status["errors"] += 1 # Increment error counter for any posting failure
            await save_bot_state() # Save state after error (and potential queue addition)
            return False # Return False if posting failed

    finally:
        # Use asyncio.to_thread for synchronous file operations like os.remove
        if os.path.exists(temp_image_path):
            await asyncio.to_thread(os.remove, temp_image_path)
        # await asyncio.sleep(2) # Small pause after attempting to post (optional, make_twitter_request might already wait)

async def process_queue():
    global bot_status
    bot_status["last_task"] = "Processing publication queue"
    print_section_header("Procesando Cola de Publicación")
    logging.debug("Starting publication queue processing...")

    try:
        cursor.execute("SELECT id, title, summary, url, image_data, news_hash, score, source, language, trends FROM cola_publicacion ORDER BY added_at ASC")
        pending_news = await run_db_sync(cursor.fetchall)

        if not pending_news:
            print("ℹ️ La cola de publicación está vacía.")
            logging.debug("Publication queue is empty.")
            return

        logging.info(f"Items in publication queue: {len(pending_news)}")

        async with aiohttp.ClientSession() as session:
            items_to_remove = []

            for news in pending_news:
                id, title, summary, url, image_data_bytes, news_hash, score, source, language, trends_json = news
                trends = json.loads(trends_json)
                category = f"write_{language}"
                current_time_ts = time.time()

                if current_time_ts < bot_status["twitter_wait_until"].get(category, 0):
                    logging.debug(f"Escritura pausada para {language}. Omitiendo elemento de la cola (ID: {id}).")
                    continue

                if bot_status["daily_tweets_total"] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
                    logging.debug("Límite diario de tweets alcanzado. Deteniendo procesamiento de la cola.")
                    break

                if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
                    logging.debug("Límite mensual total de publicaciones alcanzado. Deteniendo procesamiento de la cola.")
                    break

                last_tweet_time = bot_status.get(f"last_tweet_time_{language}")
                if last_tweet_time and (current_time_ts - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
                    remaining = CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"] - (current_time_ts - last_tweet_time.timestamp())
                    logging.debug(f"Enfriamiento requerido para {language}: {remaining:.1f}s restantes.")
                    continue

                if await is_published_in_language(title, summary, url, source, language):
                    logging.info(f"Elemento de la cola ya publicado ({language}): {title[:50]}. Marcando para eliminar (ID: {id}).")
                    items_to_remove.append(id)
                    continue

                logging.info(f"Intentando publicar elemento de la cola (ID: {id}) en {language}: {title[:50]}...")

                try:
                    success = await post_tweet(session, title, summary, url, image_data_bytes, news_hash, score, source, language, trends)
                    if success:
                        logging.info(f"Elemento de la cola (ID: {id}) publicado exitosamente en {language}. Marcando para eliminar.")
                        items_to_remove.append(id)
                        await asyncio.sleep(CONFIG["INTERVALS"]["API_REQUEST_DELAY_SECONDS"])
                    else:
                        logging.warning(f"No se pudo publicar el elemento de la cola (ID: {id}) en {language}. Permanece en la cola.")
                except Exception as e:
                    logging.error(f"Error intentando publicar elemento de la cola (ID: {id}) en {language}: {e}", exc_info=True)
                    bot_status["errors"] += 1
                    await save_bot_state()

            if items_to_remove:
                logging.info(f"Eliminando {len(items_to_remove)} elementos de la cola de publicación.")
                try:
                    await run_db_sync(
                        cursor.execute,
                        f"DELETE FROM cola_publicacion WHERE id IN ({','.join('?' * len(items_to_remove))})",
                        tuple(items_to_remove)
                    )
                    await run_db_sync(conn.commit)
                    await save_bot_state()
                except Exception as e:
                    logging.error(f"Error eliminando elementos de la cola: {e}", exc_info=True)
                    bot_status["errors"] += 1
                    await save_bot_state()

    except Exception as e:
        logging.error(f"Error general procesando la cola de publicación: {e}", exc_info=True)
        bot_status["errors"] += 1
        await save_bot_state()

async def get_image_from_search(session: aiohttp.ClientSession, query: str) -> bytes | None:
    """
    Busca una imagen relacionada con una consulta usando la búsqueda de imágenes de Google
    y descarga la primera imagen válida encontrada.
    Mejorado manejo de errores y validación.
    """
    logging.debug(f"Searching for image for query: '{query}'")
    try:
        # Use aiohttp for the HTTP request
        # Using a more specific image search URL if possible, or rely on tbm=isch
        search_url = f"https://www.google.com/search?tbm=isch&q={urllib.parse.quote(query)}"
        # Using a User-Agent to avoid being blocked
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}

        async with session.get(search_url, headers=headers, timeout=aiohttp.ClientTimeout(total=15)) as response:
            if response.status != 200:
                logging.warning(f"Error in image search for '{query}': Status code {response.status}")
                return None
            html_content = await response.text()

        soup = BeautifulSoup(html_content, "html.parser")
        # Google Images loads dynamically, extracting URLs can be tricky.
        # A more robust approach might involve using a dedicated image search API or a headless browser.
        # This simple approach looks for img tags with src or data-src.
        img_tags = soup.find_all("img", src=True)
        if not img_tags:
             img_tags = soup.find_all("img", {"data-src": True})

        image_urls = []
        for img_tag in img_tags:
            img_url = img_tag.get("src") or img_tag.get("data-src")
            # Filter out URLs that are likely not content images (e.g., icons, spacers)
            if img_url and img_url.startswith(("http://", "https://")) and not any(ext in img_url.lower() for ext in [".gif", ".svg", ".ico", "spacer.gif", "gstatic.com"]): # Added gstatic.com filter
                image_urls.append(img_url)

        logging.debug(f"Image URLs found in search for '{query}': {image_urls[:10]}...") # Log only the first few

        # Attempt to download and validate the first few found URLs
        for img_url in image_urls[:5]: # Limit the number of downloads from search
            logging.debug(f"Attempting to download and validate search image: {img_url}")
            # Use download_image which includes validation
            img_data = await download_image(session, img_url)
            if img_data:
                # Attempt to optimize the downloaded image
                # optimize_image now handles None input and returns None on failure
                optimized_data = await optimize_image(img_data)
                if optimized_data:
                    logging.info(f"Image obtained from search and optimized for '{query[:50]}'")
                    return optimized_data
                else:
                    logging.debug(f"Could not optimize the search image: {img_url}")
            else:
                 logging.debug(f"Could not download or validate the search image: {img_url}")

        logging.info(f"Could not obtain a valid image from search for '{query}'")
        return None
    except Exception as e:
        logging.error(f"Error searching for image for '{query[:50]}': {e}", exc_info=True)
        # Decide if this should increment the error counter
        # bot_status["errors"] += 1
        # save_bot_state_sync(cursor, conn)
        return None
    
async def publish_news(session: aiohttp.ClientSession, entry: dict, news_hash: str, score: float, language: str, trends: list[str]) -> bool:
    """
    Publica una noticia en Twitter/X, manejando imágenes y tweets/reposts.
    
    Args:
        session: Sesión de aiohttp para solicitudes HTTP.
        entry: Diccionario con los datos de la noticia.
        news_hash: Hash único de la noticia.
        score: Puntuación de relevancia.
        language: Idioma de publicación ("en" o "es").
        trends: Lista de tendencias actuales.
    
    Returns:
        bool: True si se publicó exitosamente, False en caso contrario.
    """
    title = entry.get("title", "Untitled")
    url = entry.get("link")
    summary = entry.get("summary", "")
    source = entry.get("source", "Unknown Source")
    is_tweet = entry.get("is_tweet", False)
    
    logging.debug(f"Preparando publicación para {language}: {title[:50]} (is_tweet: {is_tweet})")
    
    if is_tweet:
        try:
            result = await repost_tweet_from_url(url, summary, language)
            if result:
                logging.info(f"Repost exitoso en {language}: {title[:50]}")
            return result
        except tweepy.TooManyRequests as e:
            logging.warning(f"Error 429 al repostear en {language}: {title[:50]}: {e}")
            return False
        except Exception as e:
            logging.error(f"Error al repostear en {language}: {title[:50]}: {e}")
            bot_status["errors"] += 1
            await save_bot_state()
            return False
    
    image_data = None
    if entry.get("image_url"):
        logging.debug(f"Descargando imagen desde {entry['image_url']}")
        image_data = await download_image(session, entry["image_url"])
        image_data = await optimize_image(image_data) if image_data else None
    
    if not image_data and url and not url.startswith(("patchbot://", "discord://")):
        logging.debug(f"Buscando imagen en URL: {url}")
        img_url = await get_image_from_url(session, url)
        if img_url:
            image_data = await download_image(session, img_url)
            image_data = await optimize_image(image_data) if image_data else None
    
    if not image_data:
        search_query = clean_text(title)[:100]
        logging.debug(f"Buscando imagen en Unsplash para: {search_query}")
        image_data = await get_image_from_search(session, search_query)
    
    try:
        result = await post_tweet(session, title, summary, url, image_data, news_hash, score, source, language, trends)
        if result:
            logging.info(f"Publicación exitosa en {language}: {title[:50]}")
        return result
    except tweepy.TooManyRequests as e:
        logging.warning(f"Error 429 al publicar en {language}: {title[:50]}: {e}")
        return False
    except tweepy.TweepyException as e:
        logging.error(f"Error de API al publicar en {language}: {title[:50]}: {e}")
        bot_status["errors"] += 1
        await save_bot_state()
        return False
    except Exception as e:
        logging.error(f"Error inesperado al publicar en {language}: {title[:50]}: {e}")
        bot_status["errors"] += 1
        await save_bot_state()
        return False

async def process_publication_queue():
    """
    Procesa las noticias encoladas en la tabla cola_publicacion y las publica si es posible.
    
    Returns:
        int: Número de noticias publicadas exitosamente.
    """
    global bot_status
    published_count = 0
    
    async with aiohttp.ClientSession() as session:
        cursor.execute("SELECT * FROM cola_publicacion")
        queued_items = await run_db_sync(cursor.fetchall)
        
        for item in queued_items:
            title = item["title"]
            summary = item["summary"]
            url = item["url"]
            news_hash = item["news_hash"]
            score = item["score"]
            source = item["source"]
            language = item["language"]
            trends = json.loads(item["trends"] or "[]")
            
            category = f"write_{language}"
            # Verificar restricciones
            if time.time() < bot_status["twitter_wait_until"].get(category, 0):
                logging.debug(f"No se puede procesar noticia encolada en {language}: En período de espera")
                continue
            if bot_status["daily_tweets_total"] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
                logging.debug(f"No se puede procesar noticia encolada en {language}: Límite diario alcanzado")
                break
            if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
                logging.debug(f"No se puede procesar noticia encolada en {language}: Límite mensual alcanzado")
                break
            last_tweet_time = bot_status.get(f"last_tweet_time_{language}")
            if last_tweet_time and (time.time() - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
                logging.debug(f"No se puede procesar noticia encolada en {language}: En período de enfriamiento")
                continue
            
            logging.info(f"Procesando noticia encolada en {language}: {title[:50]} (score: {score})")
            entry = {
                "title": title,
                "summary": summary,
                "link": url,
                "source": source,
                "is_tweet": False
            }
            
            if await publish_news(session, entry, news_hash, score, language, trends):
                published_count += 1
                bot_status[f"monthly_posts_{language}"] += 1
                bot_status["daily_tweets_total"] += 1
                bot_status[f"last_tweet_time_{language}"] = datetime.now()
                bot_status["recent_processed_news"] += 1
                logging.info(f"Noticia encolada publicada en {language}: {title[:50]}")
                
                # Eliminar de la cola
                await run_db_sync(
                    cursor.execute,
                    "DELETE FROM cola_publicacion WHERE news_hash = ? AND language = ?",
                    (news_hash, language)
                )
                await run_db_sync(conn.commit)
            else:
                logging.warning(f"Fallo al publicar noticia encolada en {language}: {title[:50]}")
        
        await save_bot_state()
        return published_count

async def process_single_news(session: aiohttp.ClientSession, entry: dict, news_hash: str, trends: list[str]) -> bool:
    """
    Procesa una noticia individual, la puntúa y decide si publicarla o encolarla.
    
    Args:
        session: Sesión de aiohttp para solicitudes HTTP.
        entry: Diccionario con los datos de la noticia.
        news_hash: Hash único de la noticia.
        trends: Lista de tendencias actuales.
    
    Returns:
        bool: True si la noticia se publicó en al menos un idioma, False si se encoló o descartó.
    """
    global bot_status
    title = entry.get("title", "Untitled")
    bot_status["last_task"] = f"Processing single news/tweet: {title[:50]}"
    logging.debug(f"Procesando noticia: {title[:50]} (source: {entry.get('source', 'Unknown')})")
    
    score = await score_news(entry, trends)
    if score < (6 if entry.get("is_tweet", False) else 2):
        logging.info(f"{'Tweet' if entry.get('is_tweet', False) else 'News'} '{title[:50]}' descartado por baja puntuación ({score:.1f})")
        return False
    
    published_status = {"en": False, "es": False}
    for language in published_status:
        category = f"write_{language}"
        # Verificar restricciones de publicación
        if time.time() < bot_status["twitter_wait_until"].get(category, 0):
            logging.info(f"No se puede publicar en {language}: En período de espera hasta {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%H:%M:%S')}")
            continue
        if bot_status["daily_tweets_total"] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
            logging.info(f"No se puede publicar en {language}: Límite diario alcanzado ({bot_status['daily_tweets_total']}/{CONFIG['API_LIMITS']['TWEETS_PER_DAY']})")
            continue
        if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
            logging.info(f"No se puede publicar en {language}: Límite mensual alcanzado ({bot_status['monthly_posts_en'] + bot_status['monthly_posts_es']}/{CONFIG['API_LIMITS']['MONTHLY_POSTS_TOTAL']})")
            continue
        last_tweet_time = bot_status.get(f"last_tweet_time_{language}")
        if last_tweet_time and (time.time() - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
            remaining = CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"] - (time.time() - last_tweet_time.timestamp())
            logging.info(f"No se puede publicar en {language}: En período de enfriamiento ({remaining/60:.1f} minutos restantes)")
            continue
        if await is_published_in_language(title, entry.get("summary", ""), entry.get("link"), entry.get("source", "Unknown"), language):
            logging.info(f"Noticia ya publicada en {language}: {title[:50]}")
            published_status[language] = True
            continue
        
        logging.info(f"Intentando publicar noticia en {language}: {title[:50]} (score: {score:.1f})")
        try:
            if await publish_news(session, entry, news_hash, score, language, trends):
                published_status[language] = True
                bot_status[f"monthly_posts_{language}"] += 1
                bot_status["daily_tweets_total"] += 1
                bot_status[f"last_tweet_time_{language}"] = datetime.now()
                logging.info(f"Noticia publicada exitosamente en {language}: {title[:50]}")
            else:
                logging.warning(f"Fallo al publicar noticia en {language}: {title[:50]}. Intentando encolar")
        except Exception as e:
            logging.error(f"Error inesperado publicando en {language}: {title[:50]}: {e}", exc_info=True)
            bot_status["errors"] += 1
    
    if any(published_status.values()):
        source_usage[entry.get("source", "Unknown")] += 1
        bot_status["recent_processed_news"] += 1
        await save_bot_state()
        return True
    
    # Encolar si no se publicó en algún idioma
    for language in published_status:
        if not published_status[language] and not await is_published_in_language(title, entry.get("summary", ""), entry.get("link"), entry.get("source", "Unknown"), language):
            cursor.execute("SELECT 1 FROM cola_publicacion WHERE news_hash = ? AND language = ?", (news_hash, language))
            if not await run_db_sync(cursor.fetchone):
                logging.info(f"Encolando noticia para {language}: {title[:50]}")
                trends_json = json.dumps(trends)
                try:
                    await run_db_sync(
                        cursor.execute,
                        '''INSERT INTO cola_publicacion (title, summary, url, image_data, news_hash, score, source, language, trends)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                        (title, entry.get("summary", ""), entry.get("link"), None, news_hash, score, entry.get("source", "Unknown"), language, trends_json)
                    )
                    await run_db_sync(conn.commit)
                    logging.debug(f"Noticia encolada correctamente para {language}: {title[:50]}")
                except Exception as e:
                    logging.error(f"Error encolando noticia para {language}: {title[:50]}: {e}")
                    bot_status["errors"] += 1
    
    await save_bot_state()
    return False

async def process_rss_feeds(session: aiohttp.ClientSession, trends: list[str]) -> list[dict]:
    """
    Procesa los feeds RSS configurados y retorna una lista de noticias.
    Evita duplicados usando la función is_duplicate.
    Ejecuta feedparser.parse en un hilo separado.
    Modificada para usar asyncio.to_thread y await is_duplicate.
    """
    global bot_status
    bot_status["last_task"] = "Procesando feeds RSS"
    logging.info("Iniciando procesamiento de feeds RSS...")

    rss_news = []
    for feed_url in RSS_FEEDS:
        try:
            # feedparser.parse es síncrono, ejecutar en un hilo separado
            feed = await asyncio.to_thread(feedparser.parse, feed_url)

            if feed.entries:
                logging.debug(f"Procesando {len(feed.entries)} entradas del feed RSS: {feed_url}")
                for entry in feed.entries:
                    title = clean_text(entry.get("title", "Sin título"))
                    link = entry.get("link")
                    summary = clean_text(entry.get("summary", entry.get("description", "")))
                    source = feed.feed.get("title", feed_url) # Usar el título del feed como fuente si está disponible
                    # Asegurarse de que 'link' no sea None antes de usarlo
                    if not link:
                         logging.warning(f"Entrada RSS sin enlace en {feed_url}: {title[:50]}")
                         continue # Omitir entrada sin enlace

                    # Usar el enlace en el hash para asegurar unicidad por entrada del feed
                    news_hash = hashlib.sha256((title + summary + link + source).encode()).hexdigest()

                    # VERIFICAR DUPLICADOS ANTES DE AÑADIR A LA LISTA
                    # is_duplicate ahora se ejecuta en un hilo separado
                    if not await is_duplicate(title, summary, link, source):
                        date_tuple = entry.get("published_parsed") or entry.get("updated_parsed")
                        # Asegurarse de que date_tuple sea válido antes de desempaquetar
                        date = datetime(*date_tuple[:6]) if date_tuple and len(date_tuple) >= 6 else datetime.now() # Usar fecha actual como fallback

                        rss_news.append({
                            "title": title,
                            "link": link,
                            "summary": summary,
                            "source": source,
                            "date": date,
                            "hash": news_hash,
                            "is_tweet": False # Marcar como no-tweet
                        })
                        logging.debug(f"Noticia RSS añadida: {title[:50]}...")
                    else:
                        logging.debug(f"Noticia RSS duplicada detectada y omitida: {title[:50]}...")

            else:
                logging.info(f"No se encontraron entradas en el feed RSS: {feed_url}")

        except Exception as e:
            # Loguear el error procesando un feed específico, pero continuar con los demás
            logging.warning(f"Error procesando RSS {feed_url}: {e}")
            bot_status["errors"] += 1
            await save_bot_state() # Guardar estado si hay un error
            continue # Continuar con el siguiente feed

    logging.info(f"Procesamiento de feeds RSS completado. {len(rss_news)} noticias nuevas obtenidas.")
    return rss_news

async def process_discord_news(channel_id: int, trends: list[str]) -> list[dict]:
    global bot_status
    bot_status["last_task"] = f"Procesando noticias Discord (canal {channel_id})"
    processed_news = []
    try:
        while discord_news[channel_id]:
            entry = discord_news[channel_id].popleft()
            if not await is_duplicate(entry["title"], entry["summary"], entry["link"], entry["source"]):
                processed_news.append(entry)
        logging.info(f"Noticias Discord procesadas (canal {channel_id}): {len(processed_news)}")
    except Exception as e:
        logging.error(f"Error procesando Discord (canal {channel_id}): {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
    return processed_news

async def process_news():
    global bot_status
    bot_status["tasks_running"] += 1
    print_section_header("Procesando Noticias")
    logging.info("Iniciando ciclo de procesamiento de noticias...")

    try:
        current_time = datetime.now()
        current_time_ts = time.time()

        # Resetear contadores diarios
        if current_time.day != bot_status["last_reset"].day:
            bot_status["daily_tweets_total"] = 0
            bot_status["last_reset"] = current_time.replace(hour=0, minute=0, second=0, microsecond=0)
            bot_status["recent_tweets_en"] = []
            bot_status["recent_tweets_es"] = []
            logging.info("Contadores diarios de tweets reseteados")
            save_bot_state_sync(cursor, conn)

        # Resetear contadores mensuales
        if current_time.month != bot_status["monthly_reset"].month:
            bot_status["monthly_posts_en"] = 0
            bot_status["monthly_posts_es"] = 0
            bot_status["x_api_reads_remaining"] = CONFIG["API_LIMITS"]["MONTHLY_READS"]
            bot_status["x_api_writes_remaining"] = CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]
            bot_status["monthly_reset"] = current_time.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
            logging.info("Contadores mensuales reseteados")
            save_bot_state_sync(cursor, conn)

        trends = await get_trending_keywords()
        all_news = []

        async with aiohttp.ClientSession() as session:
            print_section_header("Procesando Fuentes")
            # Procesar Discord
            async with discord_processing_lock:
                for channel_id in [DISCORD_CHANNEL_1, DISCORD_CHANNEL_2]:
                    all_news.extend(await process_discord_news(channel_id, trends))

            # Procesar RSS
            all_news.extend(await process_rss_feeds(session, trends))

            # Procesar Twitter si no está pausado y hay lecturas disponibles
            twitter_read_paused = current_time_ts < bot_status["twitter_wait_until"].get("read", 0)
            if not twitter_read_paused and bot_status["x_api_reads_remaining"] > 0 and await check_api_rate_limit("read"):
                try:
                    twitter_news = await update_twitter_feeds()
                    all_news.extend(twitter_news)
                except Exception as e:
                    logging.warning(f"No se pudieron obtener tweets de Twitter/X: {e}. Continuando con RSS y Discord.")
            else:
                logging.info("Lecturas de Twitter/X pausadas o límite agotado. Usando solo RSS y Discord.")

            if not all_news:
                print("ℹ️ No hay noticias nuevas para procesar en este ciclo.")
                logging.info("No hay noticias nuevas para procesar en este ciclo.")
                return

            # Puntuar y ordenar noticias
            scored_items = []
            for entry in all_news:
                score = await score_news(entry, trends)
                if score > 0:
                    scored_items.append((entry, score))

            scored_items.sort(key=lambda x: (
                x[0].get("is_discord", False),
                not x[0].get("is_tweet", False),
                x[1]
            ), reverse=True)

            # Procesar noticias
            processed_count = 0
            for entry, score in scored_items:
                if await process_single_news(session, entry, entry["hash"], trends):
                    processed_count += 1
                    print(f"✅ Noticia procesada: {entry['title'][:50]} (Puntuación: {score:.1f})")
                    await asyncio.sleep(CONFIG["INTERVALS"]["API_REQUEST_DELAY_SECONDS"])

            bot_status["recent_processed_news"] = processed_count
            logging.info(f"Ciclo de procesamiento de noticias completado. Elementos procesados para publicación: {processed_count}")
            save_bot_state_sync(cursor, conn)

    except Exception as e:
        logging.error(f"❌ Error en process_news: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
    finally:
        bot_status["tasks_running"] -= 1

async def discord_news_processor():
    while True:
        try:
            await asyncio.sleep(CONFIG["INTERVALS"]["DISCORD_PROCESSING_SECONDS"])
            bot_status["last_task"] = "Procesando cola de Discord"
            await process_news()
        except Exception as e:
            logging.error(f"Error en discord_news_processor: {e}", exc_info=True)
            bot_status["errors"] += 1
            save_bot_state_sync(cursor, conn)
            await asyncio.sleep(5)
            

async def heartbeat():
    while True:
        try:
            # Calcular tiempo de actividad
            uptime = datetime.now() - bot_status["uptime"]
            uptime_str = f"{int(uptime.total_seconds() // 86400)}d {int((uptime.total_seconds() % 86400) // 3600)}h {int((uptime.total_seconds() % 3600) // 60)}m"

            # Imprimir encabezado
            print_section_header("Estado del Sistema", color="cyan")  # Cambié a cian para un look más vibrante

            # Información general
            print(f"\033[1;36m🛠️  Sistema Activo: \033[0m{uptime_str}")
            print(f"\033[1;36m🔄 Tarea Actual: \033[0m{bot_status.get('last_task', 'N/A')}")

            # Estado de conexiones
            print(f"\033[1;36m🔗 Conexiones:\033[0m")
            connections = [
                ("Twitter EN", bot_status["twitter_connected_en"]),
                ("Twitter ES", bot_status["twitter_connected_es"]),
                ("SQLite", bot_status["sqlite_connected"]),
                ("Discord", bot_status["discord_connected"])
            ]
            for name, status in connections:
                status_icon = "\033[32m✅\033[0m" if status else "\033[31m❌\033[0m"
                print(f"  {name:<12}: {status_icon}")

            # Estadísticas de tweets
            print(f"\033[1;36m📢 Tweets Publicados:\033[0m")
            print(f"  EN: {bot_status['posted_tweets_en']:<4} | ES: {bot_status['posted_tweets_es']}")
            print_progress_bar(
                bot_status["daily_tweets_total"],
                CONFIG["API_LIMITS"]["TWEETS_PER_DAY"],
                "Uso Diario",
                critical_threshold=80
            )

            # Uso de API
            print(f"\033[1;36m📊 API X:\033[0m")
            api_reads_percent = (bot_status["x_api_reads_remaining"] / CONFIG["API_LIMITS"]["MONTHLY_READS"]) * 100
            print_progress_bar(
                bot_status["x_api_reads_remaining"],
                CONFIG["API_LIMITS"]["MONTHLY_READS"],
                "Lecturas Restantes",
                critical_threshold=20
            )

            # Errores
            error_color = "31" if bot_status["errors"] > 0 else "32"  # Rojo si hay errores, verde si no
            print(f"\033[1;36m⚠️  Errores: \033[{error_color}m{bot_status['errors']}\033[0m")

            # Tiempos de espera
            print(f"\033[1;36m⏳ Restricciones:\033[0m")
            current_time = time.time()
            any_restrictions = False
            for key, wait_until in bot_status["twitter_wait_until"].items():
                if current_time < wait_until:
                    wait_time = wait_until - current_time
                    mode = {"read": "Lectura", "write_en": "Escritura EN", "write_es": "Escritura ES"}.get(key, key)
                    print(f"  {mode:<12}: Pausada por {wait_time:.0f}s")
                    any_restrictions = True
            if not any_restrictions:
                print("  Ninguna activa")

            # Separador final
            print(f"\033[1;34m{'─' * 60}\033[0m\n")

            # Esperar hasta el próximo heartbeat
            await asyncio.sleep(CONFIG["INTERVALS"]["HEARTBEAT_SECONDS"])
        except Exception as e:
            logging.error(f"\033[31m❌ Error en heartbeat: {e}\033[0m", exc_info=True)
            bot_status["errors"] += 1
            save_bot_state_sync(cursor, conn)
            await asyncio.sleep(5)

async def run_discord():
    try:
        await discord_bot.start(DISCORD_TOKEN)
    except Exception as e:
        logging.error(f"Error iniciando Discord: {e}", exc_info=True)
        bot_status["discord_connected"] = False
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)

async def main():
    global bot_status
    print_section_header("Iniciando Bot")
    logging.info("Iniciando bot...")
    bot_status["last_task"] = "Iniciando bot"
    
    # Limpiar imágenes temporales antiguas
    temp_dir = CONFIG["PATHS"]["TEMP_IMAGE_DIR"]
    for filename in os.listdir(temp_dir):
        file_path = os.path.join(temp_dir, filename)
        try:
            if os.path.isfile(file_path):
                os.unlink(file_path)
                logging.debug(f"Eliminado archivo temporal antiguo: {file_path}")
        except Exception as e:
            logging.warning(f"Error eliminando {file_path}: {e}")
    
    scheduler = AsyncIOScheduler()
    scheduler.add_job(process_news, IntervalTrigger(minutes=CONFIG["INTERVALS"]["PROCESS_NEWS_MINUTES"]), misfire_grace_time=300)
    scheduler.add_job(update_twitter_feeds, IntervalTrigger(minutes=CONFIG["INTERVALS"]["UPDATE_TWITTER_FEEDS_MINUTES"]), misfire_grace_time=300)
    scheduler.add_job(process_queue, IntervalTrigger(minutes=CONFIG["INTERVALS"]["QUEUE_PROCESSING_MINUTES"]), misfire_grace_time=300)
    
    # Programar tarea para el mapa del tiempo
    async def scheduled_weather_post():
        async with aiohttp.ClientSession() as session:
            await post_weather_map(session)
    scheduler.add_job(scheduled_weather_post, CronTrigger(hour=CONFIG["WEATHER"]["POST_HOUR"], minute=0), misfire_grace_time=600)
    
    loop = asyncio.get_event_loop()
    loop.create_task(run_discord())
    loop.create_task(heartbeat())
    loop.create_task(discord_news_processor())
    scheduler.start()
    
    logging.info("Esperando 10 segundos para estabilizar conexiones iniciales...")
    await asyncio.sleep(10)
    
    while True:
        try:
            await asyncio.sleep(3600)
            bot_status["last_task"] = "Esperando en bucle principal"
            save_bot_state_sync(cursor, conn)
        except Exception as e:
            logging.error(f"Error en bucle principal: {e}")
            bot_status["errors"] += 1
            save_bot_state_sync(cursor, conn)
            await asyncio.sleep(5)

if __name__ == "__main__":
    asyncio.run(main())
