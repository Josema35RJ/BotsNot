import collections
from email.utils import parsedate_to_datetime
import html
import logging
import random
from typing import Dict, Optional
import colorlog
from cachetools import TTLCache
from logging import handlers
from flask import app
from httpcore import TimeoutException
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from functools import partial
import feedparser
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import tweepy
import sqlite3
import hashlib
from datetime import datetime, timedelta
import os
from dotenv import load_dotenv
import asyncio
from concurrent.futures import ThreadPoolExecutor
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.triggers.cron import CronTrigger
from PIL import Image
import io
import aiofiles
import discord
from discord.ext import commands
import re
from collections import Counter, deque, defaultdict
import aiohttp
import time
import sys
from transformers import MarianTokenizer, MarianMTModel
import torch
from feedgen.feed import FeedGenerator
import urllib
import json

# Configuración de eventos para Windows
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# Cargar variables de entorno
load_dotenv(dotenv_path="datos.env")

queue = []  # Cola global para reintentos

# Configuración centralizada
CONFIG = {
    "API_LIMITS": {
        "MONTHLY_READS": 10000,
        "REQUESTS_PER_WINDOW": 180, # Mantener este valor, Tweepy lo usa internamente
        "RATE_LIMIT_WINDOW_SECONDS": 900,  # Mantener este valor (15 minutos)
        "TWEETS_PER_DAY": 50, # Reducir el límite diario para no agotar el mensual rápido
        "MONTHLY_POSTS_TOTAL": 1500  # Reducir el límite mensual total (1500 es el máximo, mejor dejar margen)
    },
    "INTERVALS": {
        "PROCESS_NEWS_MINUTES": 5,  # Tarea principal de procesamiento y potencial publicación. Se ejecuta cada 58 minutos.
        "CHECK_HEALTH_MINUTES": 60, # Mantener la verificación de salud
        "UPDATE_TWITTER_FEEDS_MINUTES": 2880, # Ejemplo: Cambiado de 720 a 1440 (24 horas) - Actualiza feeds de Twitter/X (solo lectura)
        "HEARTBEAT_SECONDS": 300, # Mantener el heartbeat
        "TWEET_COOLDOWN_SECONDS": 5,  # 60 MINUTOS entre tweets del mismo idioma. (Anteriormente el comentario decía 45 min)
        "DISCORD_MESSAGE_COOLDOWN_SECONDS": 10, # Mantener el cooldown de mensajes de Discord

        # --- AJUSTES PROPUESTOS PARA REDUCIR MENSAJES DE "PERÍODO DE ENFRIAMIENTO" ---
        # La tarea `discord_news_processor` llama a `process_news`.
        # `process_news` también está programada para ejecutarse cada `PROCESS_NEWS_MINUTES`.
        # Para evitar que `process_news` se ejecute con demasiada frecuencia debido a `discord_news_processor`,
        # ajustamos `DISCORD_PROCESSING_SECONDS` para que sea similar o mayor que `PROCESS_NEWS_MINUTES`.
        "DISCORD_PROCESSING_SECONDS": 3600, # ANTERIORMENTE: 30. AHORA: 3600 segundos (60 minutos).
                                            # Esto alinea la frecuencia de llamada de `process_news` desde `discord_news_processor`
                                            # con su propia programación, haciendo que `process_news` se ejecute aproximadamente cada 58-60 minutos.

        "API_REQUEST_DELAY_SECONDS": 3600, # Ejemplo: Cambiado de 90 a 120 - Pausa entre procesar diferentes cuentas de Twitter en update_twitter_feeds (no afecta frecuencia de publicación directa de noticias)

        # La cola de publicación (`process_queue` y `review_queue`) intenta publicar tweets que ya fueron seleccionados
        # y están en la base de datos. Si el cooldown principal es de 1 hora, y `process_news` (que añade a la cola)
        # se ejecuta cada ~1 hora, no tiene sentido procesar la cola cada minuto.
        "QUEUE_PROCESSING_MINUTES": 3000,  # ANTERIORMENTE: 1. AHORA: 55 minutos.
                                        # Esto permite que los elementos encolados se intenten publicar poco antes
                                        # del próximo ciclo principal de `process_news`, respetando mejor el cooldown.
    },
    "PATHS": {
        "RSS_CACHE_DIR": "rss_cache",
        "TEMP_IMAGE_DIR": "optimized_images",
        "CACHE_DIR": "cache",
        "DB_FILE": "bot.db",
        "DETAILED_LOG": "bot_detailed.log",
        "ERROR_LOG": "bot_errors.log",
    },
    "TWEET": {
        "MAX_LENGTH": 280,
    },
    "STORAGE": {
        "PERSISTENT_IMAGE_DIR": "persistent_images/",  # Imágenes permanentes
        "TEMP_IMAGE_DIR": "temp_images/"              # Imágenes temporales
    }
}
# Asegurar que los directorios existan
os.makedirs(CONFIG["STORAGE"]["PERSISTENT_IMAGE_DIR"], exist_ok=True)
os.makedirs(CONFIG["STORAGE"]["TEMP_IMAGE_DIR"], exist_ok=True)

# Configuración de logging
handler = colorlog.StreamHandler(stream=sys.stdout)
handler.setFormatter(colorlog.ColoredFormatter(
    '%(log_color)s%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S',
    log_colors={
        'DEBUG': 'cyan',
        'INFO': 'green',
        'WARNING': 'yellow',
        'ERROR': 'red',
        'CRITICAL': 'bold_red',
    }
))
handler.stream.reconfigure(encoding='utf-8')

logging.basicConfig(
    level=logging.INFO,
    handlers=[
        handlers.RotatingFileHandler(CONFIG["PATHS"]["DETAILED_LOG"], maxBytes=512*1024, backupCount=30, encoding='utf-8'),
        handler
    ]
)
error_handler = handlers.RotatingFileHandler(CONFIG["PATHS"]["ERROR_LOG"], maxBytes=512*1024, backupCount=10, encoding='utf-8')
error_handler.setLevel(logging.ERROR)
logging.getLogger().addHandler(error_handler)
logging.getLogger('apscheduler').setLevel(logging.WARNING)

# Variables de entorno para las APIs
TWITTER_API_KEY_EN = os.getenv("TWITTER_API_KEY_EN")
TWITTER_API_SECRET_EN = os.getenv("TWITTER_API_SECRET_EN")
TWITTER_ACCESS_TOKEN_EN = os.getenv("TWITTER_ACCESS_TOKEN_EN")
TWITTER_ACCESS_SECRET_EN = os.getenv("TWITTER_ACCESS_SECRET_EN")
TWITTER_BEARER_TOKEN_EN = os.getenv("TWITTER_BEARER_TOKEN_EN")

TWITTER_API_KEY_ES = os.getenv("TWITTER_API_KEY_ES")
TWITTER_API_SECRET_ES = os.getenv("TWITTER_API_SECRET_ES")
TWITTER_ACCESS_TOKEN_ES = os.getenv("TWITTER_ACCESS_TOKEN_ES")
TWITTER_ACCESS_SECRET_ES = os.getenv("TWITTER_ACCESS_SECRET_ES")

DISCORD_TOKEN = os.getenv("DISCORD_TOKEN")
DISCORD_CHANNEL_1 = int(os.getenv("DISCORD_CHANNEL_1"))
DISCORD_CHANNEL_2 = int(os.getenv("DISCORD_CHANNEL_2"))

duplicate_cache = TTLCache(maxsize=1000, ttl=3600)  # Cachear por 1 hora

# Lista de fuentes RSS
RSS_FEEDS = [
    "https://store.epicgames.com/es-ES/news",
    "https://www.gameinformer.com/rss.xml",
    "https://www.engadget.com/gaming/rss.xml",
    "https://www.gamespot.com/feeds/news/",
    "https://blog.playstation.com/feed/",
    "https://www.engadget.com/tech/rss.xml",
    "https://kotaku.com/rss",
    "https://www.polygon.com/rss/index.xml",
    "https://www.engadget.com/rss.xml",
    "https://www.gematsu.com/feed",
    "https://www.pcgamer.com/rss",
    "https://www.gameranx.com/feed/",
    "https://www.ubisoft.com/en-us/company/newsroom/rss",
    "https://www.steampowered.com/news/feed",
    "https://www.gog.com/news/feed",
]

# Lista de cuentas de Twitter/X
TWITTER_ACCOUNTS = [
    "NoMansSky",
    "Jorge_Most_",
    "ControlMision",
    "RockstarGames",
    "NVIDIAAI",
    "PlayStation",
    "Xbox",
    "NintendoAmerica",
    "Bethesda",
    "Steam",
    "CallofDuty",
    "XboxGamePass",
    "PlayStationES",
]

KEYWORDS_EN = {
    "gaming": {
        # Términos generales
        "game": 0.8,
        "games": 0.8,
        "gaming": 0.9,
        "video game": 0.8,
        "gamer": 0.8,
        "play": 0.7,
        "player": 0.7,
        "multiplayer": 0.8,
        "singleplayer": 0.8,
        # Plataformas y consolas
        "nintendo": 1.0,
        "playstation": 1.0,
        "xbox": 1.0,
        "pc": 0.9,
        "console": 0.9,
        "switch": 1.0,
        "ps5": 1.0,
        "xbox series": 1.0,
        "steam": 0.9,
        "epic": 0.9,
        "origin": 0.8,
        "battlenet": 0.8,
        "uplay": 0.8,
        "gamepass": 0.9,
        "stadia": 0.8,
        "luna": 0.8,
        # Compañías y desarrolladores
        "activision": 0.9,
        "blizzard": 0.9,
        "ea": 0.9,
        "electronic arts": 0.9,
        "ubisoft": 0.9,
        "square enix": 0.9,
        "bethesda": 0.9,
        "rockstar": 0.9,
        "cd projekt": 0.9,
        "fromsoftware": 0.8,
        "capcom": 0.8,
        "sega": 0.8,
        "konami": 0.8,
        "bandai namco": 0.8,
        # Franquicias y juegos populares
        "gta": 1.0,
        "grand theft auto": 1.0,
        "call of duty": 1.0,
        "fifa": 1.0,
        "madden": 0.9,
        "nba 2k": 0.9,
        "assassin's creed": 1.0,
        "zelda": 1.0,
        "mario": 1.0,
        "pokemon": 1.0,
        "doom": 1.0,
        "witcher": 1.0,
        "cyberpunk": 1.0,
        "elden ring": 1.0,
        "final fantasy": 1.0,
        "resident evil": 1.0,
        "halo": 1.0,
        "gears": 0.9,
        "forza": 0.9,
        "starfield": 0.9,
        "baldur's gate": 1.0,
        "diablo": 1.0,
        "overwatch": 0.9,
        "fortnite": 1.0,
        "apex legends": 0.9,
        "valorant": 0.9,
        "league of legends": 1.0,
        "dota": 0.9,
        "smash bros": 1.0,
        "street fighter": 0.9,
        "tekken": 0.9,
        "mortal kombat": 0.9,
        "star citizen": 0.9,
        "hello kitty": 0.8,
        "infinity nikki": 0.9,
        # Tecnologías y accesorios
        "vr": 0.8,
        "virtual reality": 0.8,
        "ar": 0.8,
        "augmented reality": 0.8,
        "controller": 0.8,
        "joycon": 0.8,
        "headset": 0.8,
        "motion control": 0.7,
        "ray tracing": 0.8,
        "dlss": 0.8,
        "fsr": 0.8,
        # Contenido y eventos
        "dlc": 0.8,
        "expansion": 0.8,
        "mod": 0.7,
        "mods": 0.7,
        "esports": 0.8,
        "tournament": 0.8,
        "stream": 0.7,
        "streaming": 0.7,
        "twitch": 0.8,
        "youtube gaming": 0.8,
        # Géneros y mecánicas
        "rpg": 0.8,
        "fps": 0.8,
        "shooter": 0.8,
        "battle royale": 0.8,
        "moba": 0.8,
        "strategy": 0.8,
        "simulator": 0.8,
        "puzzle": 0.7,
        "adventure": 0.7,
        "open world": 0.8
    },
    "tech": {
        # Términos generales
        "technology": 0.8,
        "tech": 0.8,
        "innovation": 0.7,
        "digital": 0.7,
        "device": 0.7,
        "gadget": 0.7,
        # Compañías
        "apple": 1.0,
        "google": 1.0,
        "microsoft": 1.0,
        "amazon": 1.0,
        "meta": 1.0,
        "tesla": 1.0,
        "intel": 0.9,
        "amd": 0.9,
        "nvidia": 0.9,
        "qualcomm": 0.8,
        "samsung": 0.9,
        "sony": 0.9,
        "dell": 0.9,
        "hp": 0.8,
        "lenovo": 0.8,
        "asus": 0.8,
        "acer": 0.8,
        # Tecnologías
        "ai": 1.2,
        "artificial intelligence": 1.2,
        "machine learning": 1.0,
        "deep learning": 1.0,
        "neural network": 0.9,
        "npu": 0.9,
        "gpu": 0.9,
        "cpu": 0.9,
        "hardware": 0.9,
        "software": 0.9,
        "cloud": 0.9,
        "cloud computing": 0.9,
        "cybersecurity": 0.9,
        "encryption": 0.8,
        "blockchain": 0.8,
        "crypto": 0.8,
        "cryptocurrency": 0.8,
        "nft": 0.8,
        "quantum": 0.8,
        "quantum computing": 0.8,
        "5g": 0.9,
        "6g": 0.8,
        "wifi": 0.8,
        "iot": 0.8,
        "internet of things": 0.8,
        "robotics": 0.8,
        "drone": 0.8,
        "autonomous": 0.8,
        "self-driving": 0.8,
        "ar": 0.8,
        "vr": 0.8,
        "metaverse": 0.8,
        # Productos y categorías
        "smartphone": 0.9,
        "tablet": 0.8,
        "laptop": 0.9,
        "desktop": 0.8,
        "monitor": 0.8,
        "ssd": 0.8,
        "ram": 0.8,
        "processor": 0.8,
        "chip": 0.8,
        "semiconductor": 0.8,
        "battery": 0.8,
        "wearable": 0.8,
        "smartwatch": 0.8,
        "earbuds": 0.8,
        # Negocios y tendencias
        "startup": 0.8,
        "venture": 0.7,
        "funding": 0.7,
        "ipo": 0.7,
        "acquisition": 0.8,
        "merger": 0.8
    },
    "news_indicators": [
        # Anuncios y lanzamientos
        "release",
        "launch",
        "debut",
        "unveil",
        "reveal",
        "announcement",
        "announce",
        "introduced",
        "rolled out",
        "shipped",
        "available",
        "preorder",
        "order",
        "sale",
        "drop",
        # Actualizaciones
        "update",
        "patch",
        "upgrade",
        "fix",
        "hotfix",
        "overhaul",
        "refresh",
        "new version",
        # Eventos
        "event",
        "showcase",
        "conference",
        "stream",
        "livestream",
        "broadcast",
        "expo",
        "convention",
        "direct",
        "state of play",
        "e3",
        "gamescom",
        "blizzcon",
        # Noticias generales
        "news",
        "breaking",
        "report",
        "leak",
        "rumor",
        "speculation",
        "confirmed",
        "official",
        "statement",
        "notice",
        # Promociones y exclusivas
        "free",
        "deal",
        "discount",
        "sale",
        "bundle",
        "exclusive",
        "limited",
        "special",
        "promo",
        "offer",
        # Pruebas y desarrollo
        "beta",
        "alpha",
        "demo",
        "trial",
        "test",
        "preview",
        "early access",
        # Cambios y ajustes
        "rotation",
        "change",
        "tweak",
        "balance",
        "nerf",
        "buff",
        # Otros
        "delay",
        "postponed",
        "canceled",
        "shutdown",
        "end",
        "retired"
    ]
}

# Estructuras de datos
trending_keywords = deque(maxlen=50)
source_usage = defaultdict(int)
image_cache = {}
url_cache = {}
translation_cache = {}
discord_processing_lock = asyncio.Lock()
user_id_cache = {}  # Caché para IDs de usuarios de Twitter
bot_status = {
    "twitter_connected_en": False,
    "twitter_connected_es": False,
    "sqlite_connected": False,
    "discord_connected": False,
    "tasks_running": 0,
    "last_task": "Idle",
    "processed_news": 0,
    "recent_processed_news": 0,
    "posted_tweets_en": 0,
    "posted_tweets_es": 0,
    "errors": 0,
    "uptime": datetime.now(),
    "daily_tweets_total": 0,
    "monthly_posts_en": 0,
    "monthly_posts_es": 0,
    "last_reset": datetime.now().replace(hour=0, minute=0, second=0, microsecond=0),
    "monthly_reset": datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0),
    "last_tweet_time_en": None,
    "last_tweet_time_es": None,
    "x_api_reads_remaining": CONFIG["API_LIMITS"]["MONTHLY_READS"],
    "last_x_api_reset": datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0),
    "api_request_count": 0,
    "api_window_start": datetime.now(),
    "twitter_wait_until": {"read": 0, "write_en": 0, "write_es": 0}  # Tiempos de espera por categoría e idioma
}

# Crear directorios
for dir_path in [CONFIG["PATHS"]["RSS_CACHE_DIR"], CONFIG["PATHS"]["TEMP_IMAGE_DIR"], CONFIG["PATHS"]["CACHE_DIR"]]:
    os.makedirs(dir_path, exist_ok=True)

# Inicializar modelo de traducción local
translation_tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-es")
translation_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-es")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
translation_model.to(device)

# Conexión a Twitter
auth_en = tweepy.OAuthHandler(TWITTER_API_KEY_EN, TWITTER_API_SECRET_EN)
auth_en.set_access_token(TWITTER_ACCESS_TOKEN_EN, TWITTER_ACCESS_SECRET_EN)
twitter_api_en = tweepy.API(auth_en, wait_on_rate_limit=True)
twitter_client_en = tweepy.Client(
    bearer_token=TWITTER_BEARER_TOKEN_EN,
    consumer_key=TWITTER_API_KEY_EN,
    consumer_secret=TWITTER_API_SECRET_EN,
    access_token=TWITTER_ACCESS_TOKEN_EN,
    access_token_secret=TWITTER_ACCESS_SECRET_EN
)

auth_es = tweepy.OAuthHandler(TWITTER_API_KEY_ES, TWITTER_API_SECRET_ES)
auth_es.set_access_token(TWITTER_ACCESS_TOKEN_ES, TWITTER_ACCESS_SECRET_ES)
twitter_api_es = tweepy.API(auth_es, wait_on_rate_limit=True)
twitter_client_es = tweepy.Client(
    consumer_key=TWITTER_API_KEY_ES,
    consumer_secret=TWITTER_API_SECRET_ES,
    access_token=TWITTER_ACCESS_TOKEN_ES,
    access_token_secret=TWITTER_ACCESS_SECRET_ES
)

# Verificar conexión a Twitter
try:
    twitter_client_en.get_me()
    logging.info("Twitter EN conectado exitosamente")
    bot_status["twitter_connected_en"] = True
except Exception as e:
    logging.error(f"Error conectando a Twitter EN: {e}")
    bot_status["twitter_connected_en"] = False

try:
    twitter_client_es.get_me()
    logging.info("Twitter ES conectado exitosamente")
    bot_status["twitter_connected_es"] = True
except Exception as e:
    logging.error(f"Error conectando a Twitter ES: {e}")
    bot_status["twitter_connected_es"] = False

# Semáforos y eventos para control de concurrencia y límites de tasa
read_semaphore = asyncio.Semaphore(3)
write_semaphore_en = asyncio.Semaphore(1)
write_semaphore_es = asyncio.Semaphore(1)
read_rate_limit_event = asyncio.Event()
write_rate_limit_event_en = asyncio.Event()
write_rate_limit_event_es = asyncio.Event()
read_rate_limit_event.set()  # Inicialmente no pausado
write_rate_limit_event_en.set()
write_rate_limit_event_es.set()

def print_section_header(title, color="blue"):
    """Imprime un encabezado de sección con estilo futurista."""
    color_codes = {
        "blue": "34",
        "green": "32",
        "red": "31",
        "cyan": "36",
        "magenta": "35"
    }
    color_code = color_codes.get(color, "34")  # Default a azul si el color no está definido
    print(f"\033[1;{color_code}m{'═' * 60}\033[0m")
    print(f"\033[1;{color_code}m🚀 {title:^56} 🚀\033[0m")
    print(f"\033[1;{color_code}m{'═' * 60}\033[0m")

def print_progress_bar(current, total, label, critical_threshold=80):
    """Imprime una barra de progreso con colores según el porcentaje."""
    percent = (current / total) * 100 if total > 0 else 0
    bar_length = 40
    filled = int(bar_length * percent / 100)
    bar = '█' * filled + '─' * (bar_length - filled)
    color = "32" if percent < critical_threshold else "31"  # Verde si <80%, rojo si ≥80%
    print(f"\033[1m{label:<20}\033[0m \033[{color}m[{bar}] {percent:>5.1f}%\033[0m")

def get_dynamic_update_interval(account=None):
    """
    Calcula un intervalo dinámico para actualizar feeds de Twitter/X basado en lecturas restantes y prioridad de la cuenta.
    
    Args:
        account: Nombre de la cuenta de Twitter/X (opcional).
    
    Returns:
        float: Intervalo en minutos.
    """
    base_interval = CONFIG["INTERVALS"]["UPDATE_TWITTER_FEEDS_MINUTES"]
    remaining_reads = bot_status.get("x_api_reads_remaining", CONFIG["API_LIMITS"]["MONTHLY_READS"])
    total_reads = CONFIG["API_LIMITS"]["MONTHLY_READS"]
    adjustment_factor = max(1, total_reads / max(remaining_reads, 1))
    
    # Conservar lecturas cuando son bajas
    if remaining_reads < 1000:
        adjustment_factor *= 4  # Cuadruplicar intervalo
    elif remaining_reads < 5000:
        adjustment_factor *= 2  # Duplicar intervalo
    
    dynamic_interval = base_interval * adjustment_factor
    max_interval = 720  # Máximo de 12 horas
    
    # Priorizar cuentas importantes
    priority_accounts = [   "NoMansSky",
    "Jorge_Most_",
    "ControlMision",
    "RockstarGames",
    "NVIDIAAI",
    "PlayStation",
    "Xbox",
    "NintendoAmerica",
    "Bethesda",
    "Steam",
    "CallofDuty",
    "XboxGamePass",
    "PlayStationES"]
    if account in priority_accounts and remaining_reads > 1000:
        dynamic_interval *= 0.5  # Reducir intervalo
    
    return min(dynamic_interval, max_interval)


def normalize_text_for_duplicate_check(text: str) -> str:
    """
    Normaliza el texto (título/resumen) para la verificación de duplicados:
    minúsculas, elimina puntuación y espacios extra.
    """
    if not isinstance(text, str):
        return ""
    # Eliminar puntuación y convertir a minúsculas
    text = re.sub(r'[^\w\s]', '', text).lower()
    # Eliminar espacios extra
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Conexión a SQLite3 y persistencia de estado
def connect_sqlite():
    """
    Establece la conexión a la base de datos SQLite.
    Modificada para ser llamada al inicio del script.
    """
    global bot_status, conn, cursor
    try:
        conn = sqlite3.connect(CONFIG["PATHS"]["DB_FILE"], check_same_thread=False)
        cursor = conn.cursor()

        # Crear tablas si no existen
        cursor.execute('''CREATE TABLE IF NOT EXISTS historial (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            hash TEXT,
                            title TEXT,
                            url TEXT,
                            tweet TEXT,
                            relevance REAL,
                            source TEXT,
                            date TEXT,
                            engagement INTEGER,
                            summary TEXT,
                            language TEXT,
                            link TEXT,
                            UNIQUE(hash, language)
                        )''')
        cursor.execute('''CREATE INDEX IF NOT EXISTS idx_hash ON historial (hash)''')

        cursor.execute('''CREATE TABLE IF NOT EXISTS bot_state (
                            key TEXT PRIMARY KEY,
                            value TEXT
                        )''')

        # Tabla cola_publicacion con image_url en lugar de image_data
        cursor.execute('''CREATE TABLE IF NOT EXISTS cola_publicacion (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            title TEXT,
                            summary TEXT,
                            url TEXT,
                            image_url TEXT,
                            news_hash TEXT,
                            score REAL,
                            source TEXT,
                            language TEXT,
                            trends TEXT,
                            added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )''')

        conn.commit()
        bot_status["sqlite_connected"] = True
        return conn, cursor
    except Exception as e:
        logging.error(f"Error conectando a SQLite3: {e}", exc_info=True)
        bot_status["sqlite_connected"] = False
        bot_status["errors"] += 1
        raise
    
async def run_db_sync(func, *args):
    """
    Helper para ejecutar funciones síncronas de base de datos en un ThreadPoolExecutor.
    """
    loop = asyncio.get_event_loop()
    # Ejecuta la función síncrona 'func' con argumentos '*args' en un hilo del pool por defecto.
    return await loop.run_in_executor(None, func, *args)

async def save_bot_state():
    """
    Función asíncrona para guardar el estado del bot usando el helper run_db_sync.
    """
    await run_db_sync(save_bot_state_sync, cursor, conn)

def load_bot_state_sync(cursor):
    """
    Carga el estado del bot desde la base de datos (versión síncrona).
    Diseñada para ejecutarse en un hilo separado o al inicio del script.
    """
    global bot_status
    try:
        cursor.execute("SELECT key, value FROM bot_state")
        state = dict(cursor.fetchall())

        bot_status["posted_tweets_en"] = int(state.get("posted_tweets_en", 0))
        bot_status["posted_tweets_es"] = int(state.get("posted_tweets_es", 0))
        bot_status["daily_tweets_total"] = int(state.get("daily_tweets_total", 0))
        bot_status["monthly_posts_en"] = int(state.get("monthly_posts_en", 0))
        bot_status["monthly_posts_es"] = int(state.get("monthly_posts_es", 0))
        bot_status["errors"] = int(state.get("errors", 0))
        bot_status["x_api_reads_remaining"] = int(state.get("x_api_reads_remaining", CONFIG["API_LIMITS"]["MONTHLY_READS"]))
        bot_status["api_request_count"] = int(state.get("api_request_count", 0))

        def parse_date(value, default):
            if not value:
                return default
            try:
                if isinstance(value, (int, float)):
                    return datetime.fromtimestamp(value)
                return datetime.fromisoformat(value)
            except (ValueError, TypeError):
                logging.warning(f"Valor de fecha inválido: {value}. Usando predeterminado.")
                return default

        bot_status["last_reset"] = parse_date(
            state.get("last_reset"),
            datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        )
        bot_status["monthly_reset"] = parse_date(
            state.get("monthly_reset"),
            datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        )
        bot_status["last_x_api_reset"] = parse_date(
            state.get("last_x_api_reset"),
            datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        )
        bot_status["api_window_start"] = parse_date(
            state.get("api_window_start"),
            datetime.now()
        )

        bot_status["last_tweet_time_en"] = parse_date(state.get("last_tweet_time_en"), None) if state.get("last_tweet_time_en") else None
        bot_status["last_tweet_time_es"] = parse_date(state.get("last_tweet_time_es"), None) if state.get("last_tweet_time_es") else None

        bot_status["twitter_wait_until"]["read"] = float(state.get("twitter_read_wait_until", 0))
        bot_status["twitter_wait_until"]["write_en"] = float(state.get("twitter_write_wait_en", 0))
        bot_status["twitter_wait_until"]["write_es"] = float(state.get("twitter_write_wait_es", 0))

        logging.info("Estado del bot cargado correctamente")
    except Exception as e:
        logging.error(f"Error cargando estado del bot: {e}", exc_info=True)
        bot_status["last_reset"] = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        bot_status["monthly_reset"] = datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        bot_status["last_x_api_reset"] = datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        bot_status["api_window_start"] = datetime.now()
        bot_status["last_tweet_time_en"] = None
        bot_status["last_tweet_time_es"] = None

def save_bot_state_sync(cursor, conn):
    """
    Guarda el estado del bot en la base de datos (versión síncrona).
    Diseñada para ejecutarse en un hilo separado.
    """
    global bot_status
    try:
        state = {
            "posted_tweets_en": str(bot_status["posted_tweets_en"]),
            "posted_tweets_es": str(bot_status["posted_tweets_es"]),
            "daily_tweets_total": str(bot_status["daily_tweets_total"]),
            "monthly_posts_en": str(bot_status["monthly_posts_en"]),
            "monthly_posts_es": str(bot_status["monthly_posts_es"]),
            "errors": str(bot_status["errors"]),
            "x_api_reads_remaining": str(bot_status["x_api_reads_remaining"]),
            "api_request_count": str(bot_status["api_request_count"]),
            "last_reset": bot_status["last_reset"].isoformat() if isinstance(bot_status["last_reset"], datetime) else datetime.now().replace(hour=0, minute=0, second=0, microsecond=0).isoformat(),
            "monthly_reset": bot_status["monthly_reset"].isoformat() if isinstance(bot_status["monthly_reset"], datetime) else datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0).isoformat(),
            "last_x_api_reset": bot_status["last_x_api_reset"].isoformat() if isinstance(bot_status["last_x_api_reset"], datetime) else datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0).isoformat(),
            "api_window_start": bot_status["api_window_start"].isoformat() if isinstance(bot_status["api_window_start"], datetime) else datetime.now().isoformat(),
            "last_tweet_time_en": bot_status["last_tweet_time_en"].isoformat() if isinstance(bot_status["last_tweet_time_en"], datetime) else "",
            "last_tweet_time_es": bot_status["last_tweet_time_es"].isoformat() if isinstance(bot_status["last_tweet_time_es"], datetime) else "",
            "twitter_read_wait_until": str(bot_status["twitter_wait_until"]["read"]),
            "twitter_write_wait_en": str(bot_status["twitter_wait_until"]["write_en"]),
            "twitter_write_wait_es": str(bot_status["twitter_wait_until"]["write_es"]),
        }
        for key, value in state.items():
            cursor.execute("INSERT OR REPLACE INTO bot_state (key, value) VALUES (?, ?)", (key, value))
        conn.commit()
        logging.debug("Estado del bot guardado")
    except Exception as e:
        logging.error(f"Error guardando estado del bot: {e}", exc_info=True)

conn, cursor = connect_sqlite()
load_bot_state_sync(cursor)

# Configuración de Discord
intents = discord.Intents.default()
intents.messages = True
intents.message_content = True
discord_bot = commands.Bot(command_prefix="recuperar", intents=intents)
discord_news = {DISCORD_CHANNEL_1: deque(maxlen=200), DISCORD_CHANNEL_2: deque(maxlen=200)}
last_message_time = 0

@discord_bot.event
async def on_ready():
    global bot_status
    logging.info(f"Discord conectado como {discord_bot.user}")
    bot_status["discord_connected"] = True

@discord_bot.event
async def on_message(message):
    global last_message_time, bot_status, discord_news
    bot_status["last_task"] = f"Procesando mensaje de Discord (canal {message.channel.id})"

    try:
        if message.author == discord_bot.user or message.channel.id not in [DISCORD_CHANNEL_1, DISCORD_CHANNEL_2]:
            return

        await discord_bot.process_commands(message)

        current_time = time.time()
        if current_time - last_message_time < CONFIG["INTERVALS"]["DISCORD_MESSAGE_COOLDOWN_SECONDS"]:
            return
        last_message_time = current_time

        is_from_bot = message.author.bot
        source = f"discord_bot_{message.author.name}" if is_from_bot else f"discord_{message.author.name}"

        urls = re.findall(
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
            message.content or ""
        )

        is_retweet = message.content.startswith("RT @")

        if message.embeds:
            for embed in message.embeds:
                title = embed.title or "Sin título"
                summary = embed.description or message.content or "Sin descripción"
                url = embed.url or None

                if not url and embed.description:
                    embed_urls = re.findall(
                        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
                        embed.description or ""
                    )
                    url = embed_urls[0] if embed_urls else f"discord://{message.channel.id}/{message.id}"

                image_url = embed.image.url if embed.image else None
                if not image_url and url:  # Solo usar url, que ya está definida
                    async with aiohttp.ClientSession() as session:
                        image_url = await get_image_from_url(session, url)

                if await is_duplicate(title, summary, url, source):
                    logging.info(f"Duplicado detectado en embed: {title[:50]}")
                    continue

                news_hash = hashlib.sha256((title + summary + url + source).encode()).hexdigest()
                discord_news[message.channel.id].append({
                    "title": title,
                    "link": url,
                    "summary": summary,
                    "source": source,
                    "date": datetime.now(),
                    "hash": news_hash,
                    "image_url": image_url,
                    "is_discord": True,
                    "is_retweet": is_retweet
                })
                logging.info(f"Noticia añadida desde {source}: {title[:50]} - {url}")

        elif urls:
            content = " ".join((message.content or "").lower().split())
            title = message.content[:100] or "Mensaje con enlace"
            summary = message.content or ""

            for url in urls:
                if await is_duplicate(title, summary, url, source):
                    logging.info(f"Duplicado detectado en URL: {title[:50]}")
                    continue

                async with aiohttp.ClientSession() as session:
                    try:
                        async with session.head(url, timeout=aiohttp.ClientTimeout(total=5)) as response:
                            if response.status != 200:
                                logging.warning(f"URL no válida para '{title}': {url}")
                                continue
                    except Exception as e:
                        logging.warning(f"Error validando URL para '{title}': {e}")
                        continue

                image_url = None
                if not image_url:
                    async with aiohttp.ClientSession() as session:
                        image_url = await get_image_from_url(session, url)

                news_hash = hashlib.sha256((title + summary + url + source).encode()).hexdigest()
                discord_news[message.channel.id].append({
                    "title": title,
                    "link": url,
                    "summary": summary,
                    "source": source,
                    "date": datetime.now(),
                    "hash": news_hash,
                    "image_url": image_url,
                    "is_discord": True,
                    "is_retweet": is_retweet
                })
                logging.info(f"Noticia añadida desde {source} (URL): {title[:50]} - {url}")

        elif any(keyword.lower() in (message.content or "").lower() for keyword in KEYWORDS_EN["news_indicators"]):
            title = (message.content or "")[:100]
            summary = message.content or ""
            url = f"discord://{message.channel.id}/{message.id}"

            if await is_duplicate(title, summary, url, source):
                logging.info(f"Duplicado detectado en texto plano: {title[:50]}")
                return

            image_url = message.attachments[0].url if message.attachments else None
            if not image_url:
                async with aiohttp.ClientSession() as session:
                    image_url = await get_image_from_url(session, url)

            news_hash = hashlib.sha256((title + summary + url + source).encode()).hexdigest()
            discord_news[message.channel.id].append({
                "title": title,
                "link": url,
                "summary": summary,
                "source": source,
                "date": datetime.now(),
                "hash": news_hash,
                "image_url": image_url,
                "is_discord": True,
                "is_retweet": is_retweet
            })
            logging.info(f"Noticia de texto añadida desde {source}: {title[:50]}")

    except Exception as e:
        logging.error(f"Error procesando mensaje de Discord (canal {message.channel.id}): {e}", exc_info=True)
        bot_status["errors"] += 1
        await save_bot_state()

@discord_bot.command(name="noticias")
async def fetch_news(ctx, number: int):
    global bot_status
    bot_status["last_task"] = f"Ejecutando 'recuperarnoticias' en canal {ctx.channel.id}"
    if number <= 0 or number > 50:
        await ctx.send("❌ El número debe estar entre 1 y 50. Ejemplo: `recuperarnoticias 5`")
        return

    log_message = await ctx.send("📋 **Procesando noticias...**")
    log_content = "📋 **Procesando noticias...**\n"
    await log_message.edit(content=log_content)
    
    try:
        async with aiohttp.ClientSession() as session:
            trends = await get_trending_keywords()
            news_items = []
            
            async for message in ctx.channel.history(limit=100):
                if len(news_items) >= number:
                    break
                
                if message.embeds:
                    for embed in message.embeds:
                        title = embed.title or "Sin título"
                        summary = embed.description or message.content or "Sin descripción"
                        url = embed.url or None
                        image_url = embed.image.url if embed.image else None
                        
                        if not url and embed.description:
                            embed_urls = re.findall(
                                r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
                                embed.description
                            )
                            url = embed_urls[0] if embed_urls else None
                        
                        if not url:
                            url = f"discord://{message.channel.id}/{message.id}"
                        
                        if url:
                            short_url = await shorten_url(url)
                            news_hash = hashlib.sha256((title + summary + short_url + f"discord_channel_{ctx.channel.id}").encode()).hexdigest()
                            news_items.append({
                                "title": title,
                                "link": short_url,
                                "summary": summary,
                                "source": f"discord_channel_{ctx.channel.id}",
                                "date": message.created_at,
                                "hash": news_hash,
                                "image_url": image_url,
                                "is_discord": True
                            })
                
                else:
                    urls = re.findall(
                        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
                        message.content
                    )
                    if urls:
                        for url in urls:
                            if len(news_items) < number:
                                short_url = await shorten_url(url)
                                news_hash = hashlib.sha256((message.content + summary + short_url + f"discord_channel_{ctx.channel.id}").encode()).hexdigest()
                                news_items.append({
                                    "title": message.content[:100],
                                    "link": short_url,
                                    "summary": message.content,
                                    "source": f"discord_channel_{ctx.channel.id}",
                                    "date": message.created_at,
                                    "hash": news_hash,
                                    "image_url": None,
                                    "is_discord": True
                                })
                    elif any(keyword.lower() in message.content.lower() for keyword in KEYWORDS_EN["news_indicators"]):
                        if len(news_items) < number:
                            title = message.content[:100]
                            summary = message.content
                            url = f"discord://{message.channel.id}/{message.id}"
                            image_url = message.attachments[0].url if message.attachments else None
                            news_hash = hashlib.sha256((title + summary + url + f"discord_channel_{ctx.channel.id}").encode()).hexdigest()
                            news_items.append({
                                "title": title,
                                "link": url,
                                "summary": summary,
                                "source": f"discord_channel_{ctx.channel.id}",
                                "date": message.created_at,
                                "hash": news_hash,
                                "image_url": image_url,
                                "is_discord": True
                            })

            processed = 0
            for i, entry in enumerate(news_items, 1):
                log_content += f"\n**Noticia {i}/{number}:** {entry['title'][:50]}...\n"
                log_content += f"**Fuente:** {entry['source']}\n"
                log_content += "Calculando puntuación...\n"
                await log_message.edit(content=log_content)
                
                score = await score_news(entry, trends)
                log_content += f"**Puntuación:** {score:.1f}\n"
                log_content += "🔝 **Prioridad máxima (origen Discord)**\n"
                await log_message.edit(content=log_content)
                
                total_daily_tweets = bot_status["daily_tweets_total"]
                if total_daily_tweets >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
                    log_content += f"❌ **Límite diario alcanzado:** {CONFIG['API_LIMITS']['TWEETS_PER_DAY']} tweets\n"
                    await log_message.edit(content=log_content)
                    break

                languages = ["en", "es"]
                for language in languages:
                    client = twitter_client_en if language == "en" else twitter_client_es
                    api = twitter_api_en if language == "en" else twitter_api_es
                    daily_key = "daily_tweets_total"
                    monthly_key = f"monthly_posts_{language}"
                    posted_key = f"posted_tweets_{language}"
                    last_tweet_key = f"last_tweet_time_{language}"
                    category = f"write_{language}"
                    semaphore = write_semaphore_en if language == "en" else write_semaphore_es
                    rate_limit_event = write_rate_limit_event_en if language == "en" else write_rate_limit_event_es

                    current_time = time.time()
                    if current_time < bot_status["twitter_wait_until"].get(category, 0):
                        log_content += f"⏳ **En espera para {language.upper()} hasta {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%H:%M:%S')}**\n"
                        continue

                    if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
                        log_content += f"❌ **Límite mensual total alcanzado**\n"
                        continue

                    last_tweet_time = bot_status[last_tweet_key]
                    if last_tweet_time and (datetime.now() - last_tweet_time).total_seconds() < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
                        log_content += f"⏳ **Espera requerida para {language}**\n"
                        continue

                    if await is_published_in_language(entry["title"], entry["summary"], entry["link"], entry["source"], language):
                        log_content += f"✅ **Ya publicado en {language.upper()}**\n"
                        continue

                    log_content += f"**Publicando en {language.upper()}...**\n"
                    await log_message.edit(content=log_content)

                    async def make_twitter_request(func, *args, **kwargs):
                        async with semaphore:
                            await rate_limit_event.wait()
                            for attempt in range(5):
                                try:
                                    result = await asyncio.to_thread(func, *args, **kwargs)
                                    return result
                                except tweepy.TweepyException as e:
                                    if e.response and e.response.status_code == 429:
                                        retry_after = int(e.response.headers.get("Retry-After", 900))
                                        reset_time = time.time() + retry_after
                                        bot_status["twitter_wait_until"][category] = reset_time
                                        rate_limit_event.clear()
                                        logging.warning(f"429 detectado en {language}. Pausando hasta {datetime.fromtimestamp(reset_time)}")
                                        await asyncio.sleep(retry_after)
                                        rate_limit_event.set()
                                    else:
                                        logging.error(f"Error en intento {attempt + 1}/5 para {language}: {e}")
                                        if attempt == 4:
                                            raise
                                        await asyncio.sleep(2 ** attempt)
                                except Exception as e:
                                    logging.error(f"Error inesperado en intento {attempt + 1}/5 para {language}: {e}")
                                    if attempt == 4:
                                        raise
                                    await asyncio.sleep(2 ** attempt)

                    try:
                        tweet_match = re.search(r'https://(nitter\.net|twitter\.com|x\.com)/(\w+)/status/(\d+)', entry["link"])
                        if tweet_match:
                            success = await make_twitter_request(
                                repost_tweet_from_url, entry["link"], entry["summary"], language
                            )
                            if success:
                                log_content += f"✅ **Reposteado en {language.upper()}** (ID: {tweet_match.group(3)})\n"
                                bot_status[daily_key] += 1
                                bot_status[monthly_key] += 1
                                bot_status[posted_key] += 1
                                bot_status[last_tweet_key] = datetime.now()
                                processed += 1
                        else:
                            tweet = await generate_detailed_tweet(entry["title"], entry["summary"], entry["link"], trends, language)
                            image_url = entry.get("image_url")
                            if image_url:
                                img_data = await download_image(session, image_url)
                                if img_data:
                                    optimized_img = await optimize_image(img_data)
                                    if optimized_img:
                                        temp_image_path = os.path.join(CONFIG["PATHS"]["TEMP_IMAGE_DIR"], f"{entry['hash']}_{language}.jpg")
                                        async with aiofiles.open(temp_image_path, "wb") as f:
                                            await f.write(optimized_img)
                                        media = await make_twitter_request(api.media_upload, temp_image_path)
                                        tweet_response = await make_twitter_request(
                                            client.create_tweet, text=tweet, media_ids=[media.media_id]
                                        )
                                    else:
                                        tweet_response = await make_twitter_request(client.create_tweet, text=tweet)
                                else:
                                    tweet_response = await make_twitter_request(client.create_tweet, text=tweet)
                            else:
                                tweet_response = await make_twitter_request(client.create_tweet, text=tweet)
                            
                            log_content += f"✅ **Publicado en {language.upper()}** (ID: {tweet_response.data['id']})\n"
                            bot_status[daily_key] += 1
                            bot_status[monthly_key] += 1
                            bot_status[posted_key] += 1
                            bot_status[last_tweet_key] = datetime.now()
                            processed += 1

                        news_hash = hashlib.sha256((entry["title"] + entry["summary"] + entry["link"] + entry["source"]).encode()).hexdigest()
                        cursor.execute('''INSERT OR IGNORE INTO historial (hash, title, url, tweet, relevance, source, date, engagement, summary, language)
                                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                                      (news_hash, entry["title"], entry["link"], tweet, score, entry["source"], datetime.now().isoformat(), 0, entry["summary"], language))
                        conn.commit()
                        save_bot_state_sync(cursor, conn)
                    except tweepy.TweepyException as e:
                        log_content += f"❌ **Error en {language.upper()}:** {str(e)}\n"
                        if e.response and e.response.status_code == 429:
                            log_content += f"⏳ **Límite de tasa, esperando hasta {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%H:%M:%S')}**\n"
                        bot_status["errors"] += 1
                        save_bot_state_sync(cursor, conn)
                    except Exception as e:
                        log_content += f"❌ **Error inesperado en {language.upper()}:** {str(e)}\n"
                        bot_status["errors"] += 1
                        save_bot_state_sync(cursor, conn)

                await log_message.edit(content=log_content)
                await asyncio.sleep(1)

            log_content += f"\n🏁 **Finalizado:** {processed} noticias publicadas."
            await log_message.edit(content=log_content)

            await asyncio.sleep(5)
            await ctx.message.delete()
            await log_message.delete()

        logging.info(f"Comando 'recuperarnoticias' completado: {processed}/{number} noticias procesadas")
        bot_status["processed_news"] += processed
        bot_status["recent_processed_news"] += processed
        save_bot_state_sync(cursor, conn)

    except Exception as e:
        logging.error(f"Error en 'recuperarnoticias' (canal {ctx.channel.id}): {e}", exc_info=True)
        log_content += f"\n❌ **Error general:** {str(e)}"
        await log_message.edit(content=log_content)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        await asyncio.sleep(5)
        await ctx.message.delete()
        await log_message.delete()

executor = ThreadPoolExecutor(max_workers=2)

async def check_api_rate_limit(response_headers=None, category="read", endpoint=None, exception=None):
    current_time_ts = time.time()
    
    logging.debug(f"Encabezados recibidos para {category}: {response_headers}, Tipo: {type(response_headers)}")
    
    if response_headers and isinstance(response_headers, collections.abc.Mapping):
        try:
            logging.debug(f"Procesando encabezados para {category}: {response_headers}")
            # Manejo de Retry-After
            if "Retry-After" in response_headers:
                retry_after = response_headers["Retry-After"]
                try:
                    wait_time = min(int(retry_after) + 5, 3600)  # Máximo 1 hora
                    logging.info(f"Retry-After detectado. Esperando {wait_time}s")
                    return wait_time
                except ValueError:
                    retry_date = parsedate_to_datetime(retry_after)
                    wait_time = min((retry_date.timestamp() - current_time_ts) + 5, 3600) if retry_date else 3600
                    logging.info(f"Retry-After (fecha) detectado. Esperando {wait_time}s")
                    return wait_time
            
            # Manejo de límites de tasa
            remaining = int(response_headers.get("x-rate-limit-remaining", float('inf')))
            reset_ts = int(response_headers.get("x-rate-limit-reset", 0))
            if remaining <= 0 and reset_ts > current_time_ts:
                wait_time = min((reset_ts - current_time_ts) + 5, 10800)  # Máximo 3 horas
                logging.warning(f"Límite alcanzado para {category}. Esperando {wait_time:.1f}s")
                return wait_time
        except Exception as e:
            logging.error(f"Error procesando encabezados: {e}")
    
    # Respaldo según el tipo de límite
    if exception and isinstance(exception, tweepy.TooManyRequests):
        if category.startswith("write"):
            # Suponer límite diario (50 tweets/día en plan gratuito) si no hay más info
            wait_time = 10800  # 24 horas
            logging.warning(f"Posible límite diario alcanzado para {category}. Esperando {wait_time}s por defecto.")
        else:
            wait_time = 900  # 15 minutos para otros límites
            logging.warning(f"No se encontraron encabezados útiles para {category}. Esperando {wait_time}s por defecto.")
        return wait_time
    
    return 0

async def make_twitter_request(func, *args, category="read", semaphore=None, rate_limit_event=None, endpoint=None, **kwargs):
    global bot_status
    semaphore = semaphore or (read_semaphore if category == "read" else (write_semaphore_en if category == "write_en" else write_semaphore_es))
    rate_limit_event = rate_limit_event or (read_rate_limit_event if category == "read" else (write_rate_limit_event_en if category == "write_en" else write_rate_limit_event_es))
    
    async with semaphore:
        for attempt in range(5):
            # Verificar límite antes del intento
            wait_time = await check_api_rate_limit(category=category, endpoint=endpoint)
            if wait_time > 0:
                logging.debug(f"Esperando {wait_time:.1f}s antes del intento {attempt + 1} para {category}")
                await asyncio.sleep(wait_time)
            
            if not rate_limit_event.is_set():
                await rate_limit_event.wait()
            
            try:
                bot_status["api_request_count"] += 1
                result = await asyncio.to_thread(func, *args, **kwargs)
                # Extraer encabezados de la respuesta
                response_headers = getattr(result, 'response', None).headers if hasattr(result, 'response') else None
                logging.debug(f"Encabezados en intento exitoso para {category}: {response_headers}")
                wait_time = await check_api_rate_limit(response_headers=response_headers, category=category, endpoint=endpoint)
                if wait_time > 0:
                    rate_limit_event.clear()
                    logging.debug(f"Pausando {wait_time:.1f}s tras respuesta para {category}")
                    await asyncio.sleep(wait_time)
                    rate_limit_event.set()
                return result
            except tweepy.TooManyRequests as e:
                # Extraer encabezados del error
                response_headers = getattr(e, 'response', None).headers if hasattr(e, 'response') else None
                logging.debug(f"Encabezados en error 429 para {category}: {response_headers}")
                wait_time = await check_api_rate_limit(response_headers=response_headers, category=category, endpoint=endpoint, exception=e)
                if wait_time > 0:
                    rate_limit_event.clear()
                    logging.warning(f"Error 429 en intento {attempt + 1}/5 para {category}: {e}. Esperando {wait_time}s")
                    await asyncio.sleep(wait_time)
                    rate_limit_event.set()
                elif attempt == 4:
                    logging.error(f"Fallo tras 5 intentos por 429 para {category}: {e}")
                    raise
            except Exception as e:
                logging.error(f"Error inesperado en intento {attempt + 1}/5 para {category}: {e}")
                if attempt == 4:
                    raise
                await asyncio.sleep(2 ** attempt)
                
async def generate_rss_feed(username: str, num_tweets: int = 5) -> str | None:
    global bot_status
    bot_status["last_task"] = f"Generando feed RSS para @{username}"
    logging.debug(f"Intentando generar feed RSS para @{username}")

    category = "read"
    current_time_ts = time.time()

    if current_time_ts < bot_status["twitter_wait_until"].get(category, 0):
        logging.warning(f"Lecturas de Twitter/X pausadas hasta {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%Y-%m-%d %H:%M:%S')}")
        return None

    try:
        if username in user_id_cache:
            user_id = user_id_cache[username]
        else:
            user_response = await make_twitter_request(twitter_client_en.get_user, username=username, category=category)
            if not user_response or not user_response.data:
                logging.warning(f"No se encontró el usuario @{username}")
                return None
            user_id = user_response.data.id
            user_id_cache[username] = user_id
            save_bot_state_sync(cursor, conn)

        tweets_response = await make_twitter_request(
            twitter_client_en.get_users_tweets,
            id=user_id,
            max_results=num_tweets,
            tweet_fields=["created_at"],
            category=category
        )

        if not tweets_response or not tweets_response.data:
            logging.info(f"No se encontraron tweets recientes para @{username}")
            return None

        fg = FeedGenerator()
        fg.title(f"Tweets from @{username}")
        fg.link(href=f"https://x.com/{username}", rel="alternate")
        fg.description(f"Latest tweets from @{username}")

        for tweet in tweets_response.data:
            fe = fg.add_entry()
            fe.title(tweet.text[:100] + "..." if len(tweet.text) > 100 else tweet.text)
            fe.link(href=f"https://x.com/{username}/status/{tweet.id}")
            fe.description(tweet.text)
            if isinstance(tweet.created_at, datetime):
                fe.pubDate(tweet.created_at.isoformat())
            else:
                logging.warning(f"Formato de fecha inesperado para tweet {tweet.id}: {tweet.created_at}")
                fe.pubDate(datetime.now().isoformat())

        rss_file = os.path.join(CONFIG["PATHS"]["RSS_CACHE_DIR"], f"{username}.rss")
        fg.rss_file(rss_file)
        logging.info(f"Feed RSS generado para @{username}: {rss_file}")
        return rss_file

    except Exception as e:
        logging.error(f"Error generando RSS para @{username}: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return None

async def update_twitter_feeds():
    """
    Actualiza los feeds de Twitter/X obteniendo los últimos tweets de las cuentas configuradas.
    
    Returns:
        list: Lista de noticias/tweets obtenidos.
    """
    twitter_news = []
    async with aiohttp.ClientSession() as session:
        for account in TWITTER_ACCOUNTS:
            interval = get_dynamic_update_interval(account)
            retry_delay = 1  # Retraso inicial para retroceso exponencial
            for attempt in range(3):
                try:
                    # Verificar existencia de la cuenta
                    user = await make_twitter_request(
                        twitter_client_en.get_user,
                        username=account,
                        category="read",
                        endpoint=f"users/by/username/{account}"
                    )
                    if not user.data:
                        logging.warning(f"Cuenta {account} no encontrada o inaccesible")
                        break
                    
                    tweets = await make_twitter_request(
                        twitter_client_en.get_users_tweets,
                        id=user.data.id,
                        max_results=10,
                        exclude=["retweets", "replies"],
                        category="read",
                        endpoint=f"users/{user.data.id}/tweets"
                    )
                    if tweets.data:
                        for tweet in tweets.data:
                            news_entry = {
                                "title": tweet.text[:50],
                                "summary": tweet.text,
                                "link": f"https://twitter.com/{account}/status/{tweet.id}",
                                "source": f"Twitter: {account}",
                                "is_tweet": True
                            }
                            twitter_news.append(news_entry)
                    break  # Salir si la solicitud es exitosa
                except tweepy.TooManyRequests as e:
                    logging.warning(f"Error 429 obteniendo tweets de {account} en intento {attempt + 1}/3: {e}")
                    if attempt == 2:
                        logging.error(f"No se pudieron obtener tweets de {account} tras 3 intentos por 429")
                        break
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2  # Retroceso exponencial
                except tweepy.TweepyException as e:
                    logging.warning(f"Error obteniendo tweets de {account} en intento {attempt + 1}/3: {e}")
                    if attempt == 2:
                        logging.error(f"No se pudieron obtener tweets de {account} tras 3 intentos")
                        break
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                except Exception as e:
                    logging.error(f"Error inesperado procesando {account}: {e}")
                    break
    return twitter_news

def clean_text(text: str) -> str:
    if not text or not isinstance(text, str):
        return ""
    text = re.sub(r'<.*?>', '', text)  # Eliminar etiquetas HTML
    text = re.sub(r'[^\w\s.,!?\'"-]', '', text)  # Mantener comillas y guiones
    return text.strip()

def post_process_translation(translated_text: str) -> str:
    corrections = {
        "interruptor": "Switch",
        "deja caer": "no incluirá",
        "preordenes": "preventas",
        "carro": "cartucho",
        "añade el": "añade soporte para"
    }
    for wrong, correct in corrections.items():
        translated_text = translated_text.replace(wrong, correct)
    return translated_text

def translate_text(text: str) -> str:
    cleaned_text = clean_text(text)
    if not cleaned_text:
        return text
    if cleaned_text in translation_cache:
        return translation_cache[cleaned_text]
    try:
        inputs = translation_tokenizer(cleaned_text, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
        with torch.inference_mode():
            translated = translation_model.generate(**inputs)
        translated_text = translation_tokenizer.decode(translated[0], skip_special_tokens=True)
        translation_cache[cleaned_text] = translated_text
        return translated_text
    except Exception as e:
        logging.error(f"Error en MarianMT para texto '{cleaned_text[:50]}...': {e}")
        return cleaned_text

async def fetch_url(session: aiohttp.ClientSession, url: str) -> bytes | None:
    """
    Obtiene el contenido binario de una URL.
    """
    global bot_status
    # No actualizamos last_task aquí ya que es una función auxiliar llamada por otras
    try:
        # Aumentar un poco el timeout para descargas de imágenes potencialmente más grandes
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=15)) as response:
            if response.status != 200:
                logging.warning(f"Error obteniendo {url}: Código de estado {response.status}")
                return None
            # Verificar el tipo de contenido para asegurar que es una imagen o HTML (para scraping)
            content_type = response.headers.get("Content-Type", "").lower()
            if not (content_type.startswith("image/") or "text/html" in content_type):
                 logging.warning(f"Contenido no es imagen ni HTML en {url}: {content_type}")
                 return None
            return await response.read()
    except Exception as e:
        # Loguear como debug o warning si es un error común de red, error si es inesperado
        logging.debug(f"Error obteniendo {url}: {e}")
        # No incrementar contador de errores aquí para errores de red comunes, solo para errores lógicos.
        return None

async def get_image_from_url(session: aiohttp.ClientSession, url: str, max_attempts: int = 5) -> str | None:
    """
    Intenta encontrar y validar URLs de imágenes en una página web.
    Prioriza imágenes Open Graph y Twitter Card, y luego busca en etiquetas <img>.
    Intenta validar las URLs encontradas.
    """
    logging.debug(f"Attempting to get image URL from: {url}")
    try:
        # Use fetch_url to get the page content
        html_content_bytes = await fetch_url(session, url)
        if not html_content_bytes:
            logging.debug(f"Could not get content from {url} to search for images.")
            return None

        # Decode HTML content, trying different encodings
        try:
            html_content = html_content_bytes.decode('utf-8')
        except UnicodeDecodeError:
            try:
                html_content = html_content_bytes.decode('latin-1')
            except Exception as e:
                logging.warning(f"Could not decode content from {url}: {e}")
                return None

        soup = BeautifulSoup(html_content, "html.parser")
        image_urls = []

        # Prioritize meta tags (Open Graph and Twitter Card)
        og_image = soup.find("meta", attrs={"property": "og:image"})
        if og_image and og_image.get("content"):
            image_urls.append(og_image.get("content"))
            logging.debug(f"Found Open Graph image: {og_image.get('content')}")

        twitter_image = soup.find("meta", attrs={"name": "twitter:image"})
        if twitter_image and twitter_image.get("content"):
             image_urls.append(twitter_image.get("content"))
             logging.debug(f"Found Twitter Card image: {twitter_image.get('content')}")

        # Search for <img> tags. Try to find URLs that look high-resolution
        img_tags = soup.find_all("img", attrs={"src": True})
        for tag in img_tags:
            img_url = tag.get("src")
            if img_url and img_url.startswith(("http://", "https://")):
                 # Simple heuristic: check for common high-res indicators in URL
                 if any(indicator in img_url.lower() for indicator in ["large", "full", "original"]):
                     image_urls.insert(0, img_url) # Add to the beginning to prioritize
                 else:
                    image_urls.append(img_url)

        # Remove duplicates while preserving order
        image_urls = list(dict.fromkeys(image_urls))

        logging.debug(f"Image URLs found (including meta and img): {image_urls}")

        # Validate the found URLs and return the first valid one
        for img_url in image_urls[:max_attempts]: # Limit the number of validations
            if await validate_image_url(session, img_url):
                logging.debug(f"Validated image URL: {img_url}")
                return img_url
            else:
                logging.debug(f"Image URL not valid or inaccessible: {img_url}")


        logging.info(f"No valid images found on {url} after {max_attempts} validation attempts.")
        return None

    except Exception as e:
        logging.error(f"Error getting image URL from {url}: {e}", exc_info=True)
        # Decide if this is a critical error for the bot
        # bot_status["errors"] += 1
        # save_bot_state_sync(cursor, conn)
        return None

async def validate_image_url(session: aiohttp.ClientSession, img_url: str) -> bool:
    """
    Verifica si una URL apunta a una imagen válida y accesible.
    """
    try:
        # Usar método HEAD para verificar sin descargar todo el contenido
        async with session.head(img_url, timeout=5) as response:
            if response.status != 200:
                logging.debug(f"Validación HEAD fallida para {img_url}: {response.status}")
                return False
            content_type = response.headers.get("Content-Type", "").lower()
            # Verificar si el tipo de contenido es una imagen
            if not content_type.startswith("image/"):
                 logging.debug(f"Validación HEAD fallida para {img_url}: Content-Type no es imagen ({content_type})")
                 return False
            # Opcional: Verificar tamaño mínimo/máximo si es relevante
            content_length = int(response.headers.get("Content-Length", 0))
            if content_length > 0 and content_length < 1024: # Ejemplo: ignorar imágenes muy pequeñas (<1KB)
                 logging.debug(f"Validación HEAD fallida para {img_url}: Imagen demasiado pequeña ({content_length} bytes)")
                 return False

            logging.debug(f"Validación HEAD exitosa para {img_url}")
            return True
    except Exception as e:
        logging.debug(f"Error durante validación HEAD para {img_url}: {e}")
        return False
    
async def download_image(session: aiohttp.ClientSession, image_url: str) -> bytes | None:
    """
    Descarga el contenido binario de una URL de imagen.
    Verifica si los datos descargados son una imagen válida.
    Manejo más robusto de errores y validación.
    """
    logging.debug(f"Attempting to download image from: {image_url}")
    try:
        # Use fetch_url to download binary content
        # fetch_url should already handle basic status code checks
        image_data = await fetch_url(session, image_url)
        if not image_data:
            logging.debug(f"Failed to download image data from {image_url}.")
            return None

        # Verify if the downloaded data is a valid image using PIL
        try:
            # Use a context manager for the image to ensure it's closed
            with Image.open(io.BytesIO(image_data)) as img:
                img.verify() # Verify if it's a valid image without loading fully
                # Check image format if needed, e.g., exclude GIFs if not supported by Twitter
                if img.format not in ['JPEG', 'PNG', 'WEBP']: # Add/remove formats as needed
                     logging.warning(f"Downloaded data from {image_url} is in unsupported format: {img.format}")
                     return None
                logging.debug(f"Downloaded data from {image_url} verified as valid image.")
            # Return the original downloaded data if verification is successful
            return image_data
        except Exception as e:
            # Catch specific PIL errors if possible, or a general Exception
            logging.warning(f"Downloaded data from {image_url} is not a valid image or verification failed: {e}")
            return None

    except Exception as e:
        # Log as error for unexpected issues during download
        logging.error(f"Error downloading image from {image_url}: {e}", exc_info=True)
        # Do not increment error counter for common network issues, only critical logic errors.
        return None

async def shorten_url(url: str) -> str:
    global bot_status
    bot_status["last_task"] = f"Acortando URL: {url}"
    try:
        if url in url_cache:
            return url_cache[url]
        cache_path = os.path.join(CONFIG["PATHS"]["CACHE_DIR"], f"{hashlib.sha256(url.encode()).hexdigest()}.url")
        if os.path.exists(cache_path):
            async with aiofiles.open(cache_path, "r") as f:
                short_url = await f.read()
                url_cache[url] = short_url
                return short_url
        async with aiohttp.ClientSession() as session:
            async with session.get(f"http://tinyurl.com/api-create.php?url={url}") as response:
                if response.status != 200:
                    logging.error(f"Error acortando {url}: Código de estado {response.status}")
                    return url
                short_url = await response.text()
                async with aiofiles.open(cache_path, "w") as f:
                    await f.write(short_url)
                url_cache[url] = short_url
                return short_url
    except Exception as e:
        logging.error(f"Error acortando {url}: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return url

async def get_trending_keywords() -> list[str]:
    global trending_keywords, bot_status
    bot_status["last_task"] = "Obteniendo tendencias"
    try:
        return list(KEYWORDS_EN["gaming"].keys()) + list(KEYWORDS_EN["tech"].keys())
    except Exception as e:
        logging.error(f"Error obteniendo tendencias: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return list(KEYWORDS_EN["gaming"].keys()) + list(KEYWORDS_EN["tech"].keys())

def calculate_relevance(title: str, summary: str) -> float:
    content = (title + " " + summary).lower()
    keyword_score = 0
    for category in ["gaming", "tech"]:
        kw_dict = KEYWORDS_EN[category]
        for kw, weight in kw_dict.items():
            if kw in content:
                keyword_score += weight * 1.5
    for kw in KEYWORDS_EN["news_indicators"]:
        if kw in content:
            keyword_score += 0.75
    return keyword_score

def calculate_freshness(news_date: datetime) -> float:
    try:
        age_hours = (datetime.now() - news_date).total_seconds() / 3600
        return max(0, 1 - 0.1 * age_hours)
    except Exception as e:
        logging.error(f"Error calculando frescura: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return 0

async def is_duplicate(title: str, summary: str, url: str, source: str, entry_date: datetime = None) -> bool:
    """
    Verifica si una noticia es un duplicado basándose en hash.

    Args:
        title (str): Título de la noticia.
        summary (str): Resumen de la noticia.
        url (str): URL de la noticia.
        source (str): Fuente de la noticia.
        entry_date (datetime, optional): Fecha de publicación de la noticia.

    Returns:
        bool: True si la noticia es un duplicado, False si es nueva.
    """
    news_hash = hashlib.sha256((title + summary + url + source).encode()).hexdigest()
    cache_key = f"{news_hash}_duplicate"

    # Verificar si es reciente (dentro de 24 horas) y está en caché
    if entry_date and (datetime.now() - entry_date).total_seconds() < 86400:  # 24 horas
        if cache_key in duplicate_cache:
            logging.debug(f"✅ Duplicado encontrado en caché: {title[:50]}")
            return duplicate_cache[cache_key]
    
    try:
        # Crear un cursor local
        local_cursor = conn.cursor()
        try:
            local_cursor.execute('''SELECT 1 FROM historial WHERE hash = ?''', (news_hash,))
            result = await run_db_sync(local_cursor.fetchone)
            is_duplicate = result is not None
        finally:
            local_cursor.close()  # Cerrar el cursor explícitamente

        # Almacenar en caché si es reciente
        if entry_date and (datetime.now() - entry_date).total_seconds() < 86400:
            duplicate_cache[cache_key] = is_duplicate

        # Registrar resultado
        if is_duplicate:
            logging.debug(f"❌ Noticia duplicada: {title[:50]}")
        else:
            logging.debug(f"✅ Noticia nueva: {title[:50]}")

        return is_duplicate
    except Exception as e:
        logging.error(f"❌ Error verificando duplicado: {e}", exc_info=True)
        bot_status["errors"] += 1
        await save_bot_state()
        return False
    
async def is_published_in_language(title: str, summary: str, url: str, source: str, language: str) -> bool:
     """
     Checks if a news item has already been published in a specific language.
     Uses robust criteria, including the language.
     Executes the database query in a separate thread using run_db_sync.
     """
     # Add checks for None or empty strings for inputs
     if not title or not url or not source or not language:
         return False # Cannot check with missing info

     try:
         # Use normalized text for title and summary if needed for language-specific check
         # normalized_title = normalize_text_for_duplicate_check(title)
         # normalized_summary = normalize_text_for_duplicate_check(summary)

         # Query includes language
         result = await run_db_sync(
             cursor.execute,
             "SELECT 1 FROM historial WHERE title = ? AND summary = ? AND url = ? AND source = ? AND language = ?",
             (title, summary, url, source, language)
         )
         fetch_result = await run_db_sync(result.fetchone)
         is_published = fetch_result is not None
         # logging.debug(f"Published check for '{title[:50]}' in {language}: {is_published}") # Too verbose
         return is_published
     except Exception as e:
         logging.error(f"Error in is_published_in_language: {e}", exc_info=True)
         # Decide if this is a critical error
         # bot_status["errors"] += 1
         # await save_bot_state()
         return False # Assume not published in case of DB error
     
try:
    tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
    bert_model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
    bert_model.eval()
    if torch.cuda.is_available():
        bert_model.to("cuda")
    logging.info("Modelo DistilBERT y tokenizer inicializados correctamente.")
except Exception as e:
    logging.error(f"Error al inicializar DistilBERT: {e}", exc_info=True)
    tokenizer = None
    bert_model = None

async def score_news(entry, trends):
    if tokenizer is None or bert_model is None:
        logging.error("Modelo DistilBERT o tokenizer no inicializados.")
        return 0.0

    try:
        text = entry.get('title', '')
        if not text:
            logging.warning("Título de noticia vacío.")
            return 0.0
    
        # Filtrar noticias no deseadas basadas en KEYWORDS (lista)
        text_lower = text.lower()
        if any(keyword in text_lower for keyword in KEYWORDS_EN):
            logging.debug(f"Noticia descartada por tema de gaming/tecnología: {text}")
            return 0.0

        # Calcular puntaje base con DistilBERT
        def run_bert(inputs):
            with torch.no_grad():
                outputs = bert_model(**inputs)
                probs = torch.softmax(outputs.logits, dim=-1)
                return probs[0][1].item()  # Probabilidad de la clase positiva

        inputs = tokenizer(
            text,
            return_tensors="pt",
            max_length=128,
            truncation=True,
            padding=True
        )
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}

        loop = asyncio.get_running_loop()
        score = await loop.run_in_executor(None, partial(run_bert, inputs))

        # Intentar recuperar imagen si no existe mediante scraping
        image_score = 0.0
        if not entry.get("image_url"):
            async with aiohttp.ClientSession() as session:
                try:
                    image_url = await get_image_from_url(session, entry.get("link", ""))
                    if image_url:
                        entry["image_url"] = image_url
                        image_score = 10.0
                        logging.info(f"Imagen recuperada para noticia: {text[:50]} - {image_url}")
                    else:
                        logging.debug(f"No se encontró imagen para noticia: {text[:50]}")
                except Exception as e:
                    logging.warning(f"Error al recuperar imagen para noticia {text[:50]}: {e}")
        
        # Ajustar con tendencias
        trend_score = 0.0
        for trend in trends:
            if trend.lower() in text_lower:
                trend_score += 20.0

        # Bonificación para retuits
        retweet_bonus = 10.0 if entry.get("is_retweet", False) else 0.0

        # Calcular puntuación final
        final_score = min((score * 100) + trend_score + image_score + retweet_bonus, 100.0)

        logging.debug(f"Noticia puntuada: {text} con score {final_score} (base: {score*100}, tendencias: {trend_score}, imagen: {image_score}, retweet: {retweet_bonus})")
        return final_score

    except Exception as e:
        logging.error(f"Error en score_news: {e}", exc_info=True)
        return 0.0
    
# Directorio para almacenar imágenes
IMAGE_DIR = "images"
os.makedirs(IMAGE_DIR, exist_ok=True)

async def get_and_store_image(session: aiohttp.ClientSession, entry: dict) -> str | None:
    """
    Obtiene una imagen para la noticia y la guarda localmente.
    
    Args:
        session: Sesión de aiohttp para solicitudes HTTP.
        entry: Diccionario con datos de la noticia (url, title, summary, hash).
    
    Returns:
        Ruta local de la imagen o None si falla.
    """
    # Intentar obtener imagen desde la URL de la noticia
    image_url = await get_image_from_url(session, entry["link"])
    if not image_url:
        # Si no hay imagen en la URL, buscar con título y resumen
        search_query = entry.get('title', '') + " " + entry.get('summary', '')
        image_url = await get_image_from_search(session, search_query)
    
    if image_url:
        # Descargar la imagen
        image_data = await download_image(session, image_url)
        if image_data:
            # Usar el hash de la noticia como nombre del archivo
            news_hash = entry["hash"]
            image_path = os.path.join(IMAGE_DIR, f"{news_hash}.jpg")
            # Guardar la imagen localmente
            async with aiofiles.open(image_path, "wb") as f:
                await f.write(image_data)
            logging.info(f"Imagen guardada en: {image_path}")
            return image_path
    logging.warning(f"No se pudo obtener imagen para {entry['title'][:50]}")
    return None

async def process_queued_news():
    """
    Procesa noticias encoladas sin imagen, obtiene imágenes y las almacena.
    """
    # Consulta noticias sin imagen en la BD
    cursor.execute("SELECT id, title, summary, url, news_hash FROM cola_publicacion WHERE image_data IS NULL")
    queued_items = await run_db_sync(cursor.fetchall)
    
    async with aiohttp.ClientSession() as session:
        for item in queued_items:
            entry = {
                "title": item["title"],
                "summary": item["summary"],
                "link": item["url"],
                "hash": item["news_hash"]
            }
            # Obtener y guardar la imagen
            image_path = await get_and_store_image(session, entry)
            if image_path:
                # Actualizar la BD con la ruta de la imagen
                await run_db_sync(
                    cursor.execute,
                    "UPDATE cola_publicacion SET image_data = ? WHERE id = ?",
                    (image_path, item["id"])
                )
                await run_db_sync(conn.commit)
                logging.info(f"BD actualizada para {entry['title'][:50]} con {image_path}")
            else:
                logging.warning(f"Sin imagen para {entry['title'][:50]}, se reintentará después")

async def optimize_image(image_data: bytes) -> bytes | None:
    """
    Optimiza una imagen para reducir su tamaño manteniendo una calidad aceptable.
    Utiliza Pillow para redimensionar y ajustar la calidad JPEG.
    Ejecuta operaciones de Pillow en un hilo separado.
    Manejo de errores mejorado.
    """
    loop = asyncio.get_event_loop()
    # Ensure image_data is not None before proceeding
    if image_data is None:
        logging.warning("optimize_image received None data.")
        return None

    logging.debug(f"Starting image optimization. Original size: {len(image_data)} bytes.")

    try:
        # Define a synchronous function for Pillow operations
        def optimize_image_sync(img_data_bytes):
            try:
                # Use a context manager for the image
                with Image.open(io.BytesIO(img_data_bytes)) as img:
                    # Ensure image is in RGB mode for JPEG saving
                    if img.mode != 'RGB':
                        img = img.convert("RGB")

                    max_long_dim = 1280
                    width, height = img.size
                    # Resize if necessary
                    if max(width, height) > max_long_dim:
                        if width > height:
                            new_width = max_long_dim
                            new_height = int(height * (new_width / width))
                        else:
                            new_height = max_long_dim
                            new_width = int(width * (new_height / height))
                        # Use LANCZOS for better quality resizing
                        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)

                    output = io.BytesIO()
                    jpeg_quality = 85
                    # Try saving with initial quality
                    img.save(output, format="JPEG", quality=jpeg_quality, optimize=True)
                    optimized_data = output.getvalue()

                    # Re-optimize with lower quality if size reduction is not significant
                    # Check if optimized size is still close to original AND size is large
                    if len(optimized_data) >= len(img_data_bytes) * 0.95 and len(optimized_data) > 100 * 1024 and jpeg_quality > 80: # Added size check > 100KB
                         logging.debug("Optimized image size not significantly smaller, attempting lower quality.")
                         output = io.BytesIO() # Reset buffer
                         img.save(output, format="JPEG", quality=80, optimize=True)
                         optimized_data = output.getvalue()

                    # Final check on size, Twitter limit is 5MB for most image types
                    if len(optimized_data) > 5 * 1024 * 1024:
                         logging.warning(f"Optimized image is still too large: {len(optimized_data)} bytes.")
                         return None # Return None if still too large

                    return optimized_data

            except Exception as e:
                # Log the error within the synchronous function
                logging.error(f"Error optimizing image in thread: {e}", exc_info=True)
                return None

        # Execute the synchronous function in the executor
        optimized_data = await loop.run_in_executor(None, optimize_image_sync, image_data)

        if optimized_data:
            logging.debug(f"Image optimized successfully. Final size: {len(optimized_data)} bytes.")
        else:
            logging.warning("Image optimization failed or resulted in invalid data.")

        return optimized_data

    except Exception as e:
        # Log general errors from the async wrapper
        logging.error(f"General error in optimize_image async wrapper: {e}", exc_info=True)
        # bot_status["errors"] += 1 # Decide if this is a critical bot error
        # await save_bot_state()
        return None

async def translate_to_spanish(text: str) -> str:
    cleaned_text = clean_text(text)
    if not cleaned_text:
        return text
    loop = asyncio.get_event_loop()
    translated = await loop.run_in_executor(executor, translate_text, cleaned_text)
    translated = post_process_translation(translated)
    return translated

def get_summary_sentences(summary: str, max_chars: int) -> str:
    """
    Divide el resumen en oraciones y selecciona las que caben en max_chars.
    """
    sentences = re.split(r'(?<=[.!?])\s+', summary)
    selected_sentences = []
    current_length = 0
    for sentence in sentences:
        sentence_length = len(sentence) + 1  # +1 para el espacio
        if current_length + sentence_length > max_chars:
            break
        selected_sentences.append(sentence)
        current_length += sentence_length
    return " ".join(selected_sentences).strip()

import spacy
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer
from yake import KeywordExtractor
import re
import random

async def generate_detailed_tweet(title: str, summary: str, url: str, trends: list[str], language: str) -> str:
    # Load AI tools
    nlp = spacy.load("en_core_web_sm")
    kw_extractor = KeywordExtractor(n=3, top=5)
    summarizer = TextRankSummarizer()

    # Shorten URL and calculate max content length
    short_url = await shorten_url(url)
    max_content_length = CONFIG["TWEET"]["MAX_LENGTH"] - len(short_url) - 50  # Increased buffer for hashtags

    # Semantic analysis for news type, emoji, and tone
    title_lower = title.lower()
    summary_lower = summary.lower()
    news_types = {
        "release": (["release", "launch", "available", "out now"], "🎮", "enthusiastic"),
        "update": (["update", "patch", "fix", "improvement"], "🔧", "informative"),
        "announce": (["announcement", "reveal", "trailer", "teaser"], "🚨", "urgent"),
        "deal": (["free", "deal", "sale", "discount", "offer"], "💸", "promotional"),
        "urgent": (["break", "urgent", "alert", "emergency"], "🔥", "urgent")
    }
    news_type, emoji, tone = "default", "📰", "enthusiastic"
    for n_type, (keywords, e, t) in news_types.items():
        if any(kw in title_lower or kw in summary_lower for kw in keywords):
            news_type, emoji, tone = n_type, e, t
            break

    # Extract entities and keywords
    doc = nlp(summary)
    entities = [ent.text for ent in doc.ents if ent.label_ in ["PERSON", "ORG", "PRODUCT", "GPE"]][:2]
    keywords = kw_extractor.extract_keywords(summary)
    topics = [kw[0] for kw in keywords][:3]

    # Select key sentences
    parser = PlaintextParser.from_string(clean_text(summary), Tokenizer("english"))
    summary_sentences = summarizer(parser.document, 2)
    content = " ".join(str(sentence) for sentence in summary_sentences)[:max_content_length].strip()

    # Translate if needed
    if language == "es" and "twitter" not in url.lower():
        content = await translate_to_spanish(content)

    # Hook templates
    hooks = {
        "en": {
            "release": {"enthusiastic": ["🎮 {entity} just dropped! Dive in!", "🚀 {entity} is LIVE!"],
                        "default": ["🎮 {entity} hits the scene!"]},
            "update": {"informative": ["🔧 {entity} updated! See what's new.", "🔧 New {entity} patch!"],
                       "default": ["🔧 {entity} update out now!"]},
            "announce": {"urgent": ["🚨 {entity} reveal! Don't miss it!", "🚨 Big {entity} news!"],
                         "default": ["🚨 {entity} announced!"]},
            "deal": {"promotional": ["💸 Grab {entity} on sale!", "💸 Hot {entity} deal!"],
                     "default": ["💸 {entity} discount awaits!"]},
            "urgent": {"urgent": ["🔥 {entity} alert! Read now!", "🔥 Urgent {entity} update!"],
                       "default": ["🔥 {entity} news!"]},
            "default": {"enthusiastic": ["📰 {entity} news to excite you!"],
                        "default": ["📰 Latest on {entity}!"]}
        },
        "es": {
            "release": {"enthusiastic": ["🎮 ¡{entity} ya está aquí! ¡Juega!", "🚀 ¡{entity} lanzado!"],
                        "default": ["🎮 ¡{entity} llega hoy!"]},
            "update": {"informative": ["🔧 ¡{entity} actualizado! Descubre más.", "🔧 ¡Parche para {entity}!"],
                       "default": ["🔧 ¡Actualización de {entity}!"]},
            "announce": {"urgent": ["🚨 ¡Noticia de {entity}! ¡No te lo pierdas!", "🚨 ¡{entity} revelado!"],
                         "default": ["🚨 ¡{entity} anunciado!"]},
            "deal": {"promotional": ["💸 ¡Oferta en {entity}!", "💸 ¡{entity} en rebaja!"],
                     "default": ["💸 ¡Descuento en {entity}!"]},
            "urgent": {"urgent": ["🔥 ¡Alerta de {entity}! ¡Lee ya!", "🔥 ¡Noticia urgente de {entity}!"],
                       "default": ["🔥 ¡Novedades de {entity}!"]},
            "default": {"enthusiastic": ["📰 ¡{entity} trae noticias emocionantes!"],
                        "default": ["📰 ¡Lo último de {entity}!"]}
        }
    }
    hook_dict = hooks[language]
    hook = random.choice(hook_dict.get(news_type, {}).get(tone, hook_dict["default"]["enthusiastic"]))\
           .format(entity=entities[0] if entities else "game")

    # Generate hashtags: prioritize entities and topics, then trends
    hashtag_candidates = [re.sub(r"[^\w\s]", "", e).replace(" ", "") for e in entities + topics if len(e) <= 15]
    for trend in trends[:2]:  # Limit to 2 trends for relevance
        clean_trend = re.sub(r"[^\w\s]", "", trend).replace(" ", "")
        if len(clean_trend) <= 15 and clean_trend not in hashtag_candidates:
            hashtag_candidates.append(clean_trend)
    hashtag_candidates = list(dict.fromkeys(hashtag_candidates))[:3]  # Remove duplicates, limit to 3
    if not hashtag_candidates:
        hashtag_candidates = ["Videojuegos", "Gaming"] if language == "es" else ["Gaming", "VideoGames"]
    hashtags = " ".join(f"#{h}" for h in hashtag_candidates)

    # Build tweet
    tweet = f"{hook} {content} {short_url} {hashtags}"
    if len(tweet) > CONFIG["TWEET"]["MAX_LENGTH"]:
        content = content[:max_content_length - len(hook) - len(hashtags) - len(short_url) - 5] + "..."
        tweet = f"{hook} {content} {short_url} {hashtags}"

    return tweet.strip()

import aiohttp
import os
import hashlib
import logging
import colorlog
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import requests
import asyncio
import logging
from datetime import datetime, timedelta
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from tweepy import API, OAuthHandler
from tweepy.errors import TweepyException
from bs4 import BeautifulSoup
from PIL import Image, ImageDraw, ImageFont
import io

# 3. Actualizar función capture_map_screenshot
def capture_map_screenshot(map_url, output_path):
    """
    Captura una captura de pantalla del mapa meteorológico usando Selenium con reintentos,
    optimizaciones para carga completa y evasión de detección anti-bots.
    """
    logging.info(f"Iniciando captura de mapa meteorológico desde {map_url}")
    driver = None
    for attempt in range(2):
        try:
            # Configurar ChromeDriver para evitar detección
            chrome_options = Options()
            chrome_options.add_argument("--headless")  # Sin interfaz gráfica
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option("useAutomationExtension", False)

            driver = webdriver.Chrome(options=chrome_options)
            driver.set_page_load_timeout(30)  # Timeout de carga de página

            # Cargar la página
            driver.get(map_url)
            logging.debug("Página cargada, esperando carga completa del DOM")

            # Esperar carga completa del DOM
            WebDriverWait(driver, 20).until(
                lambda d: d.execute_script("return document.readyState") == "complete"
            )

            # Intentar manejar popup de cookies (si existe)
            try:
                cookie_button = WebDriverWait(driver, 5).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, "button#accept-cookies, .accept-cookies"))
                )
                cookie_button.click()
                logging.info("Popup de cookies aceptado")
            except TimeoutException:
                logging.info("No se encontró el popup de cookies o ya fue aceptado")

            # Esperar el elemento del mapa (ajusta el selector según tu página)
            map_element = WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div#map"))  # Reemplaza con el selector correcto
            )
            logging.debug("Elemento del mapa encontrado")

            # Asegurar que el mapa esté visible
            driver.execute_script("arguments[0].scrollIntoView();", map_element)
            time.sleep(2)  # Espera adicional para renderizado

            # Capturar captura de pantalla del elemento
            map_element.screenshot(output_path)
            logging.info(f"Captura de pantalla guardada en {output_path}")
            return True

        except TimeoutException as e:
            logging.warning(f"Timeout en captura de mapa, intento {attempt + 1}/2: {e}")
            if attempt == 1:
                logging.error("Fallo tras 2 intentos en captura de mapa")
                return False
            time.sleep(30)  # Esperar antes de reintentar
        except Exception as e:
            logging.error(f"Error en captura de mapa, intento {attempt + 1}/2: {e}")
            if attempt == 1:
                logging.error("Fallo tras 2 intentos en captura de mapa")
                return False
            time.sleep(30)
        finally:
            if driver:
                driver.quit()

    return False

async def repost_tweet_from_url(tweet_url: str, summary: str, language: str = 'en') -> bool:
    global bot_status
    category = f"write_{language}"
    current_time = time.time()
    if current_time < bot_status["twitter_wait_until"].get(category, 0):
        logging.warning(f"En período de espera para {category}")
        return False

    try:
        tweet_match = re.search(r'https://(nitter\.net|twitter\.com|x\.com)/(\w+)/status/(\d+)', tweet_url)
        if not tweet_match:
            logging.warning(f"URL no válida para republicación: {tweet_url}")
            return False

        username = tweet_match.group(2)
        tweet_id = tweet_match.group(3)
        title = f"Tweet from @{username}"
        source = f"twitter_{username}"
        client = twitter_client_en if language == 'en' else twitter_client_es
        daily_key = "daily_tweets_total"
        monthly_key = f"monthly_posts_{language}"
        posted_key = f"posted_tweets_{language}"
        last_tweet_key = f"last_tweet_time_{language}"
        semaphore = write_semaphore_en if language == "en" else write_semaphore_es
        rate_limit_event = write_rate_limit_event_en if language == "en" else write_rate_limit_event_es

        if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"] or bot_status[daily_key] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
            logging.info(f"Límite alcanzado para {language}")
            return False

        last_tweet_time = bot_status[last_tweet_key]
        if last_tweet_time and (time.time() - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
            logging.info(f"Espera requerida para {language}")
            return False

        if await is_published_in_language(title, summary, tweet_url, source, language):
            logging.info(f"Tweet ya publicado en {language}: {tweet_url}")
            return False

        tweet_response = await make_twitter_request(client.get_tweet, id=tweet_id, tweet_fields=["text"], category="read")
        if not tweet_response.data:
            logging.warning(f"No se pudo obtener el tweet {tweet_id}")
            return False

        tweet_text = tweet_response.data.text
        comment = summary.strip() if summary else ""
        if comment.startswith(f"RT @{username}:"):
            comment = comment[len(f"RT @{username}:"):].strip()
        if language == 'es' and comment:
            comment = await translate_to_spanish(comment)

        if comment and len(comment) > CONFIG["TWEET"]["MAX_LENGTH"] - 50:
            comment = comment[:CONFIG["TWEET"]["MAX_LENGTH"]-53] + "..."

        if comment:
            tweet_response = await make_twitter_request(
                client.create_tweet,
                text=comment,
                quote_tweet_id=tweet_id,
                category=category,
                semaphore=semaphore,
                rate_limit_event=rate_limit_event
            )
            logging.info(f"Quote Tweet publicado en {language}: {comment[:50]}... ID: {tweet_response.data['id']}")
        else:
            tweet_response = await make_twitter_request(
                client.retweet,
                tweet_id=tweet_id,
                category=category,
                semaphore=semaphore,
                rate_limit_event=rate_limit_event
            )
            logging.info(f"Retweet simple publicado en {language}: Tweet ID {tweet_id}")

        bot_status[daily_key] += 1
        bot_status[monthly_key] += 1
        bot_status[posted_key] += 1
        bot_status[last_tweet_key] = datetime.now()

        news_hash = hashlib.sha256((title + summary + tweet_url + source).encode()).hexdigest()
        cursor.execute('''INSERT OR IGNORE INTO historial (hash, title, url, tweet, relevance, source, date, engagement, summary, language)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                      (news_hash, title, tweet_url, comment or "Retweet", 50.0, source, datetime.now().isoformat(), 0, tweet_text, language))
        conn.commit()
        save_bot_state_sync(cursor, conn)
        return True

    except Exception as e:
        logging.error(f"Error al citar tweet {tweet_url} en {language}: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return False

async def post_tweet(session: aiohttp.ClientSession, title: str, summary: str, url: str, image_data: bytes | None, news_hash: str, score: float, source: str, language: str, trends: list[str]) -> bool:
    """
    Attempts to post a tweet. Handles limits, cooldown, duplicates, image upload
    and adds to the queue if it fails due to recoverable limits.
    Uses CONFIG["API_LIMITS"] and CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"] values.
    Benefits from dynamic limit handling in make_twitter_request.
    Modified to handle image_data being None and improved error handling.
    """
    global bot_status
    category = f"write_{language}"
    current_time = time.time()

    # Check if writing for this language is paused
    if current_time < bot_status["twitter_wait_until"].get(category, 0):
        logging.warning(f"In waiting period for {category} until {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%H:%M:%S')}. Skipping post attempt.")
        return False

    client = twitter_client_en if language == "en" else twitter_client_es
    daily_key = "daily_tweets_total"
    monthly_key = f"monthly_posts_{language}"
    posted_key = f"posted_tweets_{language}"
    last_tweet_key = f"last_tweet_time_{language}"
    semaphore = write_semaphore_en if language == "en" else write_semaphore_es
    rate_limit_event = write_rate_limit_event_en if language == "en" else write_rate_limit_event_es

    try:
        # Check monthly and daily limits using CONFIG["API_LIMITS"] values
        if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
            logging.info(f"Total monthly limit reached: {bot_status['monthly_posts_en'] + bot_status['monthly_posts_es']}/{CONFIG['API_LIMITS']['MONTHLY_POSTS_TOTAL']}. Skipping post attempt.")
            return False

        if bot_status[daily_key] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
            logging.info(f"Daily limit reached: {bot_status[daily_key]}/{CONFIG['API_LIMITS']['TWEETS_PER_DAY']}. Skipping post attempt.")
            return False

        # Check tweet cooldown using CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]
        last_tweet_time = bot_status.get(last_tweet_key)
        if last_tweet_time and (current_time - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
            remaining = CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"] - (current_time - last_tweet_time.timestamp())
            logging.info(f"Cooldown required for {language}: {remaining/60:.1f} minutes remaining. Skipping post attempt.")
            return False

        # Check if already published using the async function is_published_in_language
        if await is_published_in_language(title, summary, url, source, language):
            logging.info(f"News already published in {language}: {title[:50]}. Skipping post attempt.")
            return False

        logging.info(f"Intentando publicar en {language}: {title[:50]}...")
        tweet = await generate_detailed_tweet(title, summary, url, trends, language)
        
        # Sanitize tweet text: decode HTML entities, remove HTML tags, clean extra spaces/lines
        tweet = html.unescape(tweet)  # Decode entities like ltpgt to <p>
        tweet = re.sub(r'<[^>]+>', '', tweet)  # Remove HTML tags
        tweet = re.sub(r'\s+', ' ', tweet.strip())  # Replace multiple spaces/lines with single space
        # Remove unwanted promotional phrases (customize as needed)
        tweet = re.sub(r'Release just dropped!|¡Lanzamiento épico de Release', '', tweet, flags=re.IGNORECASE)
        tweet = tweet.strip()  # Remove leading/trailing spaces
        logging.debug(f"Sanitized tweet text: {tweet[:100]}...")

        temp_image_path = os.path.join(CONFIG["PATHS"]["TEMP_IMAGE_DIR"], f"{news_hash}_{language}.jpg")

        media = None
        # Check if image_data is valid before attempting to optimize/upload
        if image_data and isinstance(image_data, bytes) and len(image_data) > 0:
            # Use aiofiles for asynchronous file operations
            try:
                # Save original downloaded image data temporarily
                async with aiofiles.open(temp_image_path, "wb") as f:
                    await f.write(image_data)

                # optimize_image now handles None input and returns None on failure
                optimized_image_data = await optimize_image(image_data)

                if optimized_image_data:
                    # Save optimized data to the temporary file for Tweepy
                    async with aiofiles.open(temp_image_path, "wb") as f:
                        await f.write(optimized_image_data)

                    try:
                        # Initialize API v1.1 for media upload
                        api = tweepy.API(
                            consumer_key=os.getenv("TWITTER_CONSUMER_KEY"),
                            consumer_secret=os.getenv("TWITTER_CONSUMER_SECRET"),
                            access_token=os.getenv("TWITTER_ACCESS_TOKEN"),
                            access_token_secret=os.getenv("TWITTER_ACCESS_TOKEN_SECRET")
                        )
                        logging.debug("API v1.1 inicializada para subida de medios")
                        # make_twitter_request handles Tweepy's sync call and rate limits
                        media = await make_twitter_request(
                            api.media_upload,
                            temp_image_path,
                            category=category,
                            semaphore=semaphore,
                            rate_limit_event=rate_limit_event
                        )
                        logging.debug(f"Image uploaded successfully for tweet in {language}.")
                    except Exception as e:
                        logging.error(f"Error uploading image for tweet in {language}: {e}")
                        media = None
                else:
                    logging.warning(f"Image optimization failed for tweet in {language}. Posting without image.")
                    media = None
            except Exception as e:
                logging.error(f"Error processing image for tweet in {language}: {e}", exc_info=True)
                media = None
        else:
            logging.debug(f"No valid image data provided for tweet in {language}. Posting without image.")
            media = None

        tweet_response = None
        try:
            # make_twitter_request handles the Tweepy call, rate limits, and retries
            tweet_response = await make_twitter_request(
                client.create_tweet,
                text=tweet,
                media_ids=[media.media_id] if media else None,
                category=category,
                semaphore=semaphore,
                rate_limit_event=rate_limit_event
            )
            logging.info(f"Tweet in {language} posted (score {score:.1f}): {tweet[:50]}... ID: {tweet_response.data['id']}")

            # Update status counters
            bot_status[daily_key] += 1
            bot_status[monthly_key] += 1
            bot_status[posted_key] += 1
            bot_status[last_tweet_key] = datetime.now()

            # Record in history using run_db_sync
            await run_db_sync(
                cursor.execute,
                '''INSERT OR IGNORE INTO historial (hash, title, url, tweet, relevance, source, date, engagement, summary, language)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                (news_hash, title, url, tweet, score, source, datetime.now().isoformat(), 0, summary, language)
            )
            await run_db_sync(conn.commit)
            await save_bot_state()

            return True

        except Exception as e:
            logging.error(f"Error posting tweet in {language}: {e}", exc_info=True)
            is_rate_limit_error = isinstance(e, tweepy.TweepyException) and e.response and e.response.status_code == 429

            if is_rate_limit_error and not await is_published_in_language(title, summary, url, source, language):
                trends_json = json.dumps(trends)
                await run_db_sync(
                    cursor.execute,
                    '''INSERT INTO cola_publicacion (title, summary, url, image_data, news_hash, score, source, language, trends)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                    (title, summary, url, image_data, news_hash, score, source, language, trends_json)
                )
                await run_db_sync(conn.commit)
                logging.info(f"News '{title[:50]}' added to queue for {language} due to rate limit.")
            else:
                logging.warning(f"News '{title[:50]}' failed to post in {language} due to non-recoverable error or already published. Not adding to queue.")

            bot_status["errors"] += 1
            await save_bot_state()
            return False

    finally:
        if os.path.exists(temp_image_path):
            await asyncio.to_thread(os.remove, temp_image_path)
            
queue_lock = asyncio.Lock()

async def review_queue():
    """
    Gestiona la cola de publicaciones con un sistema de espera avanzado que optimiza lecturas y escrituras,
    maximizando el uso de los límites del plan gratuito (50 tweets/día, 1500 posts/mes).
    """
    global bot_status
    async with queue_lock:  # Evitar concurrencia
        bot_status["last_task"] = "Revisando cola de publicaciones"
        logging.info("Iniciando revisión avanzada de la cola de publicaciones...")

        try:
            # Obtener estado actual de la API
            current_time = time.time()
            tweets_remaining = CONFIG["API_LIMITS"]["TWEETS_PER_DAY"] - bot_status["daily_tweets_total"]
            monthly_remaining = CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"] - (bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"])
            trends_last_updated = bot_status.get("trends_last_updated", 0)
            trends_expired = current_time - trends_last_updated > 4 * 3600  # Actualizar tendencias cada 4 horas

            # Obtener tendencias solo si es necesario
            trends = []
            if trends_expired or not bot_status.get("current_trends"):
                trends = await get_trending_keywords()
                bot_status["trends_last_updated"] = current_time
                bot_status["current_trends"] = trends
                logging.info("Tendencias actualizadas desde la API.")
            else:
                trends = bot_status["current_trends"]
                logging.debug("Usando tendencias en caché.")

            # Obtener elementos de la cola
            cursor.execute("SELECT id, title, summary, url, image_url, news_hash, score, source, language, trends, added_at FROM cola_publicacion")
            queue_items = await run_db_sync(cursor.fetchall)

            if not queue_items:
                logging.info("La cola de publicaciones está vacía.")
                return

            items_to_remove = []
            items_to_publish = []
            updated_items = []
            published_this_cycle = {"en": False, "es": False}  # Limitar a un tweet por idioma por ciclo

            # Calcular el número ideal de tweets por hora para distribuir uniformemente
            hours_left_in_day = (24 - datetime.now().hour) % 24 or 24
            ideal_tweets_per_hour = min(tweets_remaining / max(hours_left_in_day, 1), 3)  # Máximo 3 tweets/hora
            min_wait_between_tweets = 3600 / max(ideal_tweets_per_hour, 1) if ideal_tweets_per_hour > 0 else 300

            # Procesar cada elemento de la cola
            async with aiohttp.ClientSession() as session:
                for item in queue_items:
                    id, title, summary, url, image_url, news_hash, old_score, source, language, old_trends_json, added_at = item
                    added_at = datetime.fromisoformat(added_at) if isinstance(added_at, str) else added_at

                    # Verificar si falta image_url y buscar una imagen si es necesario
                    if not image_url or not image_url.strip():
                        logging.info(f"No se encontró imagen para noticia en cola (ID: {id}): {title[:50]}. Buscando imagen...")
                        try:
                            image_url = await get_image_from_url(session, url)
                            if image_url:
                                await run_db_sync(
                                    cursor.execute,
                                    "UPDATE cola_publicacion SET image_url = ? WHERE id = ?",
                                    (image_url, id)
                                )
                                await run_db_sync(conn.commit)
                                logging.info(f"Imagen encontrada y actualizada para noticia en cola (ID: {id}): {title[:50]} - {image_url}")
                            else:
                                logging.warning(f"No se pudo encontrar imagen para noticia en cola (ID: {id}): {title[:50]} después de búsqueda")
                        except Exception as e:
                            logging.error(f"Error al buscar imagen para noticia en cola (ID: {id}) {title[:50]}: {e}")

                    # Recalcular puntuación
                    entry = {
                        "title": title,
                        "summary": summary,
                        "link": url,
                        "image_url": image_url,
                        "source": source,
                        "date": added_at
                    }
                    new_score = await score_news(entry, trends)

                    # Aplicar decaimiento por antigüedad (15% por día)
                    age_days = (datetime.now() - added_at).total_seconds() / (24 * 3600)
                    decay_factor = max(0, 1 - 0.15 * age_days)
                    new_score *= decay_factor

                    # Calcular índice de urgencia
                    urgency = new_score
                    if source.startswith("discord") or source in [f"twitter_{acc}" for acc in TWITTER_ACCOUNTS]:
                        urgency *= 1.2  # Priorizar fuentes clave
                    if any(trend.lower() in title.lower() or trend.lower() in summary.lower() for trend in trends):
                        urgency *= 1.3  # Aumentar si coincide con tendencias
                    urgency *= min(1, 1.5 - age_days / 2)  # Reducir para noticias antiguas
                    urgency *= (1 + 0.5 * tweets_remaining / CONFIG["API_LIMITS"]["TWEETS_PER_DAY"])  # Aumentar si hay muchos tweets disponibles
                    urgency *= (1 + 0.2 * monthly_remaining / CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"])  # Aumentar si hay margen mensual

                    # Decidir acción
                    if new_score < 5 or age_days > 3:
                        items_to_remove.append(id)
                        logging.debug(f"Elemento {title[:50]} (ID: {id}) marcado para eliminar: puntuación {new_score:.1f}, edad {age_days:.1f} días")
                    else:
                        updated_items.append((new_score, json.dumps(trends), id))
                        # Determinar si publicar ahora o esperar
                        wait_time = 0
                        if tweets_remaining <= 3 or monthly_remaining <= 20:
                            wait_time = 600  # Esperar 10 minutos si los límites están muy bajos
                        elif len(queue_items) > 40:
                            wait_time = 180 * (1 - urgency / 20)  # Escalonar espera si la cola está casi llena
                        elif urgency < 8:
                            wait_time = 90 * (1 - urgency / 15)  # Esperar más para noticias menos urgentes
                        wait_time = max(wait_time, min_wait_between_tweets)  # Respetar el ritmo ideal

                        # Verificar cooldowns y tiempos de espera de la API
                        lang_key = f"write_{language}"
                        last_tweet_time = bot_status.get(f"last_tweet_time_{language}")
                        api_wait_until = bot_status["twitter_wait_until"].get(lang_key, 0)
                        cooldown_elapsed = not last_tweet_time or (current_time - last_tweet_time.timestamp()) >= CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]

                        if wait_time <= 0 and current_time >= api_wait_until and cooldown_elapsed:
                            items_to_publish.append((id, entry, new_score, language, trends, urgency))
                            logging.debug(f"Elemento {title[:50]} (ID: {id}) listo para publicación: urgencia {urgency:.1f}")
                        else:
                            logging.debug(f"Elemento {title[:50]} (ID: {id}) en espera: {wait_time:.1f}s, cooldown: {not cooldown_elapsed}, API wait: {api_wait_until - current_time:.1f}s")

                # Ordenar elementos a publicar por urgencia
                items_to_publish.sort(key=lambda x: x[5], reverse=True)

                # Publicar elementos seleccionados, respetando límites
                for id, entry, score, language, trends, urgency in items_to_publish[:tweets_remaining]:
                    if not published_this_cycle[language]:  # Solo un tweet por idioma por ciclo
                        if await publish_news(session, entry, entry.get("hash", ""), score, language, trends):
                            items_to_remove.append(id)
                            bot_status["daily_tweets_total"] += 1
                            bot_status[f"monthly_posts_{language}"] += 1
                            bot_status[f"last_tweet_time_{language}"] = datetime.now()
                            published_this_cycle[language] = True
                            logging.info(f"Publicado desde cola (ID: {id}): {entry['title'][:50]} en {language}. Actualizando last_tweet_time a {bot_status[f'last_tweet_time_{language}']}")
                            await save_bot_state()
                            await asyncio.sleep(CONFIG["INTERVALS"]["API_REQUEST_DELAY_SECONDS"])
                        else:
                            logging.warning(f"Fallo al publicar elemento (ID: {id}): {entry['title'][:50]}")

            # Eliminar elementos irrelevantes o publicados
            if items_to_remove:
                await run_db_sync(
                    cursor.execute,
                    f"DELETE FROM cola_publicacion WHERE id IN ({','.join('?' * len(items_to_remove))})",
                    tuple(items_to_remove)
                )
                logging.info(f"Eliminados {len(items_to_remove)} elementos de la cola.")

            # Actualizar puntuaciones
            if updated_items:
                await run_db_sync(
                    cursor.executemany,
                    "UPDATE cola_publicacion SET score = ?, trends = ? WHERE id = ?",
                    updated_items
                )
                logging.info(f"Actualizadas puntuaciones de {len(updated_items)} elementos.")

            # Limitar la cola a 50 elementos
            cursor.execute("SELECT COUNT(*) FROM cola_publicacion")
            queue_size = (await run_db_sync(cursor.fetchone))[0]
            if queue_size > 50:
                excess = queue_size - 50
                cursor.execute("SELECT id FROM cola_publicacion ORDER BY score ASC LIMIT ?", (excess,))
                excess_ids = [row[0] for row in await run_db_sync(cursor.fetchall)]
                await run_db_sync(
                    cursor.execute,
                    f"DELETE FROM cola_publicacion WHERE id IN ({','.join('?' * len(excess_ids))})",
                    tuple(excess_ids)
                )
                logging.info(f"Eliminados {len(excess_ids)} elementos para mantener límite de 50.")

            await run_db_sync(conn.commit)
            logging.info("Revisión avanzada de la cola completada.")

        except Exception as e:
            logging.error(f"Error revisando la cola de publicaciones: {e}", exc_info=True)
            bot_status["errors"] += 1
        finally:
            await save_bot_state()

async def process_queue():
    global bot_status
    async with queue_lock:  # Evitar concurrencia
        bot_status["last_task"] = "Processing publication queue"
        print_section_header("Procesando Cola de Publicación")
        logging.debug("Starting publication queue processing...")

        try:
            # Usar cursor local
            local_cursor = conn.cursor()
            local_cursor.execute("SELECT id, title, summary, url, image_url, news_hash, score, source, language, trends FROM cola_publicacion ORDER BY added_at ASC")
            pending_news = await run_db_sync(local_cursor.fetchall)

            if not pending_news:
                print("ℹ️ La cola de publicación está vacía.")
                logging.debug("Publication queue is empty.")
                return

            logging.info(f"Items in publication queue: {len(pending_news)}")

            async with aiohttp.ClientSession() as session:
                items_to_remove = []

                for news in pending_news:
                    id, title, summary, url, image_url, news_hash, score, source, language, trends_json = news
                    trends = json.loads(trends_json)
                    category = f"write_{language}"
                    current_time_ts = time.time()

                    if current_time_ts < bot_status["twitter_wait_until"].get(category, 0):
                        logging.debug(f"Escritura pausada para {language}. Omitiendo elemento de la cola (ID: {id}).")
                        continue

                    if bot_status["daily_tweets_total"] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
                        logging.debug("Límite diario de tweets alcanzado. Deteniendo procesamiento de la cola.")
                        break

                    if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
                        logging.debug("Límite mensual total de publicaciones alcanzado. Deteniendo procesamiento de la cola.")
                        break

                    last_tweet_time = bot_status.get(f"last_tweet_time_{language}")
                    if last_tweet_time and (current_time_ts - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
                        remaining = CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"] - (current_time_ts - last_tweet_time.timestamp())
                        logging.debug(f"Enfriamiento requerido para {language}: {remaining/60:.1f} minutos restantes.")
                        continue

                    if await is_published_in_language(title, summary, url, source, language):
                        logging.info(f"Elemento de la cola ya publicado ({language}): {title[:50]}. Marcando para eliminar (ID: {id}).")
                        items_to_remove.append(id)
                        continue

                    logging.info(f"Intentando publicar elemento de la cola (ID: {id}) en {language}: {title[:50]}...")

                    try:
                        success = await post_tweet(session, title, summary, url, image_url, news_hash, score, source, language, trends)
                        if success:
                            logging.info(f"Elemento de la cola (ID: {id}) publicado exitosamente en {language}. Marcando para eliminar.")
                            items_to_remove.append(id)
                            bot_status[f"last_tweet_time_{language}"] = datetime.now()
                            bot_status[f"monthly_posts_{language}"] += 1
                            logging.info(f"Actualizando last_tweet_time_{language} a {bot_status[f'last_tweet_time_{language}']}")
                            await save_bot_state()
                            await asyncio.sleep(CONFIG["INTERVALS"]["API_REQUEST_DELAY_SECONDS"])
                        else:
                            logging.warning(f"No se pudo publicar el elemento de la cola (ID: {id}) en {language}. Permanece en la cola.")
                    except Exception as e:
                        logging.error(f"Error intentando publicar elemento de la cola (ID: {id}) en {language}: {e}", exc_info=True)
                        bot_status["errors"] += 1
                        await save_bot_state()

                if items_to_remove:
                    logging.info(f"Eliminando {len(items_to_remove)} elementos de la cola de publicación.")
                    try:
                        await run_db_sync(
                            local_cursor.execute,
                            f"DELETE FROM cola_publicacion WHERE id IN ({','.join('?' * len(items_to_remove))})",
                            tuple(items_to_remove)
                        )
                        await run_db_sync(conn.commit)
                        await save_bot_state()
                    except Exception as e:
                        logging.error(f"Error eliminando elementos de la cola: {e}", exc_info=True)
                        bot_status["errors"] += 1
                        await save_bot_state()

        except Exception as e:
            logging.error(f"Error general procesando la cola de publicación: {e}", exc_info=True)
            bot_status["errors"] += 1
            await save_bot_state()
        finally:
            local_cursor.close()

async def get_image_from_search(session: aiohttp.ClientSession, query: str) -> bytes | None:
    """
    Busca una imagen relacionada con una consulta usando la búsqueda de imágenes de Google
    y descarga la primera imagen válida encontrada.
    Mejorado manejo de errores y validación.
    """
    logging.debug(f"Searching for image for query: '{query}'")
    try:
        # Use aiohttp for the HTTP request
        # Using a more specific image search URL if possible, or rely on tbm=isch
        search_url = f"https://www.google.com/search?tbm=isch&q={urllib.parse.quote(query)}"
        # Using a User-Agent to avoid being blocked
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}

        async with session.get(search_url, headers=headers, timeout=aiohttp.ClientTimeout(total=15)) as response:
            if response.status != 200:
                logging.warning(f"Error in image search for '{query}': Status code {response.status}")
                return None
            html_content = await response.text()

        soup = BeautifulSoup(html_content, "html.parser")
        # Google Images loads dynamically, extracting URLs can be tricky.
        # A more robust approach might involve using a dedicated image search API or a headless browser.
        # This simple approach looks for img tags with src or data-src.
        img_tags = soup.find_all("img", src=True)
        if not img_tags:
             img_tags = soup.find_all("img", {"data-src": True})

        image_urls = []
        for img_tag in img_tags:
            img_url = img_tag.get("src") or img_tag.get("data-src")
            # Filter out URLs that are likely not content images (e.g., icons, spacers)
            if img_url and img_url.startswith(("http://", "https://")) and not any(ext in img_url.lower() for ext in [".gif", ".svg", ".ico", "spacer.gif", "gstatic.com"]): # Added gstatic.com filter
                image_urls.append(img_url)

        logging.debug(f"Image URLs found in search for '{query}': {image_urls[:10]}...") # Log only the first few

        # Attempt to download and validate the first few found URLs
        for img_url in image_urls[:5]: # Limit the number of downloads from search
            logging.debug(f"Attempting to download and validate search image: {img_url}")
            # Use download_image which includes validation
            img_data = await download_image(session, img_url)
            if img_data:
                # Attempt to optimize the downloaded image
                # optimize_image now handles None input and returns None on failure
                optimized_data = await optimize_image(img_data)
                if optimized_data:
                    logging.info(f"Image obtained from search and optimized for '{query[:50]}'")
                    return optimized_data
                else:
                    logging.debug(f"Could not optimize the search image: {img_url}")
            else:
                 logging.debug(f"Could not download or validate the search image: {img_url}")

        logging.info(f"Could not obtain a valid image from search for '{query}'")
        return None
    except Exception as e:
        logging.error(f"Error searching for image for '{query[:50]}': {e}", exc_info=True)
        # Decide if this should increment the error counter
        # bot_status["errors"] += 1
        # save_bot_state_sync(cursor, conn)
        return None
    
async def publish_news(session: aiohttp.ClientSession, entry: dict, news_hash: str, score: float, language: str, trends: list[str]) -> bool:
    """
    Publica una noticia en Twitter/X, manejando imágenes y tweets/reposts.
    
    Args:
        session: Sesión de aiohttp para solicitudes HTTP.
        entry: Diccionario con los datos de la noticia.
        news_hash: Hash único de la noticia.
        score: Puntuación de relevancia.
        language: Idioma de publicación ("en" o "es").
        trends: Lista de tendencias actuales.
    
    Returns:
        bool: True si se publicó exitosamente, False en caso contrario.
    """
    title = entry.get("title", "Untitled")
    url = entry.get("link")
    summary = entry.get("summary", "")
    source = entry.get("source", "Unknown Source")
    is_tweet = entry.get("is_tweet", False)
    image_url = entry.get("image_url")
    
    logging.debug(f"Preparando publicación para {language}: {title[:50]} (is_tweet: {is_tweet})")
    
    if is_tweet:
        try:
            result = await repost_tweet_from_url(url, summary, language)
            if result:
                logging.info(f"Repost exitoso en {language}: {title[:50]}")
            return result
        except tweepy.TooManyRequests as e:
            logging.warning(f"Error 429 al repostear en {language}: {title[:50]}: {e}")
            return False
        except Exception as e:
            logging.error(f"Error al repostear en {language}: {title[:50]}: {e}")
            bot_status["errors"] += 1
            await save_bot_state()
            return False
    
    image_data = None
    temp_image_path = None
    if image_url:
        logging.debug(f"Descargando imagen desde {image_url}")
        image_data = await download_image(session, image_url)
        image_data = await optimize_image(image_data) if image_data else None
    
    if not image_data and url and not url.startswith(("patchbot://", "discord://")):
        logging.debug(f"Buscando imagen en URL: {url}")
        img_url = await get_image_from_url(session, url)
        if img_url:
            image_data = await download_image(session, img_url)
            image_data = await optimize_image(image_data) if image_data else None
    
    if not image_data:
        search_query = clean_text(title)[:100]
        logging.debug(f"Buscando imagen en Unsplash para: {search_query}")
        image_data = await get_image_from_search(session, search_query)
    
    # Reintentar búsqueda de imagen si no hay image_data
    max_retries = 3
    retry_count = 0
    while not image_data and retry_count < max_retries:
        logging.warning(f"No se encontró imagen para {title[:50]}. Reintentando búsqueda (intento {retry_count + 1}/{max_retries})")
        if url and not url.startswith(("patchbot://", "discord://")):
            img_url = await get_image_from_url(session, url)
            if img_url:
                image_data = await download_image(session, img_url)
                image_data = await optimize_image(image_data) if image_data else None
        if not image_data:
            search_query = clean_text(title)[:100]
            image_data = await get_image_from_search(session, search_query)
        retry_count += 1
    
    # Si se obtiene image_data, guardar en un directorio controlado
    if image_data:
        temp_dir = os.path.join(os.getcwd(), "temp_images")
        os.makedirs(temp_dir, exist_ok=True)
        temp_image_path = os.path.join(temp_dir, f"news_{news_hash}_{language}.jpg")
        
        logging.debug(f"Guardando imagen temporal en: {temp_image_path}")
        async with aiofiles.open(temp_image_path, "wb") as temp_file:
            await temp_file.write(image_data)
        logging.debug(f"Imagen temporal guardada en: {temp_image_path}")
    
    try:
        result = await post_tweet(session, title, summary, url, temp_image_path if temp_image_path else None, news_hash, score, source, language, trends)
        if result:
            logging.info(f"Publicación exitosa en {language}: {title[:50]}")
        else:
            logging.warning(f"Fallo al publicar noticia en {language}: {title[:50]}. Intentando encolar")
    except tweepy.TooManyRequests as e:
        logging.warning(f"Error 429 al publicar en {language}: {title[:50]}: {e}")
        return False
    except tweepy.TweepyException as e:
        logging.error(f"Error de API al publicar en {language}: {title[:50]}: {e}")
        bot_status["errors"] += 1
        await save_bot_state()
        return False
    except Exception as e:
        logging.error(f"Error inesperado al publicar en {language}: {title[:50]}: {e}")
        bot_status["errors"] += 1
        await save_bot_state()
        return False
    finally:
        # Limpiar archivo temporal si existe
        if temp_image_path and os.path.exists(temp_image_path):
            try:
                os.remove(temp_image_path)
                logging.debug(f"Imagen temporal eliminada: {temp_image_path}")
            except Exception as e:
                logging.warning(f"Error al eliminar imagen temporal {temp_image_path}: {e}")
    
    return result

async def process_publication_queue():
    """
    Procesa las noticias encoladas en la tabla cola_publicacion y las publica si es posible.
    
    Returns:
        int: Número de noticias publicadas exitosamente.
    """
    global bot_status
    published_count = 0
    
    async with aiohttp.ClientSession() as session:
        cursor.execute("SELECT * FROM cola_publicacion")
        queued_items = await run_db_sync(cursor.fetchall)
        
        for item in queued_items:
            title = item["title"]
            summary = item["summary"]
            url = item["url"]
            news_hash = item["news_hash"]
            score = item["score"]
            source = item["source"]
            language = item["language"]
            trends = json.loads(item["trends"] or "[]")
            
            category = f"write_{language}"
            # Verificar restricciones
            if time.time() < bot_status["twitter_wait_until"].get(category, 0):
                logging.debug(f"No se puede procesar noticia encolada en {language}: En período de espera")
                continue
            if bot_status["daily_tweets_total"] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
                logging.debug(f"No se puede procesar noticia encolada en {language}: Límite diario alcanzado")
                break
            if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
                logging.debug(f"No se puede procesar noticia encolada en {language}: Límite mensual alcanzado")
                break
            last_tweet_time = bot_status.get(f"last_tweet_time_{language}")
            if last_tweet_time and (time.time() - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
                logging.debug(f"No se puede procesar noticia encolada en {language}: En período de enfriamiento")
                continue
            
            logging.info(f"Procesando noticia encolada en {language}: {title[:50]} (score: {score})")
            entry = {
                "title": title,
                "summary": summary,
                "link": url,
                "source": source,
                "is_tweet": False
            }
            
            if await publish_news(session, entry, news_hash, score, language, trends):
                published_count += 1
                bot_status[f"monthly_posts_{language}"] += 1
                bot_status["daily_tweets_total"] += 1
                bot_status[f"last_tweet_time_{language}"] = datetime.now()
                bot_status["recent_processed_news"] += 1
                logging.info(f"Noticia encolada publicada en {language}: {title[:50]}")
                
                # Eliminar de la cola
                await run_db_sync(
                    cursor.execute,
                    "DELETE FROM cola_publicacion WHERE news_hash = ? AND language = ?",
                    (news_hash, language)
                )
                await run_db_sync(conn.commit)
            else:
                logging.warning(f"Fallo al publicar noticia encolada en {language}: {title[:50]}")
        
        await save_bot_state()
        return published_count

async def process_single_news(session: aiohttp.ClientSession, entry: dict, news_hash: str, trends: list[str]) -> bool:
    global bot_status
    title = entry.get("title", "Untitled")
    bot_status["last_task"] = f"Processing single news/tweet: {title[:50]}"
    logging.debug(f"Procesando noticia: {title[:50]} (source: {entry.get('source', 'Unknown')})")
    
    score = await score_news(entry, trends)
    if score < (6 if entry.get("is_tweet", False) else 2):
        logging.info(f"{'Tweet' if entry.get('is_tweet', False) else 'News'} '{title[:50]}' descartado por baja puntuación ({score:.1f})")
        return False
    
    published_status = {"en": False, "es": False}
    for language in published_status:
        category = f"write_{language}"
        if time.time() < bot_status["twitter_wait_until"].get(category, 0):
            logging.info(f"No se puede publicar en {language}: En período de espera hasta {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%H:%M:%S')}")
            continue
        if bot_status["daily_tweets_total"] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
            logging.info(f"No se puede publicar en {language}: Límite diario alcanzado ({bot_status['daily_tweets_total']}/{CONFIG['API_LIMITS']['TWEETS_PER_DAY']})")
            continue
        if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
            logging.info(f"No se puede publicar en {language}: Límite mensual alcanzado ({bot_status['monthly_posts_en'] + bot_status['monthly_posts_es']}/{CONFIG['API_LIMITS']['MONTHLY_POSTS_TOTAL']})")
            continue
        last_tweet_time = bot_status.get(f"last_tweet_time_{language}")
        if last_tweet_time and (time.time() - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
            remaining = CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"] - (time.time() - last_tweet_time.timestamp())
            logging.info(f"No se puede publicar en {language}: En período de enfriamiento ({remaining/60:.1f} minutos restantes)")
            continue
        if await is_published_in_language(title, entry.get("summary", ""), entry.get("link"), entry.get("source", "Unknown"), language):
            logging.info(f"Noticia ya publicada en {language}: {title[:50]}")
            published_status[language] = True
            continue
        
        logging.info(f"Intentando publicar noticia en {language}: {title[:50]} (score: {score:.1f})")
        try:
            if await publish_news(session, entry, news_hash, score, language, trends):
                published_status[language] = True
                bot_status[f"monthly_posts_{language}"] += 1
                bot_status["daily_tweets_total"] += 1
                bot_status[f"last_tweet_time_{language}"] = datetime.now()
                logging.info(f"Noticia publicada exitosamente en {language}: {title[:50]}")
            else:
                logging.warning(f"Fallo al publicar noticia en {language}: {title[:50]}. Intentando encolar")
        except Exception as e:
            logging.error(f"Error inesperado publicando en {language}: {title[:50]}: {e}", exc_info=True)
            bot_status["errors"] += 1
    
    if any(published_status.values()):
        source_usage[entry.get("source", "Unknown")] += 1
        bot_status["recent_processed_news"] += 1
        await save_bot_state()
        return True
    
    for language in published_status:
        if not published_status[language] and not await is_published_in_language(title, entry.get("summary", ""), entry.get("link"), entry.get("source", "Unknown"), language):
            cursor.execute("SELECT 1 FROM cola_publicacion WHERE news_hash = ? AND language = ?", (news_hash, language))
            if not await run_db_sync(cursor.fetchone):
                logging.info(f"Encolando noticia para {language}: {title[:50]}")
                trends_json = json.dumps(trends)
                try:
                    await run_db_sync(
                        cursor.execute,
                        '''INSERT INTO cola_publicacion (title, summary, url, image_url, news_hash, score, source, language, trends)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                        (title, entry.get("summary", ""), entry.get("link"), entry.get("image_url"), news_hash, score, entry.get("source", "Unknown"), language, trends_json)
                    )
                    await run_db_sync(conn.commit)
                    logging.info(f"Noticia encolada correctamente para {language}: {title[:50]}")
                except Exception as e:
                    logging.error(f"Error encolando noticia para {language}: {title[:50]}: {e}")
                    bot_status["errors"] += 1
    
    await save_bot_state()
    return False

async def process_rss_feeds(session: aiohttp.ClientSession, trends: list[str]) -> list[dict]:
    """
    Procesa los feeds RSS configurados y retorna una lista de noticias.
    Evita duplicados usando la función is_duplicate.
    Ejecuta feedparser.parse en un hilo separado.
    Modificada para usar asyncio.to_thread y await is_duplicate.
    """
    global bot_status
    bot_status["last_task"] = "Procesando feeds RSS"
    logging.info("Iniciando procesamiento de feeds RSS...")

    rss_news = []
    for feed_url in RSS_FEEDS:
        try:
            # feedparser.parse es síncrono, ejecutar en un hilo separado
            feed = await asyncio.to_thread(feedparser.parse, feed_url)

            if feed.entries:
                logging.debug(f"Procesando {len(feed.entries)} entradas del feed RSS: {feed_url}")
                for entry in feed.entries:
                    title = clean_text(entry.get("title", "Sin título"))
                    link = entry.get("link")
                    summary = clean_text(entry.get("summary", entry.get("description", "")))
                    source = feed.feed.get("title", feed_url) # Usar el título del feed como fuente si está disponible
                    # Asegurarse de que 'link' no sea None antes de usarlo
                    if not link:
                         logging.warning(f"Entrada RSS sin enlace en {feed_url}: {title[:50]}")
                         continue # Omitir entrada sin enlace

                    # Usar el enlace en el hash para asegurar unicidad por entrada del feed
                    news_hash = hashlib.sha256((title + summary + link + source).encode()).hexdigest()

                    # VERIFICAR DUPLICADOS ANTES DE AÑADIR A LA LISTA
                    # is_duplicate ahora se ejecuta en un hilo separado
                    if not await is_duplicate(title, summary, link, source):
                        date_tuple = entry.get("published_parsed") or entry.get("updated_parsed")
                        # Asegurarse de que date_tuple sea válido antes de desempaquetar
                        date = datetime(*date_tuple[:6]) if date_tuple and len(date_tuple) >= 6 else datetime.now() # Usar fecha actual como fallback
                        
                        # Filtrar noticias de más de 24 horas
                        if (datetime.now() - date).total_seconds() > 24 * 3600:
                            logging.debug(f"Noticia antigua ignorada (más de 24h): {title[:50]}...")
                            continue
                         
                        rss_news.append({
                            "title": title,
                            "link": link,
                            "summary": summary,
                            "source": source,
                            "date": date,
                            "hash": news_hash,
                            "is_tweet": False # Marcar como no-tweet
                        })
                        logging.debug(f"Noticia RSS añadida: {title[:50]}...")
                    else:
                        logging.debug(f"Noticia RSS duplicada detectada y omitida: {title[:50]}...")

            else:
                logging.info(f"No se encontraron entradas en el feed RSS: {feed_url}")

        except Exception as e:
            # Loguear el error procesando un feed específico, pero continuar con los demás
            logging.warning(f"Error procesando RSS {feed_url}: {e}")
            bot_status["errors"] += 1
            await save_bot_state() # Guardar estado si hay un error
            continue # Continuar con el siguiente feed

    logging.info(f"Procesamiento de feeds RSS completado. {len(rss_news)} noticias nuevas obtenidas.")
    return rss_news

async def process_discord_news(channel_id: int, trends: list[str]) -> list[dict]:
    global bot_status
    bot_status["last_task"] = f"Procesando noticias Discord (canal {channel_id})"
    processed_news = []
    try:
        while discord_news[channel_id]:
            entry = discord_news[channel_id].popleft()
            if not await is_duplicate(entry["title"], entry["summary"], entry["link"], entry["source"]):
                processed_news.append(entry)
        logging.info(f"Noticias Discord procesadas (canal {channel_id}): {len(processed_news)}")
    except Exception as e:
        logging.error(f"Error procesando Discord (canal {channel_id}): {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
    return processed_news

async def process_news():
    global bot_status
    bot_status["tasks_running"] += 1
    print_section_header("Procesando Noticias")
    logging.info("Iniciando ciclo de procesamiento de noticias...")

    try:
        current_time = datetime.now()
        current_time_ts = time.time()

        # Resetear contadores diarios
        if current_time.day != bot_status["last_reset"].day:
            bot_status["daily_tweets_total"] = 0
            bot_status["last_reset"] = current_time.replace(hour=0, minute=0, second=0, microsecond=0)
            bot_status["recent_tweets_en"] = []
            bot_status["recent_tweets_es"] = []
            logging.info("Contadores diarios de tweets reseteados")
            save_bot_state_sync(cursor, conn)

        # Resetear contadores mensuales
        if current_time.month != bot_status["monthly_reset"].month:
            bot_status["monthly_posts_en"] = 0
            bot_status["monthly_posts_es"] = 0
            bot_status["x_api_reads_remaining"] = CONFIG["API_LIMITS"]["MONTHLY_READS"]
            bot_status["x_api_writes_remaining"] = CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]
            bot_status["monthly_reset"] = current_time.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
            logging.info("Contadores mensuales reseteados")
            save_bot_state_sync(cursor, conn)

        trends = await get_trending_keywords()
        all_news = []

        async with aiohttp.ClientSession() as session:
            print_section_header("Procesando Fuentes")
            # Procesar Discord
            async with discord_processing_lock:
                for channel_id in [DISCORD_CHANNEL_1, DISCORD_CHANNEL_2]:
                    all_news.extend(await process_discord_news(channel_id, trends))

            # Procesar RSS
            all_news.extend(await process_rss_feeds(session, trends))

            # Procesar Twitter si no está pausado y hay lecturas disponibles
            twitter_read_paused = current_time_ts < bot_status["twitter_wait_until"].get("read", 0)
            if not twitter_read_paused and bot_status["x_api_reads_remaining"] > 0 and await check_api_rate_limit("read"):
                try:
                    twitter_news = await update_twitter_feeds()
                    all_news.extend(twitter_news)
                except Exception as e:
                    logging.warning(f"No se pudieron obtener tweets de Twitter/X: {e}. Continuando con RSS y Discord.")
            else:
                logging.info("Lecturas de Twitter/X pausadas o límite agotado. Usando solo RSS y Discord.")

            if not all_news:
                print("ℹ️ No hay noticias nuevas para procesar en este ciclo.")
                logging.info("No hay noticias nuevas para procesar en este ciclo.")
                return

            # Puntuar y ordenar noticias
            scored_items = []
            for entry in all_news:
                score = await score_news(entry, trends)
                if score > 0:
                    scored_items.append((entry, score))

            scored_items.sort(key=lambda x: (
                x[0].get("is_discord", False),
                not x[0].get("is_tweet", False),
                x[1]
            ), reverse=True)

            # Procesar noticias
            processed_count = 0
            for entry, score in scored_items:
                if await process_single_news(session, entry, entry["hash"], trends):
                    processed_count += 1
                    print(f"✅ Noticia procesada: {entry['title'][:50]} (Puntuación: {score:.1f})")
                    await asyncio.sleep(CONFIG["INTERVALS"]["API_REQUEST_DELAY_SECONDS"])

            bot_status["recent_processed_news"] = processed_count
            logging.info(f"Ciclo de procesamiento de noticias completado. Elementos procesados para publicación: {processed_count}")
            save_bot_state_sync(cursor, conn)

    except Exception as e:
        logging.error(f"❌ Error en process_news: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
    finally:
        bot_status["tasks_running"] -= 1

async def discord_news_processor():
    while True:
        try:
            await asyncio.sleep(CONFIG["INTERVALS"]["DISCORD_PROCESSING_SECONDS"])
            bot_status["last_task"] = "Procesando cola de Discord"
            await process_news()
        except Exception as e:
            logging.error(f"Error en discord_news_processor: {e}", exc_info=True)
            bot_status["errors"] += 1
            save_bot_state_sync(cursor, conn)
            await asyncio.sleep(5)
            

async def heartbeat():
    while True:
        try:
            # Calcular tiempo de actividad
            uptime = datetime.now() - bot_status["uptime"]
            uptime_str = f"{int(uptime.total_seconds() // 86400)}d {int((uptime.total_seconds() % 86400) // 3600)}h {int((uptime.total_seconds() % 3600) // 60)}m"

            # Imprimir encabezado
            print_section_header("Estado del Sistema", color="cyan")  # Cambié a cian para un look más vibrante

            # Información general
            print(f"\033[1;36m🛠️  Sistema Activo: \033[0m{uptime_str}")
            print(f"\033[1;36m🔄 Tarea Actual: \033[0m{bot_status.get('last_task', 'N/A')}")

            # Estado de conexiones
            print(f"\033[1;36m🔗 Conexiones:\033[0m")
            connections = [
                ("Twitter EN", bot_status["twitter_connected_en"]),
                ("Twitter ES", bot_status["twitter_connected_es"]),
                ("SQLite", bot_status["sqlite_connected"]),
                ("Discord", bot_status["discord_connected"])
            ]
            for name, status in connections:
                status_icon = "\033[32m✅\033[0m" if status else "\033[31m❌\033[0m"
                print(f"  {name:<12}: {status_icon}")

            # Estadísticas de tweets
            print(f"\033[1;36m📢 Tweets Publicados:\033[0m")
            print(f"  EN: {bot_status['posted_tweets_en']:<4} | ES: {bot_status['posted_tweets_es']}")
            print_progress_bar(
                bot_status["daily_tweets_total"],
                CONFIG["API_LIMITS"]["TWEETS_PER_DAY"],
                "Uso Diario",
                critical_threshold=80
            )

            # Uso de API
            print(f"\033[1;36m📊 API X:\033[0m")
            api_reads_percent = (bot_status["x_api_reads_remaining"] / CONFIG["API_LIMITS"]["MONTHLY_READS"]) * 100
            print_progress_bar(
                bot_status["x_api_reads_remaining"],
                CONFIG["API_LIMITS"]["MONTHLY_READS"],
                "Lecturas Restantes",
                critical_threshold=20
            )

            # Errores
            error_color = "31" if bot_status["errors"] > 0 else "32"  # Rojo si hay errores, verde si no
            print(f"\033[1;36m⚠️  Errores: \033[{error_color}m{bot_status['errors']}\033[0m")

            # Tiempos de espera
            print(f"\033[1;36m⏳ Restricciones:\033[0m")
            current_time = time.time()
            any_restrictions = False
            for key, wait_until in bot_status["twitter_wait_until"].items():
                if current_time < wait_until:
                    wait_time = wait_until - current_time
                    mode = {"read": "Lectura", "write_en": "Escritura EN", "write_es": "Escritura ES"}.get(key, key)
                    print(f"  {mode:<12}: Pausada por {wait_time:.0f}s")
                    any_restrictions = True
            if not any_restrictions:
                print("  Ninguna activa")

            # Separador final
            print(f"\033[1;34m{'─' * 60}\033[0m\n")

            # Esperar hasta el próximo heartbeat
            await asyncio.sleep(CONFIG["INTERVALS"]["HEARTBEAT_SECONDS"])
        except Exception as e:
            logging.error(f"\033[31m❌ Error en heartbeat: {e}\033[0m", exc_info=True)
            bot_status["errors"] += 1
            save_bot_state_sync(cursor, conn)
            await asyncio.sleep(5)

async def run_discord():
    try:
        await discord_bot.start(DISCORD_TOKEN)
    except Exception as e:
        logging.error(f"Error iniciando Discord: {e}", exc_info=True)
        bot_status["discord_connected"] = False
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)

async def main():
    global bot_status
    print_section_header("Iniciando Bot")
    logging.info("Iniciando bot...")
    bot_status["last_task"] = "Iniciando bot"
    
    # Limpiar imágenes temporales antiguas
    temp_dir = CONFIG["PATHS"]["TEMP_IMAGE_DIR"]
    for filename in os.listdir(temp_dir):
        file_path = os.path.join(temp_dir, filename)
        try:
            if os.path.isfile(file_path):
                os.unlink(file_path)
                logging.debug(f"Eliminado archivo temporal antiguo: {file_path}")
        except Exception as e:
            logging.warning(f"Error eliminando {file_path}: {e}")
    
    scheduler = AsyncIOScheduler()
    scheduler.add_job(process_news, IntervalTrigger(minutes=CONFIG["INTERVALS"]["PROCESS_NEWS_MINUTES"]), misfire_grace_time=300)
    scheduler.add_job(update_twitter_feeds, IntervalTrigger(minutes=CONFIG["INTERVALS"]["UPDATE_TWITTER_FEEDS_MINUTES"]), misfire_grace_time=300)
    scheduler.add_job(process_queue, IntervalTrigger(minutes=CONFIG["INTERVALS"]["QUEUE_PROCESSING_MINUTES"]), misfire_grace_time=300)
    scheduler.add_job(review_queue, IntervalTrigger(minutes=CONFIG["INTERVALS"]["QUEUE_PROCESSING_MINUTES"]), misfire_grace_time=300)
    
    loop = asyncio.get_event_loop()
    loop.create_task(run_discord())
    loop.create_task(heartbeat())
    loop.create_task(discord_news_processor())
    scheduler.start()
    
    logging.info("Esperando 10 segundos para estabilizar conexiones iniciales...")
    await asyncio.sleep(10)
    
    while True:
        try:
            await asyncio.sleep(3600)
            bot_status["last_task"] = "Esperando en bucle principal"
            save_bot_state_sync(cursor, conn)
        except Exception as e:
            logging.error(f"Error en bucle principal: {e}")
            bot_status["errors"] += 1
            save_bot_state_sync(cursor, conn)
            await asyncio.sleep(5)

if __name__ == "__main__":
    asyncio.run(main())
