import collections
from email.utils import parsedate_to_datetime
import logging
import random
from typing import Dict, Optional
import colorlog
from cachetools import TTLCache
from logging import handlers
from flask import app
from httpcore import TimeoutException
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from functools import partial
import feedparser
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import tweepy
import sqlite3
import hashlib
from datetime import datetime, timedelta
import os
from dotenv import load_dotenv
import asyncio
from concurrent.futures import ThreadPoolExecutor
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.triggers.cron import CronTrigger
from PIL import Image
import io
import aiofiles
import discord
from discord.ext import commands
import re
from collections import Counter, deque, defaultdict
import aiohttp
import time
import sys
from transformers import MarianTokenizer, MarianMTModel
import torch
from feedgen.feed import FeedGenerator
import urllib
import json

# Configuraci√≥n de eventos para Windows
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# Cargar variables de entorno
load_dotenv(dotenv_path="datos.env")

queue = []  # Cola global para reintentos

# Configuraci√≥n centralizada
CONFIG = {
    "API_LIMITS": {
        "MONTHLY_READS": 10000,
        "REQUESTS_PER_WINDOW": 180, # Mantener este valor, Tweepy lo usa internamente
        "RATE_LIMIT_WINDOW_SECONDS": 900,  # Mantener este valor (15 minutos)
        "TWEETS_PER_DAY": 80, # Reducir el l√≠mite diario para no agotar el mensual r√°pido
        "MONTHLY_POSTS_TOTAL": 1200,  # Reducir el l√≠mite mensual total (1500 es el m√°ximo, mejor dejar margen)
    },
    "INTERVALS": {
        "PROCESS_NEWS_MINUTES": 60,  # Procesar noticias de RSS/Discord cada hora (no usa API de X para leer)
        "CHECK_HEALTH_MINUTES": 60, # Mantener la verificaci√≥n de salud
        # *** MODIFICACI√ìN SUGERIDA 1: Aumentar este intervalo ***
        # Aumenta significativamente el tiempo entre actualizaciones de feeds de Twitter/X.
        # Prueba con 1440 (24 horas) o incluso m√°s si sigues teniendo problemas.
        "UPDATE_TWITTER_FEEDS_MINUTES": 2880, # Ejemplo: Cambiado de 720 a 1440 (24 horas)
        "HEARTBEAT_SECONDS": 300, # Mantener el heartbeat
        "TWEET_COOLDOWN_SECONDS": 60,  # 45 minutos entre tweets (ayuda a espaciar publicaciones)
        "DISCORD_MESSAGE_COOLDOWN_SECONDS": 10, # Mantener el cooldown de mensajes de Discord
        "DISCORD_PROCESSING_SECONDS": 30,  # Mantener el procesamiento frecuente de Discord
        # *** MODIFICACI√ìN SUGERIDA 2: Aumentar este retardo ***
        # Incrementa la pausa base entre procesar diferentes cuentas de Twitter en update_twitter_feeds.
        # Prueba con 120 (2 minutos) o m√°s.
        "API_REQUEST_DELAY_SECONDS": 300, # Ejemplo: Cambiado de 90 a 120
        "QUEUE_PROCESSING_MINUTES": 60,  # Procesar cola de publicaci√≥n cada 20 minutos (ajustar si es necesario)
    },
    "PATHS": {
        "RSS_CACHE_DIR": "rss_cache",
        "TEMP_IMAGE_DIR": "optimized_images",
        "CACHE_DIR": "cache",
        "DB_FILE": "bot.db",
        "DETAILED_LOG": "bot_detailed.log",
        "ERROR_LOG": "bot_errors.log",
    },
    "TWEET": {
        "MAX_LENGTH": 280,
    },
    "WEATHER": {
        "AEMET_TEXT_URL": "https://www.aemet.es/es/eltiempo/prediccion/espana",
        "AEMET_CITY_URL": "https://www.aemet.es/es/eltiempo/prediccion/municipios",
        "OUTPUT_PATH": "weather_map_fun.png",
        "WEBSITE_URL": "https://www.aemet.es",
        "BASE_MAP_URL": "https://freevectormaps.com/images/maps/spain-political-map.png"
    },
    "CITIES": [
        {"name": "Madrid", "coords": (600, 400), "temp": None, "icon": None, "aemet_id": "madrid-id28079", "region": "centro"},
        {"name": "Barcelona", "coords": (800, 300), "temp": None, "icon": None, "aemet_id": "barcelona-id08019", "region": "este"},
        {"name": "Sevilla", "coords": (500, 550), "temp": None, "icon": None, "aemet_id": "sevilla-id41091", "region": "sur"},
        {"name": "Valencia", "coords": (750, 400), "temp": None, "icon": None, "aemet_id": "valencia-id46250", "region": "este"},
        {"name": "Bilbao", "coords": (600, 200), "temp": None, "icon": None, "aemet_id": "bilbao-bilbo-id48020", "region": "norte"},
        {"name": "A Coru√±a", "coords": (400, 150), "temp": None, "icon": None, "aemet_id": "a-coruna-id15030", "region": "noroeste"},
        {"name": "M√°laga", "coords": (600, 600), "temp": None, "icon": None, "aemet_id": "malaga-id29067", "region": "sur"},
        {"name": "Palma", "coords": (900, 400), "temp": None, "icon": None, "aemet_id": "palma-id07040", "region": "islas"},
        {"name": "Las Palmas", "coords": (200, 700), "temp": None, "icon": None, "aemet_id": "las-palmas-de-gran-canaria-id35016", "region": "islas"}
    ],
    "FUN_EMOJIS": {
        "despejado": "‚òÄÔ∏è",
        "nubes": "‚òÅÔ∏è",
        "lluvia": "üåßÔ∏è",
        "truenos": "‚õàÔ∏è",
        "nieve": "‚ùÑÔ∏è",
        "niebla": "üå´Ô∏è"
    },
    "EXTRA_EMOJIS": {
        "despejado": "üå∫",
        "nubes": "üå∏",
        "lluvia": "üí¶",
        "truenos": "‚ö°",
        "nieve": "‚òÉÔ∏è",
        "niebla": "üåô"
    },
    "FUN_MESSAGES": {
        "despejado": ["¬°Sol a tope! üèñÔ∏è", "¬°Brilla! üåü"],
        "nubes": ["¬°Nubes de paseo! üö∂", "¬°Cielo t√≠mido! üí§"],
        "lluvia": ["¬°Llueve risas! üòÇ", "¬°A bailar! üíÉ"],
        "truenos": ["¬°Truenos rock! üé∏", "¬°Cielo enfadado! üò±"],
        "nieve": ["¬°Nieve m√°gica! ü™Ñ", "¬°Fr√≠o √©pico! ‚òÉÔ∏è"],
        "niebla": ["¬°Escondite! üôà", "¬°Misterio! üïµÔ∏è"]
    },
    "TEXT_COLORS": {
        "despejado": (255, 215, 0, 255),
        "nubes": (135, 206, 235, 255),
        "lluvia": (0, 191, 255, 255),
        "truenos": (255, 69, 0, 255),
        "nieve": (173, 216, 230, 255),
        "niebla": (169, 169, 169, 255)
    },
    "ELECTRICITY_MAPS": {
        "CARBON_INTENSITY_URL": "https://api.electricitymap.org/v3/carbon-intensity/latest",
        "POWER_BREAKDOWN_URL": "https://api.electricitymap.org/v3/power-breakdown/latest",
        "AUTH_TOKEN": "02wv9IMX1j6eGUciuKqw",
        "ZONE": "ES",  # Zona para Espa√±a
        "WEBSITE_URL": "https://app.electricitymaps.com"
    }
}

# Configuraci√≥n de logging
handler = colorlog.StreamHandler(stream=sys.stdout)
handler.setFormatter(colorlog.ColoredFormatter(
    '%(log_color)s%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S',
    log_colors={
        'DEBUG': 'cyan',
        'INFO': 'green',
        'WARNING': 'yellow',
        'ERROR': 'red',
        'CRITICAL': 'bold_red',
    }
))
handler.stream.reconfigure(encoding='utf-8')

logging.basicConfig(
    level=logging.INFO,
    handlers=[
        handlers.RotatingFileHandler(CONFIG["PATHS"]["DETAILED_LOG"], maxBytes=512*1024, backupCount=30, encoding='utf-8'),
        handler
    ]
)
error_handler = handlers.RotatingFileHandler(CONFIG["PATHS"]["ERROR_LOG"], maxBytes=512*1024, backupCount=10, encoding='utf-8')
error_handler.setLevel(logging.ERROR)
logging.getLogger().addHandler(error_handler)
logging.getLogger('apscheduler').setLevel(logging.WARNING)

# Variables de entorno para las APIs
TWITTER_API_KEY_EN = os.getenv("TWITTER_API_KEY_EN")
TWITTER_API_SECRET_EN = os.getenv("TWITTER_API_SECRET_EN")
TWITTER_ACCESS_TOKEN_EN = os.getenv("TWITTER_ACCESS_TOKEN_EN")
TWITTER_ACCESS_SECRET_EN = os.getenv("TWITTER_ACCESS_SECRET_EN")
TWITTER_BEARER_TOKEN_EN = os.getenv("TWITTER_BEARER_TOKEN_EN")

TWITTER_API_KEY_ES = os.getenv("TWITTER_API_KEY_ES")
TWITTER_API_SECRET_ES = os.getenv("TWITTER_API_SECRET_ES")
TWITTER_ACCESS_TOKEN_ES = os.getenv("TWITTER_ACCESS_TOKEN_ES")
TWITTER_ACCESS_SECRET_ES = os.getenv("TWITTER_ACCESS_SECRET_ES")

DISCORD_TOKEN = os.getenv("DISCORD_TOKEN")
DISCORD_CHANNEL_1 = int(os.getenv("DISCORD_CHANNEL_1"))
DISCORD_CHANNEL_2 = int(os.getenv("DISCORD_CHANNEL_2"))

duplicate_cache = TTLCache(maxsize=1000, ttl=3600)  # Cachear por 1 hora

# Lista de fuentes RSS
RSS_FEEDS = [
    "https://www.gameinformer.com/rss.xml",
    "https://www.engadget.com/gaming/rss.xml",
    "https://www.gamespot.com/feeds/news/",
    "https://blog.playstation.com/feed/",
    "https://www.engadget.com/tech/rss.xml",
    "https://kotaku.com/rss",
    "https://www.polygon.com/rss/index.xml",
    "https://www.engadget.com/rss.xml",
    "https://www.gematsu.com/feed",
    "https://www.pcgamer.com/rss",
    "https://www.gameranx.com/feed/",
    "https://www.ubisoft.com/en-us/company/newsroom/rss",
    "https://www.steampowered.com/news/feed",
    "https://www.gog.com/news/feed",
]

# Lista de cuentas de Twitter/X
TWITTER_ACCOUNTS = [
    "NoMansSky",
    "Jorge_Most_",
    "ControlMision",
    "RockstarGames",
    "NVIDIAAI",
    "PlayStation",
    "Xbox",
    "NintendoAmerica",
    "Bethesda",
    "Steam",
    "CallofDuty",
    "XboxGamePass",
    "PlayStationES",
]

# Palabras clave en ingl√©s para puntuaci√≥n y detecci√≥n de noticias
KEYWORDS_EN = {
    "gaming": {"game": 0.8, "nintendo": 1.0, "playstation": 1.0, "xbox": 1.0, "pc": 0.9, "console": 0.9},
    "tech": {"technology": 0.8, "apple": 1.0, "google": 1.0, "ai": 1.2, "hardware": 0.9, "software": 0.9},
    "news_indicators": ["release", "update", "patch", "announcement", "news", "breaking", "rotation", "free", "available"]
}

# Estructuras de datos
trending_keywords = deque(maxlen=50)
source_usage = defaultdict(int)
image_cache = {}
url_cache = {}
translation_cache = {}
discord_processing_lock = asyncio.Lock()
user_id_cache = {}  # Cach√© para IDs de usuarios de Twitter
bot_status = {
    "twitter_connected_en": False,
    "twitter_connected_es": False,
    "sqlite_connected": False,
    "discord_connected": False,
    "tasks_running": 0,
    "last_task": "Idle",
    "processed_news": 0,
    "recent_processed_news": 0,
    "posted_tweets_en": 0,
    "posted_tweets_es": 0,
    "errors": 0,
    "uptime": datetime.now(),
    "daily_tweets_total": 0,
    "monthly_posts_en": 0,
    "monthly_posts_es": 0,
    "last_reset": datetime.now().replace(hour=0, minute=0, second=0, microsecond=0),
    "monthly_reset": datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0),
    "last_tweet_time_en": None,
    "last_tweet_time_es": None,
    "x_api_reads_remaining": CONFIG["API_LIMITS"]["MONTHLY_READS"],
    "last_x_api_reset": datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0),
    "api_request_count": 0,
    "api_window_start": datetime.now(),
    "twitter_wait_until": {"read": 0, "write_en": 0, "write_es": 0}  # Tiempos de espera por categor√≠a e idioma
}

# Crear directorios
for dir_path in [CONFIG["PATHS"]["RSS_CACHE_DIR"], CONFIG["PATHS"]["TEMP_IMAGE_DIR"], CONFIG["PATHS"]["CACHE_DIR"]]:
    os.makedirs(dir_path, exist_ok=True)

# Inicializar modelo de traducci√≥n local
translation_tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-es")
translation_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-es")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
translation_model.to(device)

# Conexi√≥n a Twitter
auth_en = tweepy.OAuthHandler(TWITTER_API_KEY_EN, TWITTER_API_SECRET_EN)
auth_en.set_access_token(TWITTER_ACCESS_TOKEN_EN, TWITTER_ACCESS_SECRET_EN)
twitter_api_en = tweepy.API(auth_en, wait_on_rate_limit=True)
twitter_client_en = tweepy.Client(
    bearer_token=TWITTER_BEARER_TOKEN_EN,
    consumer_key=TWITTER_API_KEY_EN,
    consumer_secret=TWITTER_API_SECRET_EN,
    access_token=TWITTER_ACCESS_TOKEN_EN,
    access_token_secret=TWITTER_ACCESS_SECRET_EN
)

auth_es = tweepy.OAuthHandler(TWITTER_API_KEY_ES, TWITTER_API_SECRET_ES)
auth_es.set_access_token(TWITTER_ACCESS_TOKEN_ES, TWITTER_ACCESS_SECRET_ES)
twitter_api_es = tweepy.API(auth_es, wait_on_rate_limit=True)
twitter_client_es = tweepy.Client(
    consumer_key=TWITTER_API_KEY_ES,
    consumer_secret=TWITTER_API_SECRET_ES,
    access_token=TWITTER_ACCESS_TOKEN_ES,
    access_token_secret=TWITTER_ACCESS_SECRET_ES
)

# Verificar conexi√≥n a Twitter
try:
    twitter_client_en.get_me()
    logging.info("Twitter EN conectado exitosamente")
    bot_status["twitter_connected_en"] = True
except Exception as e:
    logging.error(f"Error conectando a Twitter EN: {e}")
    bot_status["twitter_connected_en"] = False

try:
    twitter_client_es.get_me()
    logging.info("Twitter ES conectado exitosamente")
    bot_status["twitter_connected_es"] = True
except Exception as e:
    logging.error(f"Error conectando a Twitter ES: {e}")
    bot_status["twitter_connected_es"] = False

# Sem√°foros y eventos para control de concurrencia y l√≠mites de tasa
read_semaphore = asyncio.Semaphore(3)
write_semaphore_en = asyncio.Semaphore(1)
write_semaphore_es = asyncio.Semaphore(1)
read_rate_limit_event = asyncio.Event()
write_rate_limit_event_en = asyncio.Event()
write_rate_limit_event_es = asyncio.Event()
read_rate_limit_event.set()  # Inicialmente no pausado
write_rate_limit_event_en.set()
write_rate_limit_event_es.set()

def print_section_header(title, color="blue"):
    """Imprime un encabezado de secci√≥n con estilo futurista."""
    color_codes = {
        "blue": "34",
        "green": "32",
        "red": "31",
        "cyan": "36",
        "magenta": "35"
    }
    color_code = color_codes.get(color, "34")  # Default a azul si el color no est√° definido
    print(f"\033[1;{color_code}m{'‚ïê' * 60}\033[0m")
    print(f"\033[1;{color_code}müöÄ {title:^56} üöÄ\033[0m")
    print(f"\033[1;{color_code}m{'‚ïê' * 60}\033[0m")

def print_progress_bar(current, total, label, critical_threshold=80):
    """Imprime una barra de progreso con colores seg√∫n el porcentaje."""
    percent = (current / total) * 100 if total > 0 else 0
    bar_length = 40
    filled = int(bar_length * percent / 100)
    bar = '‚ñà' * filled + '‚îÄ' * (bar_length - filled)
    color = "32" if percent < critical_threshold else "31"  # Verde si <80%, rojo si ‚â•80%
    print(f"\033[1m{label:<20}\033[0m \033[{color}m[{bar}] {percent:>5.1f}%\033[0m")

def get_dynamic_update_interval(account=None):
    """
    Calcula un intervalo din√°mico para actualizar feeds de Twitter/X basado en lecturas restantes y prioridad de la cuenta.
    
    Args:
        account: Nombre de la cuenta de Twitter/X (opcional).
    
    Returns:
        float: Intervalo en minutos.
    """
    base_interval = CONFIG["INTERVALS"]["UPDATE_TWITTER_FEEDS_MINUTES"]
    remaining_reads = bot_status.get("x_api_reads_remaining", CONFIG["API_LIMITS"]["MONTHLY_READS"])
    total_reads = CONFIG["API_LIMITS"]["MONTHLY_READS"]
    adjustment_factor = max(1, total_reads / max(remaining_reads, 1))
    
    # Conservar lecturas cuando son bajas
    if remaining_reads < 1000:
        adjustment_factor *= 4  # Cuadruplicar intervalo
    elif remaining_reads < 5000:
        adjustment_factor *= 2  # Duplicar intervalo
    
    dynamic_interval = base_interval * adjustment_factor
    max_interval = 720  # M√°ximo de 12 horas
    
    # Priorizar cuentas importantes
    priority_accounts = [   "NoMansSky",
    "Jorge_Most_",
    "ControlMision",
    "RockstarGames",
    "NVIDIAAI",
    "PlayStation",
    "Xbox",
    "NintendoAmerica",
    "Bethesda",
    "Steam",
    "CallofDuty",
    "XboxGamePass",
    "PlayStationES"]
    if account in priority_accounts and remaining_reads > 1000:
        dynamic_interval *= 0.5  # Reducir intervalo
    
    return min(dynamic_interval, max_interval)


def normalize_text_for_duplicate_check(text: str) -> str:
    """
    Normaliza el texto (t√≠tulo/resumen) para la verificaci√≥n de duplicados:
    min√∫sculas, elimina puntuaci√≥n y espacios extra.
    """
    if not isinstance(text, str):
        return ""
    # Eliminar puntuaci√≥n y convertir a min√∫sculas
    text = re.sub(r'[^\w\s]', '', text).lower()
    # Eliminar espacios extra
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Conexi√≥n a SQLite3 y persistencia de estado
def connect_sqlite():
    """
    Establece la conexi√≥n a la base de datos SQLite.
    Modificada para ser llamada al inicio del script.
    """
    global bot_status, conn, cursor
    try:
        # check_same_thread=False permite acceder desde diferentes hilos,
        # pero requiere que cada hilo use su propio cursor y no comparta conexiones/cursores.
        # Con run_in_executor, cada llamada a la funci√≥n de DB se ejecuta en un hilo del pool,
        # por lo que es m√°s seguro si cada funci√≥n de DB obtiene su propia conexi√≥n/cursor temporal,
        # o si la conexi√≥n global solo se usa DENTRO de las funciones ejecutadas por run_in_executor.
        conn = sqlite3.connect(CONFIG["PATHS"]["DB_FILE"], check_same_thread=False)
        cursor = conn.cursor()

        # Crear tablas si no existen
        cursor.execute('''CREATE TABLE IF NOT EXISTS historial (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            hash TEXT,
                            title TEXT,
                            url TEXT,
                            tweet TEXT,
                            relevance REAL,
                            source TEXT,
                            date TEXT,
                            engagement INTEGER,
                            summary TEXT,
                            language TEXT,
                            link TEXT,
                            UNIQUE(hash, language)
                        )''')
        cursor.execute('''CREATE INDEX IF NOT EXISTS idx_hash ON historial (hash)''')

        cursor.execute("PRAGMA table_info(historial)")
        columns = [column[1] for column in cursor.fetchall()]
        if "language" not in columns:
            logging.info("La columna 'language' no existe en 'historial'. Agreg√°ndola...")
            cursor.execute("ALTER TABLE historial ADD COLUMN language TEXT")
            conn.commit()
            logging.info("Columna 'language' agregada exitosamente a 'historial'.")

        cursor.execute('''CREATE TABLE IF NOT EXISTS bot_state (
                            key TEXT PRIMARY KEY,
                            value TEXT
                        )''')

        cursor.execute('''CREATE TABLE IF NOT EXISTS cola_publicacion (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            title TEXT,
                            summary TEXT,
                            url TEXT,
                            image_data BLOB,
                            news_hash TEXT,
                            score REAL,
                            source TEXT,
                            language TEXT,
                            trends TEXT,
                            added_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )''')

        conn.commit()
        bot_status["sqlite_connected"] = True
        return conn, cursor
    except Exception as e:
        logging.error(f"Error conectando a SQLite3: {e}", exc_info=True)
        bot_status["sqlite_connected"] = False
        bot_status["errors"] += 1
        raise
    
async def run_db_sync(func, *args):
    """
    Helper para ejecutar funciones s√≠ncronas de base de datos en un ThreadPoolExecutor.
    """
    loop = asyncio.get_event_loop()
    # Ejecuta la funci√≥n s√≠ncrona 'func' con argumentos '*args' en un hilo del pool por defecto.
    return await loop.run_in_executor(None, func, *args)

async def save_bot_state():
    """
    Funci√≥n as√≠ncrona para guardar el estado del bot usando el helper run_db_sync.
    """
    await run_db_sync(save_bot_state_sync, cursor, conn)

def load_bot_state_sync(cursor):
    """
    Carga el estado del bot desde la base de datos (versi√≥n s√≠ncrona).
    Dise√±ada para ejecutarse en un hilo separado o al inicio del script.
    """
    global bot_status
    try:
        cursor.execute("SELECT key, value FROM bot_state")
        state = dict(cursor.fetchall())

        bot_status["posted_tweets_en"] = int(state.get("posted_tweets_en", 0))
        bot_status["posted_tweets_es"] = int(state.get("posted_tweets_es", 0))
        bot_status["daily_tweets_total"] = int(state.get("daily_tweets_total", 0))
        bot_status["monthly_posts_en"] = int(state.get("monthly_posts_en", 0))
        bot_status["monthly_posts_es"] = int(state.get("monthly_posts_es", 0))
        bot_status["errors"] = int(state.get("errors", 0))
        bot_status["x_api_reads_remaining"] = int(state.get("x_api_reads_remaining", CONFIG["API_LIMITS"]["MONTHLY_READS"]))
        bot_status["api_request_count"] = int(state.get("api_request_count", 0))

        def parse_date(value, default):
            if not value:
                return default
            try:
                if isinstance(value, (int, float)):
                    return datetime.fromtimestamp(value)
                return datetime.fromisoformat(value)
            except (ValueError, TypeError):
                logging.warning(f"Valor de fecha inv√°lido: {value}. Usando predeterminado.")
                return default

        bot_status["last_reset"] = parse_date(
            state.get("last_reset"),
            datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        )
        bot_status["monthly_reset"] = parse_date(
            state.get("monthly_reset"),
            datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        )
        bot_status["last_x_api_reset"] = parse_date(
            state.get("last_x_api_reset"),
            datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        )
        bot_status["api_window_start"] = parse_date(
            state.get("api_window_start"),
            datetime.now()
        )

        bot_status["last_tweet_time_en"] = parse_date(state.get("last_tweet_time_en"), None) if state.get("last_tweet_time_en") else None
        bot_status["last_tweet_time_es"] = parse_date(state.get("last_tweet_time_es"), None) if state.get("last_tweet_time_es") else None

        bot_status["twitter_wait_until"]["read"] = float(state.get("twitter_read_wait_until", 0))
        bot_status["twitter_wait_until"]["write_en"] = float(state.get("twitter_write_wait_en", 0))
        bot_status["twitter_wait_until"]["write_es"] = float(state.get("twitter_write_wait_es", 0))

        logging.info("Estado del bot cargado correctamente")
    except Exception as e:
        logging.error(f"Error cargando estado del bot: {e}", exc_info=True)
        bot_status["last_reset"] = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        bot_status["monthly_reset"] = datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        bot_status["last_x_api_reset"] = datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        bot_status["api_window_start"] = datetime.now()
        bot_status["last_tweet_time_en"] = None
        bot_status["last_tweet_time_es"] = None

def save_bot_state_sync(cursor, conn):
    """
    Guarda el estado del bot en la base de datos (versi√≥n s√≠ncrona).
    Dise√±ada para ejecutarse en un hilo separado.
    """
    global bot_status
    try:
        state = {
            "posted_tweets_en": str(bot_status["posted_tweets_en"]),
            "posted_tweets_es": str(bot_status["posted_tweets_es"]),
            "daily_tweets_total": str(bot_status["daily_tweets_total"]),
            "monthly_posts_en": str(bot_status["monthly_posts_en"]),
            "monthly_posts_es": str(bot_status["monthly_posts_es"]),
            "errors": str(bot_status["errors"]),
            "x_api_reads_remaining": str(bot_status["x_api_reads_remaining"]),
            "api_request_count": str(bot_status["api_request_count"]),
            "last_reset": bot_status["last_reset"].isoformat() if isinstance(bot_status["last_reset"], datetime) else datetime.now().replace(hour=0, minute=0, second=0, microsecond=0).isoformat(),
            "monthly_reset": bot_status["monthly_reset"].isoformat() if isinstance(bot_status["monthly_reset"], datetime) else datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0).isoformat(),
            "last_x_api_reset": bot_status["last_x_api_reset"].isoformat() if isinstance(bot_status["last_x_api_reset"], datetime) else datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0).isoformat(),
            "api_window_start": bot_status["api_window_start"].isoformat() if isinstance(bot_status["api_window_start"], datetime) else datetime.now().isoformat(),
            "last_tweet_time_en": bot_status["last_tweet_time_en"].isoformat() if isinstance(bot_status["last_tweet_time_en"], datetime) else "",
            "last_tweet_time_es": bot_status["last_tweet_time_es"].isoformat() if isinstance(bot_status["last_tweet_time_es"], datetime) else "",
            "twitter_read_wait_until": str(bot_status["twitter_wait_until"]["read"]),
            "twitter_write_wait_en": str(bot_status["twitter_wait_until"]["write_en"]),
            "twitter_write_wait_es": str(bot_status["twitter_wait_until"]["write_es"]),
        }
        for key, value in state.items():
            cursor.execute("INSERT OR REPLACE INTO bot_state (key, value) VALUES (?, ?)", (key, value))
        conn.commit()
        logging.debug("Estado del bot guardado")
    except Exception as e:
        logging.error(f"Error guardando estado del bot: {e}", exc_info=True)

conn, cursor = connect_sqlite()
load_bot_state_sync(cursor)

# Configuraci√≥n de Discord
intents = discord.Intents.default()
intents.messages = True
intents.message_content = True
discord_bot = commands.Bot(command_prefix="recuperar", intents=intents)
discord_news = {DISCORD_CHANNEL_1: deque(maxlen=200), DISCORD_CHANNEL_2: deque(maxlen=200)}
last_message_time = 0

@discord_bot.event
async def on_ready():
    global bot_status
    logging.info(f"Discord conectado como {discord_bot.user}")
    bot_status["discord_connected"] = True

@discord_bot.event
async def on_message(message):
    """
    Procesa los mensajes de Discord para identificar y a√±adir noticias.
    Modificada para usar la funci√≥n as√≠ncrona is_duplicate y await save_bot_state.
    """
    global last_message_time, bot_status, discord_news
    bot_status["last_task"] = f"Procesando mensaje de Discord (canal {message.channel.id})"

    try:
        if message.author == discord_bot.user:
            return

        await discord_bot.process_commands(message)

        if message.channel.id not in [DISCORD_CHANNEL_1, DISCORD_CHANNEL_2]:
            return

        current_time = time.time()
        if current_time - last_message_time < CONFIG["INTERVALS"]["DISCORD_MESSAGE_COOLDOWN_SECONDS"]:
            return
        last_message_time = current_time

        is_from_bot = message.author.bot
        source = f"discord_bot_{message.author.name}" if is_from_bot else f"discord_{message.author.name}"

        urls = re.findall(
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
            message.content
        )

        # Procesar embeds
        if message.embeds:
            for embed in message.embeds:
                title = embed.title or "Sin t√≠tulo"
                summary = embed.description or message.content or "Sin descripci√≥n"
                url = embed.url or None

                if not url and embed.description:
                    embed_urls = re.findall(
                        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
                        embed.description
                    )
                    url = embed_urls[0] if embed_urls else None

                if not url:
                    url = f"discord://{message.channel.id}/{message.id}"

                # Usar la funci√≥n as√≠ncrona is_duplicate
                if await is_duplicate(title, summary, url, source):
                    logging.info(f"Duplicado detectado en embed: {title[:50]}")
                    continue

                image_url = embed.image.url if embed.image else None
                # La validaci√≥n de imagen se puede hacer m√°s tarde si es necesario para publicaci√≥n,
                # no es estrictamente necesario bloquear aqu√≠ para cada mensaje de Discord.


                news_hash = hashlib.sha256((title + summary + url + source).encode()).hexdigest()
                discord_news[message.channel.id].append({
                    "title": title,
                    "link": url,
                    "summary": summary,
                    "source": source,
                    "date": datetime.now(),
                    "hash": news_hash,
                    "image_url": image_url,
                    "is_discord": True
                })
                logging.info(f"Noticia a√±adida desde {source}: {title[:50]} - {url}")
        
        # Procesar URLs en texto plano
        elif urls:
            content = " ".join(message.content.lower().split())
            title = message.content[:100] or "Mensaje con enlace"
            summary = message.content

            for url in urls:
                # Usar la funci√≥n as√≠ncrona is_duplicate
                if await is_duplicate(title, summary, url, source):
                    logging.info(f"Duplicado detectado en URL: {title[:50]}")
                    continue

                # Validaci√≥n de URL b√°sica as√≠ncrona
                async with aiohttp.ClientSession() as session:
                    try:
                        async with session.head(url, timeout=aiohttp.ClientTimeout(total=5)) as response:
                            if response.status != 200:
                                logging.warning(f"URL no v√°lida para '{title}': {url}")
                                continue
                    except Exception as e:
                        logging.warning(f"Error validando URL para '{title}': {e}")
                        continue

                news_hash = hashlib.sha256((title + summary + url + source).encode()).hexdigest()
                # No hay image_url evidente en texto plano
                image_url = None
                discord_news[message.channel.id].append({
                    "title": title,
                    "link": url,
                    "summary": summary,
                    "source": source,
                    "date": datetime.now(),
                    "hash": news_hash,
                    "image_url": image_url,
                    "is_discord": True
                })
                logging.info(f"Noticia a√±adida desde {source} (URL): {title[:50]} - {url}")

        # Procesar mensajes que contienen palabras clave indicadoras de noticias sin URL ni embed
        elif any(keyword.lower() in message.content.lower() for keyword in KEYWORDS_EN["news_indicators"]):
            title = message.content[:100]
            summary = message.content
            url = f"discord://{message.channel.id}/{message.id}"

            # Usar la funci√≥n as√≠ncrona is_duplicate
            if await is_duplicate(title, summary, url, source):
                logging.info(f"Duplicado detectado en texto plano: {title[:50]}")
                return

            news_hash = hashlib.sha256((title + summary + url + source).encode()).hexdigest()
            # Obtener image_url de adjuntos si existen
            image_url = message.attachments[0].url if message.attachments else None
            discord_news[message.channel.id].append({
                "title": title,
                "link": url,
                "summary": summary,
                "source": source,
                "date": datetime.now(),
                "hash": news_hash,
                "image_url": image_url,
                "is_discord": True
            })
            logging.info(f"Noticia de texto a√±adida desde {source}: {title[:50]}")

    except Exception as e:
        logging.error(f"Error procesando mensaje de Discord (canal {message.channel.id}): {e}", exc_info=True)
        bot_status["errors"] += 1
        await save_bot_state() # Usar la funci√≥n as√≠ncrona para guardar estado

@discord_bot.command(name="noticias")
async def fetch_news(ctx, number: int):
    global bot_status
    bot_status["last_task"] = f"Ejecutando 'recuperarnoticias' en canal {ctx.channel.id}"
    if number <= 0 or number > 50:
        await ctx.send("‚ùå El n√∫mero debe estar entre 1 y 50. Ejemplo: `recuperarnoticias 5`")
        return

    log_message = await ctx.send("üìã **Procesando noticias...**")
    log_content = "üìã **Procesando noticias...**\n"
    await log_message.edit(content=log_content)
    
    try:
        async with aiohttp.ClientSession() as session:
            trends = await get_trending_keywords()
            news_items = []
            
            async for message in ctx.channel.history(limit=100):
                if len(news_items) >= number:
                    break
                
                if message.embeds:
                    for embed in message.embeds:
                        title = embed.title or "Sin t√≠tulo"
                        summary = embed.description or message.content or "Sin descripci√≥n"
                        url = embed.url or None
                        image_url = embed.image.url if embed.image else None
                        
                        if not url and embed.description:
                            embed_urls = re.findall(
                                r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
                                embed.description
                            )
                            url = embed_urls[0] if embed_urls else None
                        
                        if not url:
                            url = f"discord://{message.channel.id}/{message.id}"
                        
                        if url:
                            short_url = await shorten_url(url)
                            news_hash = hashlib.sha256((title + summary + short_url + f"discord_channel_{ctx.channel.id}").encode()).hexdigest()
                            news_items.append({
                                "title": title,
                                "link": short_url,
                                "summary": summary,
                                "source": f"discord_channel_{ctx.channel.id}",
                                "date": message.created_at,
                                "hash": news_hash,
                                "image_url": image_url,
                                "is_discord": True
                            })
                
                else:
                    urls = re.findall(
                        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
                        message.content
                    )
                    if urls:
                        for url in urls:
                            if len(news_items) < number:
                                short_url = await shorten_url(url)
                                news_hash = hashlib.sha256((message.content + summary + short_url + f"discord_channel_{ctx.channel.id}").encode()).hexdigest()
                                news_items.append({
                                    "title": message.content[:100],
                                    "link": short_url,
                                    "summary": message.content,
                                    "source": f"discord_channel_{ctx.channel.id}",
                                    "date": message.created_at,
                                    "hash": news_hash,
                                    "image_url": None,
                                    "is_discord": True
                                })
                    elif any(keyword.lower() in message.content.lower() for keyword in KEYWORDS_EN["news_indicators"]):
                        if len(news_items) < number:
                            title = message.content[:100]
                            summary = message.content
                            url = f"discord://{message.channel.id}/{message.id}"
                            image_url = message.attachments[0].url if message.attachments else None
                            news_hash = hashlib.sha256((title + summary + url + f"discord_channel_{ctx.channel.id}").encode()).hexdigest()
                            news_items.append({
                                "title": title,
                                "link": url,
                                "summary": summary,
                                "source": f"discord_channel_{ctx.channel.id}",
                                "date": message.created_at,
                                "hash": news_hash,
                                "image_url": image_url,
                                "is_discord": True
                            })

            processed = 0
            for i, entry in enumerate(news_items, 1):
                log_content += f"\n**Noticia {i}/{number}:** {entry['title'][:50]}...\n"
                log_content += f"**Fuente:** {entry['source']}\n"
                log_content += "Calculando puntuaci√≥n...\n"
                await log_message.edit(content=log_content)
                
                score = await score_news(entry, trends)
                log_content += f"**Puntuaci√≥n:** {score:.1f}\n"
                log_content += "üîù **Prioridad m√°xima (origen Discord)**\n"
                await log_message.edit(content=log_content)
                
                total_daily_tweets = bot_status["daily_tweets_total"]
                if total_daily_tweets >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
                    log_content += f"‚ùå **L√≠mite diario alcanzado:** {CONFIG['API_LIMITS']['TWEETS_PER_DAY']} tweets\n"
                    await log_message.edit(content=log_content)
                    break

                languages = ["en", "es"]
                for language in languages:
                    client = twitter_client_en if language == "en" else twitter_client_es
                    api = twitter_api_en if language == "en" else twitter_api_es
                    daily_key = "daily_tweets_total"
                    monthly_key = f"monthly_posts_{language}"
                    posted_key = f"posted_tweets_{language}"
                    last_tweet_key = f"last_tweet_time_{language}"
                    category = f"write_{language}"
                    semaphore = write_semaphore_en if language == "en" else write_semaphore_es
                    rate_limit_event = write_rate_limit_event_en if language == "en" else write_rate_limit_event_es

                    current_time = time.time()
                    if current_time < bot_status["twitter_wait_until"].get(category, 0):
                        log_content += f"‚è≥ **En espera para {language.upper()} hasta {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%H:%M:%S')}**\n"
                        continue

                    if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
                        log_content += f"‚ùå **L√≠mite mensual total alcanzado**\n"
                        continue

                    last_tweet_time = bot_status[last_tweet_key]
                    if last_tweet_time and (datetime.now() - last_tweet_time).total_seconds() < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
                        log_content += f"‚è≥ **Espera requerida para {language}**\n"
                        continue

                    if await is_published_in_language(entry["title"], entry["summary"], entry["link"], entry["source"], language):
                        log_content += f"‚úÖ **Ya publicado en {language.upper()}**\n"
                        continue

                    log_content += f"**Publicando en {language.upper()}...**\n"
                    await log_message.edit(content=log_content)

                    async def make_twitter_request(func, *args, **kwargs):
                        async with semaphore:
                            await rate_limit_event.wait()
                            for attempt in range(5):
                                try:
                                    result = await asyncio.to_thread(func, *args, **kwargs)
                                    return result
                                except tweepy.TweepyException as e:
                                    if e.response and e.response.status_code == 429:
                                        retry_after = int(e.response.headers.get("Retry-After", 900))
                                        reset_time = time.time() + retry_after
                                        bot_status["twitter_wait_until"][category] = reset_time
                                        rate_limit_event.clear()
                                        logging.warning(f"429 detectado en {language}. Pausando hasta {datetime.fromtimestamp(reset_time)}")
                                        await asyncio.sleep(retry_after)
                                        rate_limit_event.set()
                                    else:
                                        logging.error(f"Error en intento {attempt + 1}/5 para {language}: {e}")
                                        if attempt == 4:
                                            raise
                                        await asyncio.sleep(2 ** attempt)
                                except Exception as e:
                                    logging.error(f"Error inesperado en intento {attempt + 1}/5 para {language}: {e}")
                                    if attempt == 4:
                                        raise
                                    await asyncio.sleep(2 ** attempt)

                    try:
                        tweet_match = re.search(r'https://(nitter\.net|twitter\.com|x\.com)/(\w+)/status/(\d+)', entry["link"])
                        if tweet_match:
                            success = await make_twitter_request(
                                repost_tweet_from_url, entry["link"], entry["summary"], language
                            )
                            if success:
                                log_content += f"‚úÖ **Reposteado en {language.upper()}** (ID: {tweet_match.group(3)})\n"
                                bot_status[daily_key] += 1
                                bot_status[monthly_key] += 1
                                bot_status[posted_key] += 1
                                bot_status[last_tweet_key] = datetime.now()
                                processed += 1
                        else:
                            tweet = await generate_detailed_tweet(entry["title"], entry["summary"], entry["link"], trends, language)
                            image_url = entry.get("image_url")
                            if image_url:
                                img_data = await download_image(session, image_url)
                                if img_data:
                                    optimized_img = await optimize_image(img_data)
                                    if optimized_img:
                                        temp_image_path = os.path.join(CONFIG["PATHS"]["TEMP_IMAGE_DIR"], f"{entry['hash']}_{language}.jpg")
                                        async with aiofiles.open(temp_image_path, "wb") as f:
                                            await f.write(optimized_img)
                                        media = await make_twitter_request(api.media_upload, temp_image_path)
                                        tweet_response = await make_twitter_request(
                                            client.create_tweet, text=tweet, media_ids=[media.media_id]
                                        )
                                    else:
                                        tweet_response = await make_twitter_request(client.create_tweet, text=tweet)
                                else:
                                    tweet_response = await make_twitter_request(client.create_tweet, text=tweet)
                            else:
                                tweet_response = await make_twitter_request(client.create_tweet, text=tweet)
                            
                            log_content += f"‚úÖ **Publicado en {language.upper()}** (ID: {tweet_response.data['id']})\n"
                            bot_status[daily_key] += 1
                            bot_status[monthly_key] += 1
                            bot_status[posted_key] += 1
                            bot_status[last_tweet_key] = datetime.now()
                            processed += 1

                        news_hash = hashlib.sha256((entry["title"] + entry["summary"] + entry["link"] + entry["source"]).encode()).hexdigest()
                        cursor.execute('''INSERT OR IGNORE INTO historial (hash, title, url, tweet, relevance, source, date, engagement, summary, language)
                                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                                      (news_hash, entry["title"], entry["link"], tweet, score, entry["source"], datetime.now().isoformat(), 0, entry["summary"], language))
                        conn.commit()
                        save_bot_state_sync(cursor, conn)
                    except tweepy.TweepyException as e:
                        log_content += f"‚ùå **Error en {language.upper()}:** {str(e)}\n"
                        if e.response and e.response.status_code == 429:
                            log_content += f"‚è≥ **L√≠mite de tasa, esperando hasta {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%H:%M:%S')}**\n"
                        bot_status["errors"] += 1
                        save_bot_state_sync(cursor, conn)
                    except Exception as e:
                        log_content += f"‚ùå **Error inesperado en {language.upper()}:** {str(e)}\n"
                        bot_status["errors"] += 1
                        save_bot_state_sync(cursor, conn)

                await log_message.edit(content=log_content)
                await asyncio.sleep(1)

            log_content += f"\nüèÅ **Finalizado:** {processed} noticias publicadas."
            await log_message.edit(content=log_content)

            await asyncio.sleep(5)
            await ctx.message.delete()
            await log_message.delete()

        logging.info(f"Comando 'recuperarnoticias' completado: {processed}/{number} noticias procesadas")
        bot_status["processed_news"] += processed
        bot_status["recent_processed_news"] += processed
        save_bot_state_sync(cursor, conn)

    except Exception as e:
        logging.error(f"Error en 'recuperarnoticias' (canal {ctx.channel.id}): {e}", exc_info=True)
        log_content += f"\n‚ùå **Error general:** {str(e)}"
        await log_message.edit(content=log_content)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        await asyncio.sleep(5)
        await ctx.message.delete()
        await log_message.delete()

executor = ThreadPoolExecutor(max_workers=2)

async def check_api_rate_limit(response_headers=None, category="read", endpoint=None, exception=None):
    current_time_ts = time.time()
    
    logging.debug(f"Encabezados recibidos para {category}: {response_headers}, Tipo: {type(response_headers)}")
    
    if response_headers and isinstance(response_headers, collections.abc.Mapping):
        try:
            logging.debug(f"Procesando encabezados para {category}: {response_headers}")
            # Manejo de Retry-After
            if "Retry-After" in response_headers:
                retry_after = response_headers["Retry-After"]
                try:
                    wait_time = min(int(retry_after) + 5, 3600)  # M√°ximo 1 hora
                    logging.info(f"Retry-After detectado. Esperando {wait_time}s")
                    return wait_time
                except ValueError:
                    retry_date = parsedate_to_datetime(retry_after)
                    wait_time = min((retry_date.timestamp() - current_time_ts) + 5, 3600) if retry_date else 3600
                    logging.info(f"Retry-After (fecha) detectado. Esperando {wait_time}s")
                    return wait_time
            
            # Manejo de l√≠mites de tasa
            remaining = int(response_headers.get("x-rate-limit-remaining", float('inf')))
            reset_ts = int(response_headers.get("x-rate-limit-reset", 0))
            if remaining <= 0 and reset_ts > current_time_ts:
                wait_time = min((reset_ts - current_time_ts) + 5, 10800)  # M√°ximo 3 horas
                logging.warning(f"L√≠mite alcanzado para {category}. Esperando {wait_time:.1f}s")
                return wait_time
        except Exception as e:
            logging.error(f"Error procesando encabezados: {e}")
    
    # Respaldo seg√∫n el tipo de l√≠mite
    if exception and isinstance(exception, tweepy.TooManyRequests):
        if category.startswith("write"):
            # Suponer l√≠mite diario (50 tweets/d√≠a en plan gratuito) si no hay m√°s info
            wait_time = 86400  # 24 horas
            logging.warning(f"Posible l√≠mite diario alcanzado para {category}. Esperando {wait_time}s por defecto.")
        else:
            wait_time = 900  # 15 minutos para otros l√≠mites
            logging.warning(f"No se encontraron encabezados √∫tiles para {category}. Esperando {wait_time}s por defecto.")
        return wait_time
    
    return 0

async def make_twitter_request(func, *args, category="read", semaphore=None, rate_limit_event=None, endpoint=None, **kwargs):
    global bot_status
    semaphore = semaphore or (read_semaphore if category == "read" else (write_semaphore_en if category == "write_en" else write_semaphore_es))
    rate_limit_event = rate_limit_event or (read_rate_limit_event if category == "read" else (write_rate_limit_event_en if category == "write_en" else write_rate_limit_event_es))
    
    async with semaphore:
        for attempt in range(5):
            # Verificar l√≠mite antes del intento
            wait_time = await check_api_rate_limit(category=category, endpoint=endpoint)
            if wait_time > 0:
                logging.debug(f"Esperando {wait_time:.1f}s antes del intento {attempt + 1} para {category}")
                await asyncio.sleep(wait_time)
            
            if not rate_limit_event.is_set():
                await rate_limit_event.wait()
            
            try:
                bot_status["api_request_count"] += 1
                result = await asyncio.to_thread(func, *args, **kwargs)
                # Extraer encabezados de la respuesta
                response_headers = getattr(result, 'response', None).headers if hasattr(result, 'response') else None
                logging.debug(f"Encabezados en intento exitoso para {category}: {response_headers}")
                wait_time = await check_api_rate_limit(response_headers=response_headers, category=category, endpoint=endpoint)
                if wait_time > 0:
                    rate_limit_event.clear()
                    logging.debug(f"Pausando {wait_time:.1f}s tras respuesta para {category}")
                    await asyncio.sleep(wait_time)
                    rate_limit_event.set()
                return result
            except tweepy.TooManyRequests as e:
                # Extraer encabezados del error
                response_headers = getattr(e, 'response', None).headers if hasattr(e, 'response') else None
                logging.debug(f"Encabezados en error 429 para {category}: {response_headers}")
                wait_time = await check_api_rate_limit(response_headers=response_headers, category=category, endpoint=endpoint, exception=e)
                if wait_time > 0:
                    rate_limit_event.clear()
                    logging.warning(f"Error 429 en intento {attempt + 1}/5 para {category}: {e}. Esperando {wait_time}s")
                    await asyncio.sleep(wait_time)
                    rate_limit_event.set()
                elif attempt == 4:
                    logging.error(f"Fallo tras 5 intentos por 429 para {category}: {e}")
                    raise
            except Exception as e:
                logging.error(f"Error inesperado en intento {attempt + 1}/5 para {category}: {e}")
                if attempt == 4:
                    raise
                await asyncio.sleep(2 ** attempt)
                
async def generate_rss_feed(username: str, num_tweets: int = 5) -> str | None:
    """
    Genera un feed RSS para un usuario de Twitter/X.
    Utiliza make_twitter_request para manejar l√≠mites de API.
    """
    global bot_status
    bot_status["last_task"] = f"Generando feed RSS para @{username}"
    logging.debug(f"Intentando generar feed RSS para @{username}")

    category = "read"
    current_time_ts = time.time()

    # Verificar si las lecturas est√°n pausadas antes de intentar generar el feed
    if current_time_ts < bot_status["twitter_wait_until"].get(category, 0):
        logging.warning(f"Lecturas de Twitter/X pausadas hasta {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%Y-%m-%d %H:%M:%S')}. Omitiendo generaci√≥n de feed para @{username}")
        return None

    # make_twitter_request ya maneja el l√≠mite mensual y por ventana,
    # as√≠ que no necesitamos verificaciones expl√≠citas aqu√≠, solo confiar en √©l.

    try:
        # Obtener ID de usuario (usando cach√© y make_twitter_request)
        if username in user_id_cache:
            user_id = user_id_cache[username]
            logging.debug(f"Usuario @{username} encontrado en cach√© con ID: {user_id}")
        else:
            logging.debug(f"Buscando ID de usuario para @{username} via API.")
            user_response = await make_twitter_request(
                twitter_client_en.get_user,
                username=username,
                category=category
            )
            if not user_response or not user_response.data:
                logging.warning(f"No se encontr√≥ el usuario @{username} o error al obtenerlo.")
                return None
            user_id = user_response.data.id
            user_id_cache[username] = user_id
            logging.debug(f"ID de usuario para @{username} obtenido: {user_id}")
            save_bot_state_sync(cursor, conn) # Guardar user_id_cache si es necesario persistirlo

        # Obtener tweets del usuario (usando make_twitter_request)
        logging.debug(f"Obteniendo tweets para el usuario {user_id} (@{username}).")
        tweets_response = await make_twitter_request(
            twitter_client_en.get_users_tweets,
            id=user_id,
            max_results=num_tweets,
            tweet_fields=["created_at"], # Solicitar solo los campos necesarios
            category=category
        )

        if not tweets_response or not tweets_response.data:
            logging.info(f"No se encontraron tweets recientes para @{username} o error al obtenerlos.")
            return None

        fg = FeedGenerator()
        fg.title(f"Tweets from @{username}")
        fg.link(href=f"https://x.com/{username}", rel="alternate")
        fg.description(f"Latest tweets from @{username}")

        for tweet in tweets_response.data:
            fe = fg.add_entry()
            fe.title(tweet.text[:100] + "..." if len(tweet.text) > 100 else tweet.text)
            fe.link(href=f"https://x.com/{username}/status/{tweet.id}")
            fe.description(tweet.text)
            # Asegurarse de que created_at es un objeto datetime antes de formatear
            if isinstance(tweet.created_at, datetime):
                 fe.pubDate(tweet.created_at.isoformat())
            else:
                 logging.warning(f"Formato de fecha inesperado para tweet {tweet.id}: {tweet.created_at}")
                 fe.pubDate(datetime.now().isoformat()) # Usar fecha actual como fallback


        rss_file = os.path.join(CONFIG["PATHS"]["RSS_CACHE_DIR"], f"{username}.rss")
        fg.rss_file(rss_file)
        logging.info(f"Feed RSS generado exitosamente para @{username}: {rss_file}")
        return rss_file

    except Exception as e:
        logging.error(f"Error generando RSS para @{username}: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return None

async def update_twitter_feeds():
    """
    Actualiza los feeds de Twitter/X obteniendo los √∫ltimos tweets de las cuentas configuradas.
    
    Returns:
        list: Lista de noticias/tweets obtenidos.
    """
    twitter_news = []
    async with aiohttp.ClientSession() as session:
        for account in TWITTER_ACCOUNTS:
            interval = get_dynamic_update_interval(account)
            retry_delay = 1  # Retraso inicial para retroceso exponencial
            for attempt in range(3):
                try:
                    # Verificar existencia de la cuenta
                    user = await make_twitter_request(
                        twitter_client_en.get_user,
                        username=account,
                        category="read",
                        endpoint=f"users/by/username/{account}"
                    )
                    if not user.data:
                        logging.warning(f"Cuenta {account} no encontrada o inaccesible")
                        break
                    
                    tweets = await make_twitter_request(
                        twitter_client_en.get_users_tweets,
                        id=user.data.id,
                        max_results=10,
                        exclude=["retweets", "replies"],
                        category="read",
                        endpoint=f"users/{user.data.id}/tweets"
                    )
                    if tweets.data:
                        for tweet in tweets.data:
                            news_entry = {
                                "title": tweet.text[:50],
                                "summary": tweet.text,
                                "link": f"https://twitter.com/{account}/status/{tweet.id}",
                                "source": f"Twitter: {account}",
                                "is_tweet": True
                            }
                            twitter_news.append(news_entry)
                    break  # Salir si la solicitud es exitosa
                except tweepy.TooManyRequests as e:
                    logging.warning(f"Error 429 obteniendo tweets de {account} en intento {attempt + 1}/3: {e}")
                    if attempt == 2:
                        logging.error(f"No se pudieron obtener tweets de {account} tras 3 intentos por 429")
                        break
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2  # Retroceso exponencial
                except tweepy.TweepyException as e:
                    logging.warning(f"Error obteniendo tweets de {account} en intento {attempt + 1}/3: {e}")
                    if attempt == 2:
                        logging.error(f"No se pudieron obtener tweets de {account} tras 3 intentos")
                        break
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2
                except Exception as e:
                    logging.error(f"Error inesperado procesando {account}: {e}")
                    break
    return twitter_news

def clean_text(text: str) -> str:
    if not text or not isinstance(text, str):
        return ""
    text = re.sub(r'<.*?>', '', text)  # Eliminar etiquetas HTML
    text = re.sub(r'[^\w\s.,!?\'"-]', '', text)  # Mantener comillas y guiones
    return text.strip()

def post_process_translation(translated_text: str) -> str:
    corrections = {
        "interruptor": "Switch",
        "deja caer": "no incluir√°",
        "preordenes": "preventas",
        "carro": "cartucho",
        "a√±ade el": "a√±ade soporte para"
    }
    for wrong, correct in corrections.items():
        translated_text = translated_text.replace(wrong, correct)
    return translated_text

def translate_text(text: str) -> str:
    cleaned_text = clean_text(text)
    if not cleaned_text:
        return text
    if cleaned_text in translation_cache:
        return translation_cache[cleaned_text]
    try:
        inputs = translation_tokenizer(cleaned_text, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
        with torch.inference_mode():
            translated = translation_model.generate(**inputs)
        translated_text = translation_tokenizer.decode(translated[0], skip_special_tokens=True)
        translation_cache[cleaned_text] = translated_text
        return translated_text
    except Exception as e:
        logging.error(f"Error en MarianMT para texto '{cleaned_text[:50]}...': {e}")
        return cleaned_text

async def fetch_url(session: aiohttp.ClientSession, url: str) -> bytes | None:
    """
    Obtiene el contenido binario de una URL.
    """
    global bot_status
    # No actualizamos last_task aqu√≠ ya que es una funci√≥n auxiliar llamada por otras
    try:
        # Aumentar un poco el timeout para descargas de im√°genes potencialmente m√°s grandes
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=15)) as response:
            if response.status != 200:
                logging.warning(f"Error obteniendo {url}: C√≥digo de estado {response.status}")
                return None
            # Verificar el tipo de contenido para asegurar que es una imagen o HTML (para scraping)
            content_type = response.headers.get("Content-Type", "").lower()
            if not (content_type.startswith("image/") or "text/html" in content_type):
                 logging.warning(f"Contenido no es imagen ni HTML en {url}: {content_type}")
                 return None
            return await response.read()
    except Exception as e:
        # Loguear como debug o warning si es un error com√∫n de red, error si es inesperado
        logging.debug(f"Error obteniendo {url}: {e}")
        # No incrementar contador de errores aqu√≠ para errores de red comunes, solo para errores l√≥gicos.
        return None

async def get_image_from_url(session: aiohttp.ClientSession, url: str, max_attempts: int = 5) -> str | None:
    """
    Intenta encontrar y validar URLs de im√°genes en una p√°gina web.
    Prioriza im√°genes Open Graph y Twitter Card, y luego busca en etiquetas <img>.
    Intenta validar las URLs encontradas.
    """
    logging.debug(f"Attempting to get image URL from: {url}")
    try:
        # Use fetch_url to get the page content
        html_content_bytes = await fetch_url(session, url)
        if not html_content_bytes:
            logging.debug(f"Could not get content from {url} to search for images.")
            return None

        # Decode HTML content, trying different encodings
        try:
            html_content = html_content_bytes.decode('utf-8')
        except UnicodeDecodeError:
            try:
                html_content = html_content_bytes.decode('latin-1')
            except Exception as e:
                logging.warning(f"Could not decode content from {url}: {e}")
                return None

        soup = BeautifulSoup(html_content, "html.parser")
        image_urls = []

        # Prioritize meta tags (Open Graph and Twitter Card)
        og_image = soup.find("meta", attrs={"property": "og:image"})
        if og_image and og_image.get("content"):
            image_urls.append(og_image.get("content"))
            logging.debug(f"Found Open Graph image: {og_image.get('content')}")

        twitter_image = soup.find("meta", attrs={"name": "twitter:image"})
        if twitter_image and twitter_image.get("content"):
             image_urls.append(twitter_image.get("content"))
             logging.debug(f"Found Twitter Card image: {twitter_image.get('content')}")

        # Search for <img> tags. Try to find URLs that look high-resolution
        img_tags = soup.find_all("img", attrs={"src": True})
        for tag in img_tags:
            img_url = tag.get("src")
            if img_url and img_url.startswith(("http://", "https://")):
                 # Simple heuristic: check for common high-res indicators in URL
                 if any(indicator in img_url.lower() for indicator in ["large", "full", "original"]):
                     image_urls.insert(0, img_url) # Add to the beginning to prioritize
                 else:
                    image_urls.append(img_url)

        # Remove duplicates while preserving order
        image_urls = list(dict.fromkeys(image_urls))

        logging.debug(f"Image URLs found (including meta and img): {image_urls}")

        # Validate the found URLs and return the first valid one
        for img_url in image_urls[:max_attempts]: # Limit the number of validations
            if await validate_image_url(session, img_url):
                logging.debug(f"Validated image URL: {img_url}")
                return img_url
            else:
                logging.debug(f"Image URL not valid or inaccessible: {img_url}")


        logging.info(f"No valid images found on {url} after {max_attempts} validation attempts.")
        return None

    except Exception as e:
        logging.error(f"Error getting image URL from {url}: {e}", exc_info=True)
        # Decide if this is a critical error for the bot
        # bot_status["errors"] += 1
        # save_bot_state_sync(cursor, conn)
        return None

async def validate_image_url(session: aiohttp.ClientSession, img_url: str) -> bool:
    """
    Verifica si una URL apunta a una imagen v√°lida y accesible.
    """
    try:
        # Usar m√©todo HEAD para verificar sin descargar todo el contenido
        async with session.head(img_url, timeout=5) as response:
            if response.status != 200:
                logging.debug(f"Validaci√≥n HEAD fallida para {img_url}: {response.status}")
                return False
            content_type = response.headers.get("Content-Type", "").lower()
            # Verificar si el tipo de contenido es una imagen
            if not content_type.startswith("image/"):
                 logging.debug(f"Validaci√≥n HEAD fallida para {img_url}: Content-Type no es imagen ({content_type})")
                 return False
            # Opcional: Verificar tama√±o m√≠nimo/m√°ximo si es relevante
            content_length = int(response.headers.get("Content-Length", 0))
            if content_length > 0 and content_length < 1024: # Ejemplo: ignorar im√°genes muy peque√±as (<1KB)
                 logging.debug(f"Validaci√≥n HEAD fallida para {img_url}: Imagen demasiado peque√±a ({content_length} bytes)")
                 return False

            logging.debug(f"Validaci√≥n HEAD exitosa para {img_url}")
            return True
    except Exception as e:
        logging.debug(f"Error durante validaci√≥n HEAD para {img_url}: {e}")
        return False
    
async def download_image(session: aiohttp.ClientSession, image_url: str) -> bytes | None:
    """
    Descarga el contenido binario de una URL de imagen.
    Verifica si los datos descargados son una imagen v√°lida.
    Manejo m√°s robusto de errores y validaci√≥n.
    """
    logging.debug(f"Attempting to download image from: {image_url}")
    try:
        # Use fetch_url to download binary content
        # fetch_url should already handle basic status code checks
        image_data = await fetch_url(session, image_url)
        if not image_data:
            logging.debug(f"Failed to download image data from {image_url}.")
            return None

        # Verify if the downloaded data is a valid image using PIL
        try:
            # Use a context manager for the image to ensure it's closed
            with Image.open(io.BytesIO(image_data)) as img:
                img.verify() # Verify if it's a valid image without loading fully
                # Check image format if needed, e.g., exclude GIFs if not supported by Twitter
                if img.format not in ['JPEG', 'PNG', 'WEBP']: # Add/remove formats as needed
                     logging.warning(f"Downloaded data from {image_url} is in unsupported format: {img.format}")
                     return None
                logging.debug(f"Downloaded data from {image_url} verified as valid image.")
            # Return the original downloaded data if verification is successful
            return image_data
        except Exception as e:
            # Catch specific PIL errors if possible, or a general Exception
            logging.warning(f"Downloaded data from {image_url} is not a valid image or verification failed: {e}")
            return None

    except Exception as e:
        # Log as error for unexpected issues during download
        logging.error(f"Error downloading image from {image_url}: {e}", exc_info=True)
        # Do not increment error counter for common network issues, only critical logic errors.
        return None

async def shorten_url(url: str) -> str:
    global bot_status
    bot_status["last_task"] = f"Acortando URL: {url}"
    try:
        if url in url_cache:
            return url_cache[url]
        cache_path = os.path.join(CONFIG["PATHS"]["CACHE_DIR"], f"{hashlib.sha256(url.encode()).hexdigest()}.url")
        if os.path.exists(cache_path):
            async with aiofiles.open(cache_path, "r") as f:
                short_url = await f.read()
                url_cache[url] = short_url
                return short_url
        async with aiohttp.ClientSession() as session:
            async with session.get(f"http://tinyurl.com/api-create.php?url={url}") as response:
                if response.status != 200:
                    logging.error(f"Error acortando {url}: C√≥digo de estado {response.status}")
                    return url
                short_url = await response.text()
                async with aiofiles.open(cache_path, "w") as f:
                    await f.write(short_url)
                url_cache[url] = short_url
                return short_url
    except Exception as e:
        logging.error(f"Error acortando {url}: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return url

async def get_trending_keywords() -> list[str]:
    global trending_keywords, bot_status
    bot_status["last_task"] = "Obteniendo tendencias"
    try:
        return list(KEYWORDS_EN["gaming"].keys()) + list(KEYWORDS_EN["tech"].keys())
    except Exception as e:
        logging.error(f"Error obteniendo tendencias: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return list(KEYWORDS_EN["gaming"].keys()) + list(KEYWORDS_EN["tech"].keys())

def calculate_relevance(title: str, summary: str) -> float:
    content = (title + " " + summary).lower()
    keyword_score = 0
    for category in ["gaming", "tech"]:
        kw_dict = KEYWORDS_EN[category]
        for kw, weight in kw_dict.items():
            if kw in content:
                keyword_score += weight * 1.5
    for kw in KEYWORDS_EN["news_indicators"]:
        if kw in content:
            keyword_score += 0.75
    return keyword_score

def calculate_freshness(news_date: datetime) -> float:
    try:
        age_hours = (datetime.now() - news_date).total_seconds() / 3600
        return max(0, 1 - 0.1 * age_hours)
    except Exception as e:
        logging.error(f"Error calculando frescura: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return 0

async def is_duplicate(title: str, summary: str, url: str, source: str, entry_date: datetime = None) -> bool:
    """
    Verifica si una noticia es un duplicado bas√°ndose en hash.

    Args:
        title (str): T√≠tulo de la noticia.
        summary (str): Resumen de la noticia.
        url (str): URL de la noticia.
        source (str): Fuente de la noticia.
        entry_date (datetime, optional): Fecha de publicaci√≥n de la noticia.

    Returns:
        bool: True si la noticia es un duplicado, False si es nueva.
    """
    news_hash = hashlib.sha256((title + summary + url + source).encode()).hexdigest()
    cache_key = f"{news_hash}_duplicate"

    # Verificar si es reciente (dentro de 24 horas) y est√° en cach√©
    if entry_date and (datetime.now() - entry_date).total_seconds() < 86400:  # 24 horas
        if cache_key in duplicate_cache:
            logging.debug(f"‚úÖ Duplicado encontrado en cach√©: {title[:50]}")
            return duplicate_cache[cache_key]
    
    try:
        # Crear un cursor local
        local_cursor = conn.cursor()
        try:
            local_cursor.execute('''SELECT 1 FROM historial WHERE hash = ?''', (news_hash,))
            result = await run_db_sync(local_cursor.fetchone)
            is_duplicate = result is not None
        finally:
            local_cursor.close()  # Cerrar el cursor expl√≠citamente

        # Almacenar en cach√© si es reciente
        if entry_date and (datetime.now() - entry_date).total_seconds() < 86400:
            duplicate_cache[cache_key] = is_duplicate

        # Registrar resultado
        if is_duplicate:
            logging.debug(f"‚ùå Noticia duplicada: {title[:50]}")
        else:
            logging.debug(f"‚úÖ Noticia nueva: {title[:50]}")

        return is_duplicate
    except Exception as e:
        logging.error(f"‚ùå Error verificando duplicado: {e}", exc_info=True)
        bot_status["errors"] += 1
        await save_bot_state()
        return False
    
async def is_published_in_language(title: str, summary: str, url: str, source: str, language: str) -> bool:
     """
     Checks if a news item has already been published in a specific language.
     Uses robust criteria, including the language.
     Executes the database query in a separate thread using run_db_sync.
     """
     # Add checks for None or empty strings for inputs
     if not title or not url or not source or not language:
         return False # Cannot check with missing info

     try:
         # Use normalized text for title and summary if needed for language-specific check
         # normalized_title = normalize_text_for_duplicate_check(title)
         # normalized_summary = normalize_text_for_duplicate_check(summary)

         # Query includes language
         result = await run_db_sync(
             cursor.execute,
             "SELECT 1 FROM historial WHERE title = ? AND summary = ? AND url = ? AND source = ? AND language = ?",
             (title, summary, url, source, language)
         )
         fetch_result = await run_db_sync(result.fetchone)
         is_published = fetch_result is not None
         # logging.debug(f"Published check for '{title[:50]}' in {language}: {is_published}") # Too verbose
         return is_published
     except Exception as e:
         logging.error(f"Error in is_published_in_language: {e}", exc_info=True)
         # Decide if this is a critical error
         # bot_status["errors"] += 1
         # await save_bot_state()
         return False # Assume not published in case of DB error
     
try:
    tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
    bert_model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
    bert_model.eval()
    if torch.cuda.is_available():
        bert_model.to("cuda")
    logging.info("Modelo DistilBERT y tokenizer inicializados correctamente.")
except Exception as e:
    logging.error(f"Error al inicializar DistilBERT: {e}", exc_info=True)
    tokenizer = None
    bert_model = None

async def score_news(entry, trends):
    if tokenizer is None or bert_model is None:
        logging.error("Modelo DistilBERT o tokenizer no inicializados.")
        return 0.0

    try:
        text = entry.get('title', '')
        if not text:
            logging.warning("T√≠tulo de noticia vac√≠o.")
            return 0.0
    
        # Filtrar noticias de gaming/tecnolog√≠a
        text_lower = text.lower()
        if any(keyword in text_lower for keyword in KEYWORDS_EN):
            logging.debug(f"Noticia descartada por tema de gaming/tecnolog√≠a: {text}")
            return 0.0  # Puntuaci√≥n 0 para descartar

        def run_bert(inputs):
            with torch.no_grad():
                outputs = bert_model(**inputs)
                probs = torch.softmax(outputs.logits, dim=-1)
                return probs[0][1].item()  # Probabilidad de la clase positiva

        inputs = tokenizer(
            text,
            return_tensors="pt",
            max_length=128,
            truncation=True,
            padding=True
        )
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}

        loop = asyncio.get_running_loop()
        score = await loop.run_in_executor(None, partial(run_bert, inputs))

        # Ajustar con tendencias
        trend_score = 0.0
        for trend in trends:
            if trend.lower() in text_lower:
                trend_score += 20.0  # Aumentar puntaje en escala 0-100
        final_score = min(score * 100 + trend_score, 100.0)  # Escala 0-100

        logging.debug(f"Noticia puntuada: {text} con score {final_score} (base: {score*100}, tendencias: {trend_score})")
        return final_score

    except Exception as e:
        logging.error(f"Error en score_news: {e}", exc_info=True)
        return 0.0

async def optimize_image(image_data: bytes) -> bytes | None:
    """
    Optimiza una imagen para reducir su tama√±o manteniendo una calidad aceptable.
    Utiliza Pillow para redimensionar y ajustar la calidad JPEG.
    Ejecuta operaciones de Pillow en un hilo separado.
    Manejo de errores mejorado.
    """
    loop = asyncio.get_event_loop()
    # Ensure image_data is not None before proceeding
    if image_data is None:
        logging.warning("optimize_image received None data.")
        return None

    logging.debug(f"Starting image optimization. Original size: {len(image_data)} bytes.")

    try:
        # Define a synchronous function for Pillow operations
        def optimize_image_sync(img_data_bytes):
            try:
                # Use a context manager for the image
                with Image.open(io.BytesIO(img_data_bytes)) as img:
                    # Ensure image is in RGB mode for JPEG saving
                    if img.mode != 'RGB':
                        img = img.convert("RGB")

                    max_long_dim = 1280
                    width, height = img.size
                    # Resize if necessary
                    if max(width, height) > max_long_dim:
                        if width > height:
                            new_width = max_long_dim
                            new_height = int(height * (new_width / width))
                        else:
                            new_height = max_long_dim
                            new_width = int(width * (new_height / height))
                        # Use LANCZOS for better quality resizing
                        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)

                    output = io.BytesIO()
                    jpeg_quality = 85
                    # Try saving with initial quality
                    img.save(output, format="JPEG", quality=jpeg_quality, optimize=True)
                    optimized_data = output.getvalue()

                    # Re-optimize with lower quality if size reduction is not significant
                    # Check if optimized size is still close to original AND size is large
                    if len(optimized_data) >= len(img_data_bytes) * 0.95 and len(optimized_data) > 100 * 1024 and jpeg_quality > 80: # Added size check > 100KB
                         logging.debug("Optimized image size not significantly smaller, attempting lower quality.")
                         output = io.BytesIO() # Reset buffer
                         img.save(output, format="JPEG", quality=80, optimize=True)
                         optimized_data = output.getvalue()

                    # Final check on size, Twitter limit is 5MB for most image types
                    if len(optimized_data) > 5 * 1024 * 1024:
                         logging.warning(f"Optimized image is still too large: {len(optimized_data)} bytes.")
                         return None # Return None if still too large

                    return optimized_data

            except Exception as e:
                # Log the error within the synchronous function
                logging.error(f"Error optimizing image in thread: {e}", exc_info=True)
                return None

        # Execute the synchronous function in the executor
        optimized_data = await loop.run_in_executor(None, optimize_image_sync, image_data)

        if optimized_data:
            logging.debug(f"Image optimized successfully. Final size: {len(optimized_data)} bytes.")
        else:
            logging.warning("Image optimization failed or resulted in invalid data.")

        return optimized_data

    except Exception as e:
        # Log general errors from the async wrapper
        logging.error(f"General error in optimize_image async wrapper: {e}", exc_info=True)
        # bot_status["errors"] += 1 # Decide if this is a critical bot error
        # await save_bot_state()
        return None

async def translate_to_spanish(text: str) -> str:
    cleaned_text = clean_text(text)
    if not cleaned_text:
        return text
    loop = asyncio.get_event_loop()
    translated = await loop.run_in_executor(executor, translate_text, cleaned_text)
    translated = post_process_translation(translated)
    return translated

def get_summary_sentences(summary: str, max_chars: int) -> str:
    """
    Divide el resumen en oraciones y selecciona las que caben en max_chars.
    """
    sentences = re.split(r'(?<=[.!?])\s+', summary)
    selected_sentences = []
    current_length = 0
    for sentence in sentences:
        sentence_length = len(sentence) + 1  # +1 para el espacio
        if current_length + sentence_length > max_chars:
            break
        selected_sentences.append(sentence)
        current_length += sentence_length
    return " ".join(selected_sentences).strip()

async def generate_detailed_tweet(title: str, summary: str, url: str, trends: list[str], language: str) -> str:
    """
    Genera un tweet de m√°xima calidad usando an√°lisis sem√°ntico avanzado, ganchos ultra atractivos,
    contenido optimizado y hashtags hiper relevantes, inspirado en @IGN y @GameSpot, sin costo alguno.
    """
    short_url = await shorten_url(url)
    max_content_length = CONFIG["TWEET"]["MAX_LENGTH"] - len(short_url) - 30  # Reservar espacio para gancho, hashtags, emojis

    # An√°lisis sem√°ntico: clasificar tipo de noticia, tono y extraer entidades
    title_lower = title.lower()
    summary_lower = summary.lower()
    news_type = "default"
    emoji = "üéÆ"
    tone = "enthusiastic"
    for pattern, n_type, n_emoji, n_tone in [
        ("release|launch|available|out now", "release", "üéÆ", "enthusiastic"),
        ("update|patch|fix|improvement", "update", "üîß", "informative"),
        ("announcement|reveal|trailer|teaser", "announce", "üö®", "urgent"),
        ("free|deal|sale|discount|offer", "deal", "üí∏", "promotional"),
        ("break|urgent|alert|emergency", "urgent", "üî•", "urgent")
    ]:
        if any(keyword in title_lower or keyword in summary_lower for keyword in pattern.split("|")):
            news_type = n_type
            emoji = n_emoji
            tone = n_tone
            break

    # Extraer entidades clave (juegos, plataformas, desarrolladores)
    entities = []
    entity_patterns = [
        (r"\b(playstation|ps[45])\b", "PlayStation"),
        (r"\b(xbox|series [xs])\b", "Xbox"),
        (r"\b(nintendo|switch)\b", "Nintendo"),
        (r"\b(pc|steam|epic games)\b", "PC"),
        (r"\b(elden ring|cyberpunk 2077|call of duty|fortnite|god of war|zelda|halo)\b", lambda m: m.group(0).title()),
        (r"\b(fromsoftware|ubisoft|activision|ea sports|square enix)\b", lambda m: m.group(0).title())
    ]
    for pattern, entity_name in entity_patterns:
        for text in [title_lower, summary_lower]:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                entities.append(entity_name(match) if callable(entity_name) else entity_name)
    entities = list(dict.fromkeys(entities))[:2]  # Limitar a 2 entidades √∫nicas

    # Generar gancho din√°mico basado en tipo, tono y entidades
    hooks_en = {
        "release": {
            "enthusiastic": lambda e: f"{emoji} {e[0] if e else 'Epic game'} just dropped! Dive in!",
            "default": lambda e: f"{emoji} {e[0] if e else 'New title'} hits the scene!"
        },
        "update": {
            "informative": lambda e: f"{emoji} {e[0] if e else 'Game'} gets a fresh update! Details!",
            "default": lambda e: f"{emoji} New patch for {e[0] if e else 'your fave'}!"
        },
        "announce": {
            "urgent": lambda e: f"{emoji} Huge {e[0] if e else 'game'} reveal! Must-see!",
            "default": lambda e: f"{emoji} {e[0] if e else 'Big news'} just announced!"
        },
        "deal": {
            "promotional": lambda e: f"{emoji} Steal {e[0] if e else 'games'} now! Hot deal!",
            "default": lambda e: f"{emoji} Save big on {e[0] if e else 'top titles'}!"
        },
        "urgent": {
            "urgent": lambda e: f"{emoji} Alert! {e[0] if e else 'Game'} news you need NOW!",
            "default": lambda e: f"{emoji} Urgent {e[0] if e else 'gaming'} update!"
        },
        "default": {
            "enthusiastic": lambda e: f"{emoji} {e[0] if e else 'Gaming'} news to hype you up!",
            "default": lambda e: f"{emoji} Fresh scoop for {e[0] if e else 'gamers'}!"
        }
    }
    hooks_es = {
        "release": {
            "enthusiastic": lambda e: f"{emoji} ¬°{e[0] if e else 'Juego √©pico'} ya lleg√≥! ¬°Juega ya!",
            "default": lambda e: f"{emoji} ¬°{e[0] if e else 'Nuevo t√≠tulo'} aterriza hoy!"
        },
        "update": {
            "informative": lambda e: f"{emoji} ¬°{e[0] if e else 'Juego'} actualizado! ¬°Mira qu√© hay!",
            "default": lambda e: f"{emoji} ¬°Parche nuevo para {e[0] if e else 'tu favorito'}!"
        },
        "announce": {
            "urgent": lambda e: f"{emoji} ¬°Bombazo! ¬°{e[0] if e else 'Juego'} revelado!",
            "default": lambda e: f"{emoji} ¬°{e[0] if e else 'Gran noticia'} anunciada!"
        },
        "deal": {
            "promotional": lambda e: f"{emoji} ¬°Oferta! ¬°Consigue {e[0] if e else 'juegos'} ya!",
            "default": lambda e: f"{emoji} ¬°Ahorra en {e[0] if e else 't√≠tulos top'}!"
        },
        "urgent": {
            "urgent": lambda e: f"{emoji} ¬°Alerta! ¬°{e[0] if e else 'Noticia'} que debes saber!",
            "default": lambda e: f"{emoji} ¬°Urgente para {e[0] if e else 'gamers'}!"
        },
        "default": {
            "enthusiastic": lambda e: f"{emoji} ¬°{e[0] if e else 'Gaming'} al rojo vivo!",
            "default": lambda e: f"{emoji} ¬°Lo √∫ltimo para {e[0] if e else 'gamers'}!"
        }
    }
    hook_dict = hooks_en if language == "en" else hooks_es
    hook = hook_dict[news_type].get(tone, hook_dict[news_type]["default"])(entities)

    # Optimizar contenido: seleccionar la frase m√°s impactante
    content_source = title if len(title) < len(summary) and title else summary
    if not content_source:
        content_source = title or summary or "Check out this gaming news!"
    sentences = re.split(r'[.!?]+', clean_text(content_source))
    prioritized_sentences = []
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        score = len(sentence.split()) * 0.1  # Penalizar oraciones muy cortas
        for entity in entities:
            if entity.lower() in sentence.lower():
                score += 5
        for trend in trends:
            if trend.lower() in sentence.lower() and len(trend) <= 15:
                score += 3
        prioritized_sentences.append((sentence, score))
    prioritized_sentences.sort(key=lambda x: x[1], reverse=True)
    content = prioritized_sentences[0][0] if prioritized_sentences else content_source

    # Traducir si es necesario y asegurar coherencia
    if language == "es" and "twitter" not in url.lower():
        content = await translate_to_spanish(content)
    content = content[:max_content_length - len(hook) - 10].strip()
    if not content.endswith((".", "!", "?")):
        content = content.rstrip(".,!?") + ("!" if tone in ["enthusiastic", "urgent", "promotional"] else ".")

    # Generar hashtags hiper relevantes
    hashtag_candidates = []
    for entity in entities:
        hashtag = re.sub(r"\s+", "", entity)
        if len(hashtag) <= 15 and hashtag not in hashtag_candidates:
            hashtag_candidates.append(hashtag)
    for trend in trends:
        trend_clean = re.sub(r"\s+", "", trend)
        if (len(trend_clean) <= 15 and
            any(kw in trend.lower() for kw in ["game", "gaming"] + [e.lower() for e in entities]) and
            trend_clean not in hashtag_candidates):
            hashtag_candidates.append(trend_clean)
    if len(hashtag_candidates) < 2:
        hashtag_candidates.extend([h for h in ["GamingNews", "GamersUnite", "GameOn"] if h not in hashtag_candidates])
    hashtag_str = " " + " ".join(f"#{h}" for h in hashtag_candidates[:2])

    # Construir el tweet
    tweet = f"{hook} {content} {short_url}{hashtag_str}"
    if len(tweet) > CONFIG["TWEET"]["MAX_LENGTH"]:
        content = content[:max_content_length - len(hook) - len(hashtag_str) - 15] + "..."
        tweet = f"{hook} {content} {short_url}{hashtag_str}"

    logging.debug(f"Tweet generado ({language}, tipo: {news_type}, tono: {tone}, entidades: {entities}): {tweet}")
    return tweet

import aiohttp
import os
import hashlib
import logging
import colorlog
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import requests
import asyncio
import logging
from datetime import datetime, timedelta
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from tweepy import API, OAuthHandler
from tweepy.errors import TweepyException
from bs4 import BeautifulSoup
from PIL import Image, ImageDraw, ImageFont
import io

async def fetch_weather_data():
    """
    Obtiene datos meteorol√≥gicos de AEMET para las ciudades.
    """
    for city in CONFIG["CITIES"]:
        try:
            city_url = f"{CONFIG['WEATHER']['AEMET_CITY_URL']}/{city['aemet_id']}"
            response = requests.get(city_url, timeout=10)
            if response.status_code != 200:
                continue
            soup = BeautifulSoup(response.text, "html.parser")
            
            # Obtener temperaturas
            temp_data = soup.find("div", class_="prediccion_diaria")
            if temp_data:
                min_temp = temp_data.find("span", class_="temp_min").text.strip() if temp_data.find("span", class_="temp_min") else "N/A"
                max_temp = temp_data.find("span", class_="temp_max").text.strip() if temp_data.find("span", class_="temp_max") else "N/A"
                city["temp"] = {"min": min_temp, "max": max_temp}
            else:
                city["temp"] = {"min": "N/A", "max": "N/A"}
            
            # Obtener estado del cielo
            sky_state = soup.find("img", class_="icono_estado_cielo")["src"] if soup.find("img", class_="icono_estado_cielo") else ""
            if "11" in sky_state:
                city["icon"] = "despejado"
            elif "16" in sky_state:
                city["icon"] = "nubes"
            elif "43" in sky_state:
                city["icon"] = "lluvia"
            elif "46" in sky_state:
                city["icon"] = "truenos"
            elif "67" in sky_state:
                city["icon"] = "nieve"
            elif "80" in sky_state:
                city["icon"] = "niebla"
            else:
                city["icon"] = "nubes"  # Por defecto
        except Exception as e:
            logging.warning(f"Error al obtener datos de {city['name']}: {e}")
            city["temp"] = {"min": "N/A", "max": "N/A"}
            city["icon"] = "nubes"

    # Descargar mapa base
    try:
        map_response = requests.get(CONFIG["WEATHER"]["BASE_MAP_URL"], stream=True, timeout=10)
        if map_response.status_code == 200:
            map_image = Image.open(io.BytesIO(map_response.content)).convert("RGBA")
        else:
            logging.error(f"Error {map_response.status_code} al descargar el mapa base.")
            map_image = Image.new("RGBA", (1000, 600), (255, 255, 255, 255))
            draw = ImageDraw.Draw(map_image)
            draw.rectangle((100, 100, 900, 500), fill=(200, 200, 200, 255))
    except Exception as e:
        logging.error(f"Error al descargar el mapa base: {e}")
        map_image = Image.new("RGBA", (1000, 600), (255, 255, 255, 255))
        draw = ImageDraw.Draw(map_image)
        draw.rectangle((100, 100, 900, 500), fill=(200, 200, 200, 255))

    return map_image

async def generate_weather_summary():
    """
    Genera un resumen general del tiempo basado en las condiciones predominantes.
    """
    regions = {
        "norte": [],
        "noroeste": [],
        "centro": [],
        "este": [],
        "sur": [],
        "islas": []
    }
    for city in CONFIG["CITIES"]:
        regions[city["region"]].append(city["icon"])

    summary = []
    for region, conditions in regions.items():
        if conditions:
            most_common = Counter(conditions).most_common(1)[0][0]
            if region == "norte" and most_common:
                summary.append(f"truenos‚õàÔ∏è" if most_common == "truenos" else f"{most_common}‚òÅÔ∏è")
            elif region == "sur" and most_common:
                summary.append(f"sol‚òÄÔ∏è" if most_common == "despejado" else f"{most_common}üåßÔ∏è")
            elif region == "islas" and most_common:
                summary.append(f"sol‚òÄÔ∏è" if most_common == "despejado" else f"{most_common}üå´Ô∏è")

    if len(summary) >= 2:
        return f"¬°Ma√±ana {summary[0]} en sur & {summary[1]} en norte! üåà"
    elif summary:
        return f"¬°Ma√±ana {summary[0]} en Espa√±a! üåà"
    else:
        return "¬°Ma√±ana tiempo loco en Espa√±a! üåà"

async def create_weather_map(map_image):
    """
    Crea un mapa meteorol√≥gico colorido y lleno de emojis.
    """
    # Crear lienzo con fondo degradado amarillo-rosa
    canvas = Image.new("RGBA", (1200, 800), (255, 204, 0, 255))
    draw = ImageDraw.Draw(canvas)
    for y in range(800):
        r = 255
        g = int(204 - (y / 800) * 100)
        b = int((y / 800) * 150)
        draw.line((0, y, 1200, y), fill=(r, g, b, 255))

    # A√±adir efecto de brillo al mapa base
    map_image = map_image.resize((1000, 600), Image.Resampling.LANCZOS)
    canvas.paste(map_image, (100, 150), map_image)
    draw.rectangle((95, 145, 1105, 755), outline=(255, 0, 255, 255), width=5)

    # A√±adir t√≠tulo con emojis
    tomorrow = datetime.now() + timedelta(days=1)
    day_name = tomorrow.strftime("%A").upper()
    date_str = tomorrow.strftime("%d de %B").lower()
    title = f"¬°Tiempo Loco üéâ {day_name} {date_str}! üåà"
    try:
        font = ImageFont.truetype("arialbd.ttf", 40)
        small_font = ImageFont.truetype("arial.ttf", 15)
        emoji_font = ImageFont.truetype("seguiemj.ttf", 40)
    except:
        font = small_font = emoji_font = ImageFont.load_default()
    draw.text((52, 52), title, fill=(0, 0, 0, 255), font=font)
    draw.text((50, 50), title, fill=(255, 0, 0, 255), font=font)

    # A√±adir estrellas y emojis decorativos
    for _ in range(5):
        x, y = random.randint(50, 1150), random.randint(50, 100)
        draw.text((x-5, y-5), "‚≠ê", font=emoji_font, fill=(255, 255, 0, 255))

    # A√±adir emojis, temperaturas y mensajes
    try:
        temp_font = ImageFont.truetype("arial.ttf", 25)
    except:
        temp_font = ImageFont.load_default()
    for city in CONFIG["CITIES"]:
        x, y = city["coords"]
        emoji = CONFIG["FUN_EMOJIS"].get(city["icon"], "‚òÅÔ∏è")
        extra_emoji = CONFIG["EXTRA_EMOJIS"].get(city["icon"], "üå∏")
        draw.text((x - 20, y - 60), emoji, font=emoji_font, fill=(0, 0, 0, 255))
        draw.text((x + 20, y - 60), extra_emoji, font=emoji_font, fill=(0, 0, 0, 255))
        
        temp_text = f"{city['name']}: {city['temp']['min']}¬∞ / {city['temp']['max']}¬∞"
        color = CONFIG["TEXT_COLORS"].get(city["icon"], (0, 0, 0, 255))
        draw.rectangle([x-5, y+5, x+150, y+40], fill=(255, 255, 255, 200))
        draw.text((x, y+10), temp_text, fill=color, font=temp_font)

        # A√±adir mensaje gracioso peque√±o
        message = random.choice(CONFIG["FUN_MESSAGES"].get(city["icon"], ["¬°Qu√© tiempo tan loco!"]))
        draw.text((x, y+50), message, fill=(0, 0, 0, 255), font=small_font)

    # A√±adir personaje con m√°s emojis
    draw.ellipse((1050, 650, 1150, 750), fill=(255, 255, 0, 255))
    draw.ellipse((1070, 670, 1080, 680), fill=(0, 0, 0, 255))
    draw.ellipse((1120, 670, 1130, 680), fill=(0, 0, 0, 255))
    draw.line((1080, 675, 1120, 675), fill=(0, 0, 0, 255))
    draw.arc((1080, 700, 1120, 720), 0, 180, fill=(0, 0, 0, 255))
    draw.polygon([(1100, 650), (1080, 620), (1120, 620)], fill=(255, 0, 0, 255))
    for i in range(8):
        angle = i * 45
        x1 = 1100 + 70 * (1 if i % 2 == 0 else 0.7) * (1 if i < 4 else -1)
        y1 = 700 + 70 * (1 if i % 2 == 0 else 0.7) * (1 if i in [2, 3, 6, 7] else -1)
        draw.line((1100, 700, x1, y1), fill=(255, 255, 0, 255), width=3)
    draw.text((1060, 620), "üåü", font=emoji_font, fill=(255, 255, 0, 255))
    draw.text((1140, 620), "üéµ", font=emoji_font, fill=(255, 255, 0, 255))

    # Guardar la imagen
    canvas.save(CONFIG["WEATHER"]["OUTPUT_PATH"], "PNG")
    logging.info(f"Mapa guardado en {CONFIG['WEATHER']['OUTPUT_PATH']}")

async def fetch_and_post_weather():
    """
    Genera y publica el pron√≥stico con m√°s emojis en un solo tweet.
    """
    logging.info("Generando pron√≥stico divertido...")
    map_image = await fetch_weather_data()
    if map_image:
        await create_weather_map(map_image)
        summary = await generate_weather_summary()
        # A√±adir m√°s emojis al tweet
        message = f"{summary} ‚òÄÔ∏èüåßÔ∏è‚õàÔ∏è‚ùÑÔ∏èüå´Ô∏èüéâ #TiempoLoco {CONFIG['WEATHER']['WEBSITE_URL']}"
        # Asegurarse de que el tweet no exceda 280 caracteres
        if len(message) > 280:
            message = message[:270] + "..."
        try:
            with open(CONFIG["WEATHER"]["OUTPUT_PATH"], "rb") as image_file:
                media = app.media_upload(filename="weather_map_fun.png", file=image_file)
            app.update_status(status=message, media_ids=[media.media_id])
            logging.info(f"Tweet publicado: {message}")
        except TweepyException as e:
            logging.error(f"Error al publicar tweet: {e}")
        
async def fetch_and_post_carbon_intensity():
    """
    Consulta la API de Electricity Maps para Espa√±a (intensidad de carbono y desglose de fuentes de energ√≠a)
    usando requests, y publica un tweet diario en espa√±ol con un comentario sobre baja inercia si la generaci√≥n
    solar es alta, respetando los l√≠mites del plan gratuito de Twitter (50 tweets/d√≠a, 1500/mes).
    """
    global bot_status
    logging.info("Iniciando consulta a Electricity Maps para tweet de intensidad de carbono y fuentes en Espa√±a")

    # Verificar si ya se public√≥ hoy
    last_carbon_tweet = bot_status.get("last_carbon_tweet", 0)
    current_day = datetime.utcnow().date().isoformat()
    if last_carbon_tweet == current_day:
        logging.info("Tweet de carbono ya publicado hoy. Omitiendo.")
        return

    # Verificar l√≠mites de Twitter
    tweets_remaining = CONFIG["API_LIMITS"]["TWEETS_PER_DAY"] - bot_status["daily_tweets_total"]
    monthly_remaining = CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"] - (bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"])
    if tweets_remaining <= 0 or monthly_remaining <= 0:
        logging.warning("L√≠mite diario o mensual de Twitter alcanzado. No se publicar√° tweet de carbono.")
        return

    # Consultar intensidad de carbono
    carbon_data = None
    for attempt in range(2):
        try:
            response = await asyncio.to_thread(
                requests.get,
                CONFIG["ELECTRICITY_MAPS"]["CARBON_INTENSITY_URL"],
                headers={"auth-token": CONFIG["ELECTRICITY_MAPS"]["AUTH_TOKEN"]},
                params={"zone": CONFIG["ELECTRICITY_MAPS"]["ZONE"]}
            )
            if response.status_code == 200:
                carbon_data = response.json()
                logging.debug(f"Datos de intensidad de carbono: {carbon_data}")
                break
            elif response.status_code == 429:
                logging.warning(f"Error 429 en carbon-intensity, intento {attempt + 1}/2. Esperando 30s.")
                await asyncio.sleep(30)
            else:
                logging.error(f"Error {response.status_code} al consultar carbon-intensity: {response.text}")
                return
        except Exception as e:
            logging.error(f"Error en consulta a carbon-intensity, intento {attempt + 1}/2: {e}")
            if attempt == 1:
                logging.error("Fallo tras 2 intentos en carbon-intensity.")
                return
            await asyncio.sleep(30)

    # Consultar desglose de fuentes
    power_data = None
    for attempt in range(2):
        try:
            response = await asyncio.to_thread(
                requests.get,
                CONFIG["ELECTRICITY_MAPS"]["POWER_BREAKDOWN_URL"],
                headers={"auth-token": CONFIG["ELECTRICITY_MAPS"]["AUTH_TOKEN"]},
                params={"zone": CONFIG["ELECTRICITY_MAPS"]["ZONE"]}
            )
            if response.status_code == 200:
                power_data = response.json()
                logging.debug(f"Datos de power-breakdown: {power_data}")
                break
            elif response.status_code == 429:
                logging.warning(f"Error 429 en power-breakdown, intento {attempt + 1}/2. Esperando 30s.")
                await asyncio.sleep(30)
            else:
                logging.warning(f"Error {response.status_code} al consultar power-breakdown: {response.text}")
                break
        except Exception as e:
            logging.warning(f"Error en consulta a power-breakdown, intento {attempt + 1}/2: {e}")
            if attempt == 1:
                logging.warning("Fallo tras 2 intentos en power-breakdown. Continuando con solo intensidad de carbono.")
                break
            await asyncio.sleep(30)

    # Procesar datos de intensidad de carbono
    carbon_intensity = None
    if carbon_data:
        try:
            carbon_intensity = carbon_data.get("carbonIntensity", 0)
            zone = carbon_data.get("zone", CONFIG["ELECTRICITY_MAPS"]["ZONE"])
            if not carbon_intensity or not zone:
                logging.error("Datos inv√°lidos de carbon-intensity. No se publicar√° tweet.")
                return
        except Exception as e:
            logging.error(f"Error procesando datos de carbon-intensity: {e}")
            return

    # Procesar datos de power-breakdown y detectar baja inercia
    renewable_percentage = None
    sources_text = ""
    low_inertia_message = ""
    SOLAR_THRESHOLD = 0.3  # Umbral para alta generaci√≥n solar (30%)
    if power_data:
        try:
            renewable_percentage = power_data.get("renewablePercentage", 0)
            breakdown = power_data.get("powerConsumptionBreakdown", {})
            sources = [
                (key, value) for key, value in breakdown.items()
                if value and key in ["solar", "wind", "hydro", "nuclear", "gas", "coal"]
            ]
            total = sum(value for _, value in sources)
            if total > 0:
                solar_mw = breakdown.get("solar", 0)
                solar_percentage = solar_mw / total if total > 0 else 0
                if solar_percentage >= SOLAR_THRESHOLD:
                    low_inertia_message = "Alta generaci√≥n solar reduce la inercia del sistema. ¬°Necesitamos m√°s almacenamiento!"
                sources.sort(key=lambda x: x[1], reverse=True)
                top_sources = sources[:3]
                sources_text = ", ".join(
                    f"{key} {round((value / total) * 100)}%" for key, value in top_sources
                )
                sources_text = f" ({sources_text})"
        except Exception as e:
            logging.warning(f"Error procesando datos de power-breakdown: {e}")
            sources_text = ""
            renewable_percentage = None
            low_inertia_message = ""

    # Generar tweet en espa√±ol
    short_url = await shorten_url(CONFIG["ELECTRICITY_MAPS"]["WEBSITE_URL"])
    if carbon_intensity and renewable_percentage and sources_text:
        message = (
            f"üåç Intensidad de carbono en Espa√±a: {carbon_intensity} gCO2eq/kWh. "
            f"‚ö°Ô∏è Energ√≠a: {renewable_percentage}% renovable{sources_text}. "
            f"{low_inertia_message} ¬°Por un futuro verde! {short_url} #Sostenibilidad #IntensidadDeCarbono"
        ).strip()
    elif carbon_intensity:
        message = (
            f"üåç Intensidad de carbono en Espa√±a: {carbon_intensity} gCO2eq/kWh. "
            f"¬°Energ√≠a m√°s limpia para un futuro sostenible! ‚ö°Ô∏è {short_url} #Sostenibilidad #IntensidadDeCarbono"
        )
    else:
        logging.error("No se pudieron obtener datos v√°lidos. No se publicar√° tweet.")
        return

    # Ajustar longitud del tweet
    if len(message) > CONFIG["TWEET"]["MAX_LENGTH"]:
        if renewable_percentage and low_inertia_message:
            message = (
                f"üåç Carbono en Espa√±a: {carbon_intensity} gCO2eq/kWh. "
                f"‚ö°Ô∏è {renewable_percentage}% renovable. Alta generaci√≥n solar reduce la inercia. {short_url} #Sostenibilidad"
            )
        elif renewable_percentage:
            message = (
                f"üåç Carbono en Espa√±a: {carbon_intensity} gCO2eq/kWh. "
                f"‚ö°Ô∏è {renewable_percentage}% renovable. ¬°Vamos por lo verde! {short_url} #Sostenibilidad"
            )
        else:
            message = (
                f"üåç Carbono en Espa√±a: {carbon_intensity} gCO2eq/kWh. "
                f"¬°Vamos por lo verde! ‚ö°Ô∏è {short_url} #Sostenibilidad"
            )

    # Publicar tweet en espa√±ol
    try:
        await make_twitter_request(
            app.update_status,
            status=message,
            category="write_es",
            endpoint="/statuses/update",
            priority=1.0
        )
        bot_status["daily_tweets_total"] += 1
        bot_status["monthly_posts_es"] += 1
        bot_status["last_carbon_tweet"] = current_day
        logging.info(f"Tweet de carbono publicado: {message[:50]}...")
        await save_bot_state()
    except Exception as e:
        logging.error(f"Error publicando tweet de carbono: {e}")

# 3. Actualizar funci√≥n capture_map_screenshot
def capture_map_screenshot(map_url, output_path):
    """
    Captura una captura de pantalla del mapa meteorol√≥gico usando Selenium con reintentos,
    optimizaciones para carga completa y evasi√≥n de detecci√≥n anti-bots.
    """
    logging.info(f"Iniciando captura de mapa meteorol√≥gico desde {map_url}")
    driver = None
    for attempt in range(2):
        try:
            # Configurar ChromeDriver para evitar detecci√≥n
            chrome_options = Options()
            chrome_options.add_argument("--headless")  # Sin interfaz gr√°fica
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option("useAutomationExtension", False)

            driver = webdriver.Chrome(options=chrome_options)
            driver.set_page_load_timeout(30)  # Timeout de carga de p√°gina

            # Cargar la p√°gina
            driver.get(map_url)
            logging.debug("P√°gina cargada, esperando carga completa del DOM")

            # Esperar carga completa del DOM
            WebDriverWait(driver, 20).until(
                lambda d: d.execute_script("return document.readyState") == "complete"
            )

            # Intentar manejar popup de cookies (si existe)
            try:
                cookie_button = WebDriverWait(driver, 5).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, "button#accept-cookies, .accept-cookies"))
                )
                cookie_button.click()
                logging.info("Popup de cookies aceptado")
            except TimeoutException:
                logging.info("No se encontr√≥ el popup de cookies o ya fue aceptado")

            # Esperar el elemento del mapa (ajusta el selector seg√∫n tu p√°gina)
            map_element = WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div#map"))  # Reemplaza con el selector correcto
            )
            logging.debug("Elemento del mapa encontrado")

            # Asegurar que el mapa est√© visible
            driver.execute_script("arguments[0].scrollIntoView();", map_element)
            time.sleep(2)  # Espera adicional para renderizado

            # Capturar captura de pantalla del elemento
            map_element.screenshot(output_path)
            logging.info(f"Captura de pantalla guardada en {output_path}")
            return True

        except TimeoutException as e:
            logging.warning(f"Timeout en captura de mapa, intento {attempt + 1}/2: {e}")
            if attempt == 1:
                logging.error("Fallo tras 2 intentos en captura de mapa")
                return False
            time.sleep(30)  # Esperar antes de reintentar
        except Exception as e:
            logging.error(f"Error en captura de mapa, intento {attempt + 1}/2: {e}")
            if attempt == 1:
                logging.error("Fallo tras 2 intentos en captura de mapa")
                return False
            time.sleep(30)
        finally:
            if driver:
                driver.quit()

    return False

# 4. Actualizar funci√≥n post_weather_map
async def post_weather_map():
    """
    Publica un tweet con el mapa meteorol√≥gico capturado, respetando los l√≠mites de Twitter.
    """
    global bot_status
    logging.info("Iniciando publicaci√≥n de tweet con pron√≥stico del tiempo")

    # Verificar si ya se public√≥ hoy
    last_weather_tweet = bot_status.get("last_weather_tweet", 0)
    current_day = datetime.utcnow().date().isoformat()
    if last_weather_tweet == current_day:
        logging.info("Tweet de pron√≥stico del tiempo ya publicado hoy. Omitiendo.")
        return

    # Verificar l√≠mites de Twitter
    tweets_remaining = CONFIG["API_LIMITS"]["TWEETS_PER_DAY"] - bot_status["daily_tweets_total"]
    monthly_remaining = CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"] - (bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"])
    if tweets_remaining <= 0 or monthly_remaining <= 0:
        logging.warning("L√≠mite diario o mensual de Twitter alcanzado. No se publicar√° tweet de pron√≥stico.")
        return

    # Capturar mapa
    loop = asyncio.get_event_loop()
    success = await loop.run_in_executor(
        None,
        capture_map_screenshot,
        CONFIG["WEATHER"]["MAP_URL"],
        CONFIG["WEATHER"]["OUTPUT_PATH"]
    )

    if not success:
        logging.error("No se pudo capturar el mapa meteorol√≥gico. No se publicar√° tweet.")
        return

    # Generar tweet en espa√±ol
    short_url = await shorten_url(CONFIG["WEATHER"]["WEBSITE_URL"])
    message = (
        f"‚òÄÔ∏è Pron√≥stico del tiempo en Espa√±a: ¬°Mira el mapa de hoy! üå¶Ô∏è "
        f"Prep√°rate para el d√≠a. {short_url} #Tiempo #Pron√≥stico"
    )
    if len(message) > CONFIG["TWEET"]["MAX_LENGTH"]:
        message = (
            f"‚òÄÔ∏è Pron√≥stico en Espa√±a: ¬°Mapa de hoy! üå¶Ô∏è "
            f"{short_url} #Tiempo"
        )

    # Publicar tweet con imagen
    try:
        # Subir imagen a Twitter
        with open(CONFIG["WEATHER"]["OUTPUT_PATH"], "rb") as image_file:
            media = app.media_upload(filename="weather_map.png", file=image_file)
        # Publicar tweet con imagen
        await make_twitter_request(
            app.update_status,
            status=message,
            media_ids=[media.media_id],
            category="write_es",
            endpoint="/statuses/update",
            priority=1.0
        )
        bot_status["daily_tweets_total"] += 1
        bot_status["monthly_posts_es"] += 1
        bot_status["last_weather_tweet"] = current_day
        logging.info(f"Tweet de pron√≥stico publicado: {message[:50]}...")
        await save_bot_state()
    except Exception as e:
        logging.error(f"Error publicando tweet de pron√≥stico: {e}")

async def repost_tweet_from_url(tweet_url: str, summary: str, language: str = 'en') -> bool:
    global bot_status
    category = f"write_{language}"
    current_time = time.time()
    if current_time < bot_status["twitter_wait_until"].get(category, 0):
        logging.warning(f"En per√≠odo de espera para {category} hasta {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%H:%M:%S')}")
        return False

    try:
        tweet_match = re.search(r'https://(nitter\.net|twitter\.com|x\.com)/(\w+)/status/(\d+)', tweet_url)
        if not tweet_match:
            logging.warning(f"URL no v√°lida para republicaci√≥n: {tweet_url}")
            return False
        
        username = tweet_match.group(2)
        tweet_id = tweet_match.group(3)
        title = f"Tweet from @{username}"
        source = f"twitter_{username}"
        client = twitter_client_en if language == 'en' else twitter_client_es
        daily_key = "daily_tweets_total"
        monthly_key = f"monthly_posts_{language}"
        posted_key = f"posted_tweets_{language}"
        last_tweet_key = f"last_tweet_time_{language}"
        semaphore = write_semaphore_en if language == "en" else write_semaphore_es
        rate_limit_event = write_rate_limit_event_en if language == "en" else write_rate_limit_event_es

        if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"] or bot_status[daily_key] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
            logging.info(f"L√≠mite alcanzado para {language} (diario: {bot_status[daily_key]}/{CONFIG['API_LIMITS']['TWEETS_PER_DAY']})")
            return False
        
        last_tweet_time = bot_status[last_tweet_key]
        if last_tweet_time and (time.time() - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
            logging.info(f"Espera requerida para {language}")
            return False
        
        if await is_published_in_language(title, summary, tweet_url, source, language):
            logging.info(f"Tweet ya publicado en {language}: {tweet_url}")
            return False
        
        tweet_response = await make_twitter_request(
            client.get_tweet,
            id=tweet_id,
            tweet_fields=["text"],
            category="read"
        )
        if not tweet_response.data:
            logging.warning(f"No se pudo obtener el tweet {tweet_id}")
            return False

        tweet_text = tweet_response.data.text
        comment = summary.strip() if summary else ""
        if comment.startswith(f"RT @{username}:"):
            comment = comment[len(f"RT @{username}:"):].strip()
        if language == 'es' and comment:
            comment = await translate_to_spanish(comment)

        if comment and len(comment) > CONFIG["TWEET"]["MAX_LENGTH"] - 50:
            comment = comment[:CONFIG["TWEET"]["MAX_LENGTH"]-53] + "..."

        if comment:
            tweet_response = await make_twitter_request(
                client.create_tweet,
                text=comment,
                quote_tweet_id=tweet_id,
                category=category,
                semaphore=semaphore,
                rate_limit_event=rate_limit_event
            )
            logging.info(f"Quote Tweet publicado en {language}: {comment[:50]}... ID: {tweet_response.data['id']}")
        else:
            tweet_response = await make_twitter_request(
                client.retweet,
                tweet_id=tweet_id,
                category=category,
                semaphore=semaphore,
                rate_limit_event=rate_limit_event
            )
            logging.info(f"Retweet simple publicado en {language}: Tweet ID {tweet_id}")

        bot_status[daily_key] += 1
        bot_status[monthly_key] += 1
        bot_status[posted_key] += 1
        bot_status[last_tweet_key] = datetime.now()
        
        news_hash = hashlib.sha256((title + summary + tweet_url + source).encode()).hexdigest()
        cursor.execute('''INSERT OR IGNORE INTO historial (hash, title, url, tweet, relevance, source, date, engagement, summary, language)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                      (news_hash, title, tweet_url, comment or "Retweet", 50.0, source, datetime.now().isoformat(), 0, tweet_text, language))
        conn.commit()
        save_bot_state_sync(cursor, conn)
        return True
        
    except Exception as e:
        logging.error(f"Error al citar tweet {tweet_url} en {language}: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
        return False

async def post_tweet(session: aiohttp.ClientSession, title: str, summary: str, url: str, image_data: bytes | None, news_hash: str, score: float, source: str, language: str, trends: list[str]) -> bool:
    """
    Attempts to post a tweet. Handles limits, cooldown, duplicates, image upload
    and adds to the queue if it fails due to recoverable limits.
    Uses CONFIG["API_LIMITS"] and CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"] values.
    Benefits from dynamic limit handling in make_twitter_request.
    Modified to handle image_data being None and improved error handling.
    """
    global bot_status
    category = f"write_{language}"
    current_time = time.time()

    # Check if writing for this language is paused
    if current_time < bot_status["twitter_wait_until"].get(category, 0):
        logging.warning(f"In waiting period for {category} until {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%H:%M:%S')}. Skipping post attempt.")
        # Add to queue if not already published and not in queue?
        # This logic is better handled in process_single_news or the caller
        return False

    client = twitter_client_en if language == "en" else twitter_client_es
    api = twitter_api_en if language == "en" else twitter_api_es
    daily_key = "daily_tweets_total"
    monthly_key = f"monthly_posts_{language}"
    posted_key = f"posted_tweets_{language}"
    last_tweet_key = f"last_tweet_time_{language}"
    semaphore = write_semaphore_en if language == "en" else write_semaphore_es
    rate_limit_event = write_rate_limit_event_en if language == "en" else write_rate_limit_event_es

    try:
        # Check monthly and daily limits using CONFIG["API_LIMITS"] values
        if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
            logging.info(f"Total monthly limit reached: {bot_status['monthly_posts_en'] + bot_status['monthly_posts_es']}/{CONFIG['API_LIMITS']['MONTHLY_POSTS_TOTAL']}. Skipping post attempt.")
            return False

        if bot_status[daily_key] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
            logging.info(f"Daily limit reached: {bot_status[daily_key]}/{CONFIG['API_LIMITS']['TWEETS_PER_DAY']}. Skipping post attempt.")
            return False

        # Check tweet cooldown using CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]
        last_tweet_time = bot_status.get(last_tweet_key) # Use .get() for safety
        if last_tweet_time and (current_time - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
            remaining = CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"] - (current_time - last_tweet_time.timestamp())
            logging.info(f"Cooldown required for {language}: {remaining/60:.1f} minutes remaining. Skipping post attempt.")
            return False

        # Check if already published using the async function is_published_in_language
        # This check is critical before attempting to post
        if await is_published_in_language(title, summary, url, source, language):
            logging.info(f"News already published in {language}: {title[:50]}. Skipping post attempt.")
            return False

        logging.info(f"Intentando publicar en {language}: {title[:50]}...")
        tweet = await generate_detailed_tweet(title, summary, url, trends, language) # generate_detailed_tweet must be defined
        temp_image_path = os.path.join(CONFIG["PATHS"]["TEMP_IMAGE_DIR"], f"{news_hash}_{language}.jpg")

        media = None
        # --- MODIFICATION: Check if image_data is valid before attempting to optimize/upload ---
        # image_data is already bytes | None from previous steps
        if image_data and isinstance(image_data, bytes) and len(image_data) > 0:
            # Use aiofiles for asynchronous file operations
            try:
                # Save original downloaded image data temporarily
                async with aiofiles.open(temp_image_path, "wb") as f:
                    await f.write(image_data)

                # optimize_image now handles None input and returns None on failure
                optimized_image_data = await optimize_image(image_data)

                if optimized_image_data:
                    # Save optimized data to the temporary file for Tweepy
                    async with aiofiles.open(temp_image_path, "wb") as f:
                        await f.write(optimized_image_data)

                    try:
                        # make_twitter_request handles Tweepy's sync call and rate limits
                        media = await make_twitter_request(
                            api.media_upload,
                            temp_image_path,
                            category=category,
                            semaphore=semaphore,
                            rate_limit_event=rate_limit_event
                        )
                        logging.debug(f"Image uploaded successfully for tweet in {language}.")
                    except Exception as e:
                         logging.error(f"Error uploading image for tweet in {language}: {e}")
                         media = None # Continue without image if upload fails
                else:
                    logging.warning(f"Image optimization failed for tweet in {language}. Posting without image.")
                    media = None # Post without image if optimization fails
            except Exception as e:
                logging.error(f"Error processing image for tweet in {language}: {e}", exc_info=True)
                media = None # Ensure media is None if any image processing error occurs
        else:
            logging.debug(f"No valid image data provided for tweet in {language}. Posting without image.")
            media = None # No image data to process

        tweet_response = None
        try:
            # make_twitter_request handles the Tweepy call, rate limits, and retries
            tweet_response = await make_twitter_request(
                client.create_tweet,
                text=tweet,
                media_ids=[media.media_id] if media else None, # Pass media_ids only if media exists
                category=category,
                semaphore=semaphore,
                rate_limit_event=rate_limit_event
            )
            logging.info(f"Tweet in {language} posted (score {score:.1f}): {tweet[:50]}... ID: {tweet_response.data['id']}")

            # Update status counters
            bot_status[daily_key] += 1
            bot_status[monthly_key] += 1
            bot_status[posted_key] += 1
            bot_status[last_tweet_key] = datetime.now()

            # Record in history using run_db_sync
            await run_db_sync(
                cursor.execute,
                '''INSERT OR IGNORE INTO historial (hash, title, url, tweet, relevance, source, date, engagement, summary, language)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                (news_hash, title, url, tweet, score, source, datetime.now().isoformat(), 0, summary, language)
            )
            await run_db_sync(conn.commit)
            await save_bot_state() # Save state after successful post and history update

            return True

        except Exception as e:
            logging.error(f"Error posting tweet in {language}: {e}", exc_info=True)
            # Check if the error is a recoverable rate limit error (429)
            is_rate_limit_error = isinstance(e, tweepy.TweepyException) and e.response and e.response.status_code == 429

            # Add to queue ONLY if it's a recoverable error AND it hasn't been published yet in this language
            # The check `await is_published_in_language(...)` is crucial here to avoid queuing items that were
            # successfully posted by another process/attempt but failed to be removed from a previous queue run.
            if is_rate_limit_error and not await is_published_in_language(title, summary, url, source, language):
                 trends_json = json.dumps(trends)
                 # Add to queue using run_db_sync
                 # Store the ORIGINAL image_data (bytes or None) in the queue
                 await run_db_sync(
                     cursor.execute,
                     '''INSERT INTO cola_publicacion (title, summary, url, image_data, news_hash, score, source, language, trends)
                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                     (title, summary, url, image_data, news_hash, score, source, language, trends_json)
                 )
                 await run_db_sync(conn.commit)
                 logging.info(f"News '{title[:50]}' added to queue for {language} due to rate limit.")
            else:
                 # If it's not a rate limit error, or if it's already published, just log the error.
                 logging.warning(f"News '{title[:50]}' failed to post in {language} due to non-recoverable error or already published. Not adding to queue.")


            bot_status["errors"] += 1 # Increment error counter for any posting failure
            await save_bot_state() # Save state after error (and potential queue addition)
            return False # Return False if posting failed

    finally:
        # Use asyncio.to_thread for synchronous file operations like os.remove
        if os.path.exists(temp_image_path):
            await asyncio.to_thread(os.remove, temp_image_path)
        # await asyncio.sleep(2) # Small pause after attempting to post (optional, make_twitter_request might already wait)

async def review_queue():
    """
    Gestiona la cola de publicaciones con un sistema de espera avanzado que optimiza lecturas y escrituras,
    maximizando el uso de los l√≠mites del plan gratuito (50 tweets/d√≠a, 1500 posts/mes).
    """
    global bot_status
    bot_status["last_task"] = "Revisando cola de publicaciones"
    logging.info("Iniciando revisi√≥n avanzada de la cola de publicaciones...")

    try:
        # Obtener estado actual de la API
        current_time = time.time()
        tweets_remaining = CONFIG["API_LIMITS"]["TWEETS_PER_DAY"] - bot_status["daily_tweets_total"]
        monthly_remaining = CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"] - (bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"])
        trends_last_updated = bot_status.get("trends_last_updated", 0)
        trends_expired = current_time - trends_last_updated > 4 * 3600  # Actualizar tendencias cada 4 horas

        # Obtener tendencias solo si es necesario
        trends = []
        if trends_expired or not bot_status.get("current_trends"):
            trends = await get_trending_keywords()
            bot_status["trends_last_updated"] = current_time
            bot_status["current_trends"] = trends
            logging.info("Tendencias actualizadas desde la API.")
        else:
            trends = bot_status["current_trends"]
            logging.debug("Usando tendencias en cach√©.")

        # Obtener elementos de la cola
        cursor.execute("SELECT id, title, summary, url, news_hash, score, source, language, trends, added_at FROM cola_publicacion")
        queue_items = await run_db_sync(cursor.fetchall)

        if not queue_items:
            logging.info("La cola de publicaciones est√° vac√≠a.")
            return

        items_to_remove = []
        items_to_publish = []
        updated_items = []

        # Calcular el n√∫mero ideal de tweets por hora para distribuir uniformemente
        hours_left_in_day = (24 - datetime.now().hour) % 24 or 24
        ideal_tweets_per_hour = min(tweets_remaining / max(hours_left_in_day, 1), 3)  # M√°ximo 3 tweets/hora
        min_wait_between_tweets = 3600 / max(ideal_tweets_per_hour, 1) if ideal_tweets_per_hour > 0 else 300

        # Procesar cada elemento de la cola
        for item in queue_items:
            id, title, summary, url, news_hash, old_score, source, language, old_trends_json, added_at = item
            added_at = datetime.fromisoformat(added_at) if isinstance(added_at, str) else added_at

            # Recalcular puntuaci√≥n
            entry = {"title": title, "summary": summary, "link": url, "source": source, "date": added_at}
            new_score = await score_news(entry, trends)

            # Aplicar decaimiento por antig√ºedad (15% por d√≠a)
            age_days = (datetime.now() - added_at).total_seconds() / (24 * 3600)
            decay_factor = max(0, 1 - 0.15 * age_days)
            new_score *= decay_factor

            # Calcular √≠ndice de urgencia
            urgency = new_score
            if source.startswith("discord") or source in [f"twitter_{acc}" for acc in TWITTER_ACCOUNTS]:
                urgency *= 1.2  # Priorizar fuentes clave
            if any(trend.lower() in title.lower() or trend.lower() in summary.lower() for trend in trends):
                urgency *= 1.3  # Aumentar si coincide con tendencias
            urgency *= min(1, 1.5 - age_days / 2)  # Reducir para noticias antiguas
            urgency *= (1 + 0.5 * tweets_remaining / CONFIG["API_LIMITS"]["TWEETS_PER_DAY"])  # Aumentar si hay muchos tweets disponibles
            urgency *= (1 + 0.2 * monthly_remaining / CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"])  # Aumentar si hay margen mensual

            # Decidir acci√≥n
            if new_score < 5 or age_days > 3:
                items_to_remove.append(id)
                logging.debug(f"Elemento {title[:50]} (ID: {id}) marcado para eliminar: puntuaci√≥n {new_score:.1f}, edad {age_days:.1f} d√≠as")
            else:
                updated_items.append((new_score, json.dumps(trends), id))
                # Determinar si publicar ahora o esperar
                wait_time = 0
                if tweets_remaining <= 3 or monthly_remaining <= 20:
                    wait_time = 600  # Esperar 10 minutos si los l√≠mites est√°n muy bajos
                elif len(queue_items) > 40:
                    wait_time = 180 * (1 - urgency / 20)  # Escalonar espera si la cola est√° casi llena
                elif urgency < 8:
                    wait_time = 90 * (1 - urgency / 15)  # Esperar m√°s para noticias menos urgentes
                wait_time = max(wait_time, min_wait_between_tweets)  # Respetar el ritmo ideal

                # Verificar cooldowns y tiempos de espera de la API
                lang_key = f"write_{language}"
                last_tweet_time = bot_status.get(f"last_tweet_time_{language}")
                api_wait_until = bot_status["twitter_wait_until"].get(lang_key, 0)
                cooldown_elapsed = not last_tweet_time or (current_time - last_tweet_time.timestamp()) >= CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]

                if wait_time <= 0 and current_time >= api_wait_until and cooldown_elapsed:
                    items_to_publish.append((id, entry, new_score, language, trends, urgency))
                    logging.debug(f"Elemento {title[:50]} (ID: {id}) listo para publicaci√≥n: urgencia {urgency:.1f}")
                else:
                    logging.debug(f"Elemento {title[:50]} (ID: {id}) en espera: {wait_time:.1f}s, cooldown: {not cooldown_elapsed}, API wait: {api_wait_until - current_time:.1f}s")

        # Ordenar elementos a publicar por urgencia
        items_to_publish.sort(key=lambda x: x[5], reverse=True)

        # Publicar elementos seleccionados, respetando l√≠mites
        async with aiohttp.ClientSession() as session:
            for id, entry, score, language, trends, urgency in items_to_publish[:tweets_remaining]:
                if await publish_news(session, entry, entry.get("hash", ""), score, language, trends):
                    items_to_remove.append(id)
                    bot_status["daily_tweets_total"] += 1
                    bot_status[f"monthly_posts_{language}"] += 1
                    bot_status[f"last_tweet_time_{language}"] = datetime.now()
                    logging.info(f"Publicado desde cola (ID: {id}): {entry['title'][:50]} en {language}")
                    await asyncio.sleep(CONFIG["INTERVALS"]["API_REQUEST_DELAY_SECONDS"])
                else:
                    logging.warning(f"Fallo al publicar elemento (ID: {id}): {entry['title'][:50]}")

        # Eliminar elementos irrelevantes o publicados
        if items_to_remove:
            await run_db_sync(
                cursor.execute,
                f"DELETE FROM cola_publicacion WHERE id IN ({','.join('?' * len(items_to_remove))})",
                tuple(items_to_remove)
            )
            logging.info(f"Eliminados {len(items_to_remove)} elementos de la cola.")

        # Actualizar puntuaciones
        if updated_items:
            await run_db_sync(
                cursor.executemany,
                "UPDATE cola_publicacion SET score = ?, trends = ? WHERE id = ?",
                updated_items
            )
            logging.info(f"Actualizadas puntuaciones de {len(updated_items)} elementos.")

        # Limitar la cola a 50 elementos
        cursor.execute("SELECT COUNT(*) FROM cola_publicacion")
        queue_size = (await run_db_sync(cursor.fetchone))[0]
        if queue_size > 50:
            excess = queue_size - 50
            cursor.execute("SELECT id FROM cola_publicacion ORDER BY score ASC LIMIT ?", (excess,))
            excess_ids = [row[0] for row in await run_db_sync(cursor.fetchall)]
            await run_db_sync(
                cursor.execute,
                f"DELETE FROM cola_publicacion WHERE id IN ({','.join('?' * len(excess_ids))})",
                tuple(excess_ids)
            )
            logging.info(f"Eliminados {len(excess_ids)} elementos para mantener l√≠mite de 50.")

        await run_db_sync(conn.commit)
        logging.info("Revisi√≥n avanzada de la cola completada.")

    except Exception as e:
        logging.error(f"Error revisando la cola de publicaciones: {e}", exc_info=True)
        bot_status["errors"] += 1
    finally:
        await save_bot_state()

async def process_queue():
    global bot_status
    bot_status["last_task"] = "Processing publication queue"
    print_section_header("Procesando Cola de Publicaci√≥n")
    logging.debug("Starting publication queue processing...")

    try:
        cursor.execute("SELECT id, title, summary, url, image_data, news_hash, score, source, language, trends FROM cola_publicacion ORDER BY added_at ASC")
        pending_news = await run_db_sync(cursor.fetchall)

        if not pending_news:
            print("‚ÑπÔ∏è La cola de publicaci√≥n est√° vac√≠a.")
            logging.debug("Publication queue is empty.")
            return

        logging.info(f"Items in publication queue: {len(pending_news)}")

        async with aiohttp.ClientSession() as session:
            items_to_remove = []

            for news in pending_news:
                id, title, summary, url, image_data_bytes, news_hash, score, source, language, trends_json = news
                trends = json.loads(trends_json)
                category = f"write_{language}"
                current_time_ts = time.time()

                if current_time_ts < bot_status["twitter_wait_until"].get(category, 0):
                    logging.debug(f"Escritura pausada para {language}. Omitiendo elemento de la cola (ID: {id}).")
                    continue

                if bot_status["daily_tweets_total"] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
                    logging.debug("L√≠mite diario de tweets alcanzado. Deteniendo procesamiento de la cola.")
                    break

                if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
                    logging.debug("L√≠mite mensual total de publicaciones alcanzado. Deteniendo procesamiento de la cola.")
                    break

                last_tweet_time = bot_status.get(f"last_tweet_time_{language}")
                if last_tweet_time and (current_time_ts - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
                    remaining = CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"] - (current_time_ts - last_tweet_time.timestamp())
                    logging.debug(f"Enfriamiento requerido para {language}: {remaining:.1f}s restantes.")
                    continue

                if await is_published_in_language(title, summary, url, source, language):
                    logging.info(f"Elemento de la cola ya publicado ({language}): {title[:50]}. Marcando para eliminar (ID: {id}).")
                    items_to_remove.append(id)
                    continue

                logging.info(f"Intentando publicar elemento de la cola (ID: {id}) en {language}: {title[:50]}...")

                try:
                    success = await post_tweet(session, title, summary, url, image_data_bytes, news_hash, score, source, language, trends)
                    if success:
                        logging.info(f"Elemento de la cola (ID: {id}) publicado exitosamente en {language}. Marcando para eliminar.")
                        items_to_remove.append(id)
                        await asyncio.sleep(CONFIG["INTERVALS"]["API_REQUEST_DELAY_SECONDS"])
                    else:
                        logging.warning(f"No se pudo publicar el elemento de la cola (ID: {id}) en {language}. Permanece en la cola.")
                except Exception as e:
                    logging.error(f"Error intentando publicar elemento de la cola (ID: {id}) en {language}: {e}", exc_info=True)
                    bot_status["errors"] += 1
                    await save_bot_state()

            if items_to_remove:
                logging.info(f"Eliminando {len(items_to_remove)} elementos de la cola de publicaci√≥n.")
                try:
                    await run_db_sync(
                        cursor.execute,
                        f"DELETE FROM cola_publicacion WHERE id IN ({','.join('?' * len(items_to_remove))})",
                        tuple(items_to_remove)
                    )
                    await run_db_sync(conn.commit)
                    await save_bot_state()
                except Exception as e:
                    logging.error(f"Error eliminando elementos de la cola: {e}", exc_info=True)
                    bot_status["errors"] += 1
                    await save_bot_state()

    except Exception as e:
        logging.error(f"Error general procesando la cola de publicaci√≥n: {e}", exc_info=True)
        bot_status["errors"] += 1
        await save_bot_state()

async def get_image_from_search(session: aiohttp.ClientSession, query: str) -> bytes | None:
    """
    Busca una imagen relacionada con una consulta usando la b√∫squeda de im√°genes de Google
    y descarga la primera imagen v√°lida encontrada.
    Mejorado manejo de errores y validaci√≥n.
    """
    logging.debug(f"Searching for image for query: '{query}'")
    try:
        # Use aiohttp for the HTTP request
        # Using a more specific image search URL if possible, or rely on tbm=isch
        search_url = f"https://www.google.com/search?tbm=isch&q={urllib.parse.quote(query)}"
        # Using a User-Agent to avoid being blocked
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}

        async with session.get(search_url, headers=headers, timeout=aiohttp.ClientTimeout(total=15)) as response:
            if response.status != 200:
                logging.warning(f"Error in image search for '{query}': Status code {response.status}")
                return None
            html_content = await response.text()

        soup = BeautifulSoup(html_content, "html.parser")
        # Google Images loads dynamically, extracting URLs can be tricky.
        # A more robust approach might involve using a dedicated image search API or a headless browser.
        # This simple approach looks for img tags with src or data-src.
        img_tags = soup.find_all("img", src=True)
        if not img_tags:
             img_tags = soup.find_all("img", {"data-src": True})

        image_urls = []
        for img_tag in img_tags:
            img_url = img_tag.get("src") or img_tag.get("data-src")
            # Filter out URLs that are likely not content images (e.g., icons, spacers)
            if img_url and img_url.startswith(("http://", "https://")) and not any(ext in img_url.lower() for ext in [".gif", ".svg", ".ico", "spacer.gif", "gstatic.com"]): # Added gstatic.com filter
                image_urls.append(img_url)

        logging.debug(f"Image URLs found in search for '{query}': {image_urls[:10]}...") # Log only the first few

        # Attempt to download and validate the first few found URLs
        for img_url in image_urls[:5]: # Limit the number of downloads from search
            logging.debug(f"Attempting to download and validate search image: {img_url}")
            # Use download_image which includes validation
            img_data = await download_image(session, img_url)
            if img_data:
                # Attempt to optimize the downloaded image
                # optimize_image now handles None input and returns None on failure
                optimized_data = await optimize_image(img_data)
                if optimized_data:
                    logging.info(f"Image obtained from search and optimized for '{query[:50]}'")
                    return optimized_data
                else:
                    logging.debug(f"Could not optimize the search image: {img_url}")
            else:
                 logging.debug(f"Could not download or validate the search image: {img_url}")

        logging.info(f"Could not obtain a valid image from search for '{query}'")
        return None
    except Exception as e:
        logging.error(f"Error searching for image for '{query[:50]}': {e}", exc_info=True)
        # Decide if this should increment the error counter
        # bot_status["errors"] += 1
        # save_bot_state_sync(cursor, conn)
        return None
    
async def publish_news(session: aiohttp.ClientSession, entry: dict, news_hash: str, score: float, language: str, trends: list[str]) -> bool:
    """
    Publica una noticia en Twitter/X, manejando im√°genes y tweets/reposts.
    
    Args:
        session: Sesi√≥n de aiohttp para solicitudes HTTP.
        entry: Diccionario con los datos de la noticia.
        news_hash: Hash √∫nico de la noticia.
        score: Puntuaci√≥n de relevancia.
        language: Idioma de publicaci√≥n ("en" o "es").
        trends: Lista de tendencias actuales.
    
    Returns:
        bool: True si se public√≥ exitosamente, False en caso contrario.
    """
    title = entry.get("title", "Untitled")
    url = entry.get("link")
    summary = entry.get("summary", "")
    source = entry.get("source", "Unknown Source")
    is_tweet = entry.get("is_tweet", False)
    
    logging.debug(f"Preparando publicaci√≥n para {language}: {title[:50]} (is_tweet: {is_tweet})")
    
    if is_tweet:
        try:
            result = await repost_tweet_from_url(url, summary, language)
            if result:
                logging.info(f"Repost exitoso en {language}: {title[:50]}")
            return result
        except tweepy.TooManyRequests as e:
            logging.warning(f"Error 429 al repostear en {language}: {title[:50]}: {e}")
            return False
        except Exception as e:
            logging.error(f"Error al repostear en {language}: {title[:50]}: {e}")
            bot_status["errors"] += 1
            await save_bot_state()
            return False
    
    image_data = None
    if entry.get("image_url"):
        logging.debug(f"Descargando imagen desde {entry['image_url']}")
        image_data = await download_image(session, entry["image_url"])
        image_data = await optimize_image(image_data) if image_data else None
    
    if not image_data and url and not url.startswith(("patchbot://", "discord://")):
        logging.debug(f"Buscando imagen en URL: {url}")
        img_url = await get_image_from_url(session, url)
        if img_url:
            image_data = await download_image(session, img_url)
            image_data = await optimize_image(image_data) if image_data else None
    
    if not image_data:
        search_query = clean_text(title)[:100]
        logging.debug(f"Buscando imagen en Unsplash para: {search_query}")
        image_data = await get_image_from_search(session, search_query)
    
    try:
        result = await post_tweet(session, title, summary, url, image_data, news_hash, score, source, language, trends)
        if result:
            logging.info(f"Publicaci√≥n exitosa en {language}: {title[:50]}")
        return result
    except tweepy.TooManyRequests as e:
        logging.warning(f"Error 429 al publicar en {language}: {title[:50]}: {e}")
        return False
    except tweepy.TweepyException as e:
        logging.error(f"Error de API al publicar en {language}: {title[:50]}: {e}")
        bot_status["errors"] += 1
        await save_bot_state()
        return False
    except Exception as e:
        logging.error(f"Error inesperado al publicar en {language}: {title[:50]}: {e}")
        bot_status["errors"] += 1
        await save_bot_state()
        return False

async def process_publication_queue():
    """
    Procesa las noticias encoladas en la tabla cola_publicacion y las publica si es posible.
    
    Returns:
        int: N√∫mero de noticias publicadas exitosamente.
    """
    global bot_status
    published_count = 0
    
    async with aiohttp.ClientSession() as session:
        cursor.execute("SELECT * FROM cola_publicacion")
        queued_items = await run_db_sync(cursor.fetchall)
        
        for item in queued_items:
            title = item["title"]
            summary = item["summary"]
            url = item["url"]
            news_hash = item["news_hash"]
            score = item["score"]
            source = item["source"]
            language = item["language"]
            trends = json.loads(item["trends"] or "[]")
            
            category = f"write_{language}"
            # Verificar restricciones
            if time.time() < bot_status["twitter_wait_until"].get(category, 0):
                logging.debug(f"No se puede procesar noticia encolada en {language}: En per√≠odo de espera")
                continue
            if bot_status["daily_tweets_total"] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
                logging.debug(f"No se puede procesar noticia encolada en {language}: L√≠mite diario alcanzado")
                break
            if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
                logging.debug(f"No se puede procesar noticia encolada en {language}: L√≠mite mensual alcanzado")
                break
            last_tweet_time = bot_status.get(f"last_tweet_time_{language}")
            if last_tweet_time and (time.time() - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
                logging.debug(f"No se puede procesar noticia encolada en {language}: En per√≠odo de enfriamiento")
                continue
            
            logging.info(f"Procesando noticia encolada en {language}: {title[:50]} (score: {score})")
            entry = {
                "title": title,
                "summary": summary,
                "link": url,
                "source": source,
                "is_tweet": False
            }
            
            if await publish_news(session, entry, news_hash, score, language, trends):
                published_count += 1
                bot_status[f"monthly_posts_{language}"] += 1
                bot_status["daily_tweets_total"] += 1
                bot_status[f"last_tweet_time_{language}"] = datetime.now()
                bot_status["recent_processed_news"] += 1
                logging.info(f"Noticia encolada publicada en {language}: {title[:50]}")
                
                # Eliminar de la cola
                await run_db_sync(
                    cursor.execute,
                    "DELETE FROM cola_publicacion WHERE news_hash = ? AND language = ?",
                    (news_hash, language)
                )
                await run_db_sync(conn.commit)
            else:
                logging.warning(f"Fallo al publicar noticia encolada en {language}: {title[:50]}")
        
        await save_bot_state()
        return published_count

async def process_single_news(session: aiohttp.ClientSession, entry: dict, news_hash: str, trends: list[str]) -> bool:
    """
    Procesa una noticia individual, la punt√∫a y decide si publicarla o encolarla.
    
    Args:
        session: Sesi√≥n de aiohttp para solicitudes HTTP.
        entry: Diccionario con los datos de la noticia.
        news_hash: Hash √∫nico de la noticia.
        trends: Lista de tendencias actuales.
    
    Returns:
        bool: True si la noticia se public√≥ en al menos un idioma, False si se encol√≥ o descart√≥.
    """
    global bot_status
    title = entry.get("title", "Untitled")
    bot_status["last_task"] = f"Processing single news/tweet: {title[:50]}"
    logging.debug(f"Procesando noticia: {title[:50]} (source: {entry.get('source', 'Unknown')})")
    
    score = await score_news(entry, trends)
    if score < (6 if entry.get("is_tweet", False) else 2):
        logging.info(f"{'Tweet' if entry.get('is_tweet', False) else 'News'} '{title[:50]}' descartado por baja puntuaci√≥n ({score:.1f})")
        return False
    
    published_status = {"en": False, "es": False}
    for language in published_status:
        category = f"write_{language}"
        # Verificar restricciones de publicaci√≥n
        if time.time() < bot_status["twitter_wait_until"].get(category, 0):
            logging.info(f"No se puede publicar en {language}: En per√≠odo de espera hasta {datetime.fromtimestamp(bot_status['twitter_wait_until'][category]).strftime('%H:%M:%S')}")
            continue
        if bot_status["daily_tweets_total"] >= CONFIG["API_LIMITS"]["TWEETS_PER_DAY"]:
            logging.info(f"No se puede publicar en {language}: L√≠mite diario alcanzado ({bot_status['daily_tweets_total']}/{CONFIG['API_LIMITS']['TWEETS_PER_DAY']})")
            continue
        if bot_status["monthly_posts_en"] + bot_status["monthly_posts_es"] >= CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]:
            logging.info(f"No se puede publicar en {language}: L√≠mite mensual alcanzado ({bot_status['monthly_posts_en'] + bot_status['monthly_posts_es']}/{CONFIG['API_LIMITS']['MONTHLY_POSTS_TOTAL']})")
            continue
        last_tweet_time = bot_status.get(f"last_tweet_time_{language}")
        if last_tweet_time and (time.time() - last_tweet_time.timestamp()) < CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"]:
            remaining = CONFIG["INTERVALS"]["TWEET_COOLDOWN_SECONDS"] - (time.time() - last_tweet_time.timestamp())
            logging.info(f"No se puede publicar en {language}: En per√≠odo de enfriamiento ({remaining/60:.1f} minutos restantes)")
            continue
        if await is_published_in_language(title, entry.get("summary", ""), entry.get("link"), entry.get("source", "Unknown"), language):
            logging.info(f"Noticia ya publicada en {language}: {title[:50]}")
            published_status[language] = True
            continue
        
        logging.info(f"Intentando publicar noticia en {language}: {title[:50]} (score: {score:.1f})")
        try:
            if await publish_news(session, entry, news_hash, score, language, trends):
                published_status[language] = True
                bot_status[f"monthly_posts_{language}"] += 1
                bot_status["daily_tweets_total"] += 1
                bot_status[f"last_tweet_time_{language}"] = datetime.now()
                logging.info(f"Noticia publicada exitosamente en {language}: {title[:50]}")
            else:
                logging.warning(f"Fallo al publicar noticia en {language}: {title[:50]}. Intentando encolar")
        except Exception as e:
            logging.error(f"Error inesperado publicando en {language}: {title[:50]}: {e}", exc_info=True)
            bot_status["errors"] += 1
    
    if any(published_status.values()):
        source_usage[entry.get("source", "Unknown")] += 1
        bot_status["recent_processed_news"] += 1
        await save_bot_state()
        return True
    
    # Encolar si no se public√≥ en alg√∫n idioma
    for language in published_status:
        if not published_status[language] and not await is_published_in_language(title, entry.get("summary", ""), entry.get("link"), entry.get("source", "Unknown"), language):
            cursor.execute("SELECT 1 FROM cola_publicacion WHERE news_hash = ? AND language = ?", (news_hash, language))
            if not await run_db_sync(cursor.fetchone):
                logging.info(f"Encolando noticia para {language}: {title[:50]}")
                trends_json = json.dumps(trends)
                try:
                    await run_db_sync(
                        cursor.execute,
                        '''INSERT INTO cola_publicacion (title, summary, url, image_data, news_hash, score, source, language, trends)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                        (title, entry.get("summary", ""), entry.get("link"), None, news_hash, score, entry.get("source", "Unknown"), language, trends_json)
                    )
                    await run_db_sync(conn.commit)
                    logging.debug(f"Noticia encolada correctamente para {language}: {title[:50]}")
                except Exception as e:
                    logging.error(f"Error encolando noticia para {language}: {title[:50]}: {e}")
                    bot_status["errors"] += 1
    
    await save_bot_state()
    return False

async def process_rss_feeds(session: aiohttp.ClientSession, trends: list[str]) -> list[dict]:
    """
    Procesa los feeds RSS configurados y retorna una lista de noticias.
    Evita duplicados usando la funci√≥n is_duplicate.
    Ejecuta feedparser.parse en un hilo separado.
    Modificada para usar asyncio.to_thread y await is_duplicate.
    """
    global bot_status
    bot_status["last_task"] = "Procesando feeds RSS"
    logging.info("Iniciando procesamiento de feeds RSS...")

    rss_news = []
    for feed_url in RSS_FEEDS:
        try:
            # feedparser.parse es s√≠ncrono, ejecutar en un hilo separado
            feed = await asyncio.to_thread(feedparser.parse, feed_url)

            if feed.entries:
                logging.debug(f"Procesando {len(feed.entries)} entradas del feed RSS: {feed_url}")
                for entry in feed.entries:
                    title = clean_text(entry.get("title", "Sin t√≠tulo"))
                    link = entry.get("link")
                    summary = clean_text(entry.get("summary", entry.get("description", "")))
                    source = feed.feed.get("title", feed_url) # Usar el t√≠tulo del feed como fuente si est√° disponible
                    # Asegurarse de que 'link' no sea None antes de usarlo
                    if not link:
                         logging.warning(f"Entrada RSS sin enlace en {feed_url}: {title[:50]}")
                         continue # Omitir entrada sin enlace

                    # Usar el enlace en el hash para asegurar unicidad por entrada del feed
                    news_hash = hashlib.sha256((title + summary + link + source).encode()).hexdigest()

                    # VERIFICAR DUPLICADOS ANTES DE A√ëADIR A LA LISTA
                    # is_duplicate ahora se ejecuta en un hilo separado
                    if not await is_duplicate(title, summary, link, source):
                        date_tuple = entry.get("published_parsed") or entry.get("updated_parsed")
                        # Asegurarse de que date_tuple sea v√°lido antes de desempaquetar
                        date = datetime(*date_tuple[:6]) if date_tuple and len(date_tuple) >= 6 else datetime.now() # Usar fecha actual como fallback

                        rss_news.append({
                            "title": title,
                            "link": link,
                            "summary": summary,
                            "source": source,
                            "date": date,
                            "hash": news_hash,
                            "is_tweet": False # Marcar como no-tweet
                        })
                        logging.debug(f"Noticia RSS a√±adida: {title[:50]}...")
                    else:
                        logging.debug(f"Noticia RSS duplicada detectada y omitida: {title[:50]}...")

            else:
                logging.info(f"No se encontraron entradas en el feed RSS: {feed_url}")

        except Exception as e:
            # Loguear el error procesando un feed espec√≠fico, pero continuar con los dem√°s
            logging.warning(f"Error procesando RSS {feed_url}: {e}")
            bot_status["errors"] += 1
            await save_bot_state() # Guardar estado si hay un error
            continue # Continuar con el siguiente feed

    logging.info(f"Procesamiento de feeds RSS completado. {len(rss_news)} noticias nuevas obtenidas.")
    return rss_news

async def process_discord_news(channel_id: int, trends: list[str]) -> list[dict]:
    global bot_status
    bot_status["last_task"] = f"Procesando noticias Discord (canal {channel_id})"
    processed_news = []
    try:
        while discord_news[channel_id]:
            entry = discord_news[channel_id].popleft()
            if not await is_duplicate(entry["title"], entry["summary"], entry["link"], entry["source"]):
                processed_news.append(entry)
        logging.info(f"Noticias Discord procesadas (canal {channel_id}): {len(processed_news)}")
    except Exception as e:
        logging.error(f"Error procesando Discord (canal {channel_id}): {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
    return processed_news

async def process_news():
    global bot_status
    bot_status["tasks_running"] += 1
    print_section_header("Procesando Noticias")
    logging.info("Iniciando ciclo de procesamiento de noticias...")

    try:
        current_time = datetime.now()
        current_time_ts = time.time()

        # Resetear contadores diarios
        if current_time.day != bot_status["last_reset"].day:
            bot_status["daily_tweets_total"] = 0
            bot_status["last_reset"] = current_time.replace(hour=0, minute=0, second=0, microsecond=0)
            bot_status["recent_tweets_en"] = []
            bot_status["recent_tweets_es"] = []
            logging.info("Contadores diarios de tweets reseteados")
            save_bot_state_sync(cursor, conn)

        # Resetear contadores mensuales
        if current_time.month != bot_status["monthly_reset"].month:
            bot_status["monthly_posts_en"] = 0
            bot_status["monthly_posts_es"] = 0
            bot_status["x_api_reads_remaining"] = CONFIG["API_LIMITS"]["MONTHLY_READS"]
            bot_status["x_api_writes_remaining"] = CONFIG["API_LIMITS"]["MONTHLY_POSTS_TOTAL"]
            bot_status["monthly_reset"] = current_time.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
            logging.info("Contadores mensuales reseteados")
            save_bot_state_sync(cursor, conn)

        trends = await get_trending_keywords()
        all_news = []

        async with aiohttp.ClientSession() as session:
            print_section_header("Procesando Fuentes")
            # Procesar Discord
            async with discord_processing_lock:
                for channel_id in [DISCORD_CHANNEL_1, DISCORD_CHANNEL_2]:
                    all_news.extend(await process_discord_news(channel_id, trends))

            # Procesar RSS
            all_news.extend(await process_rss_feeds(session, trends))

            # Procesar Twitter si no est√° pausado y hay lecturas disponibles
            twitter_read_paused = current_time_ts < bot_status["twitter_wait_until"].get("read", 0)
            if not twitter_read_paused and bot_status["x_api_reads_remaining"] > 0 and await check_api_rate_limit("read"):
                try:
                    twitter_news = await update_twitter_feeds()
                    all_news.extend(twitter_news)
                except Exception as e:
                    logging.warning(f"No se pudieron obtener tweets de Twitter/X: {e}. Continuando con RSS y Discord.")
            else:
                logging.info("Lecturas de Twitter/X pausadas o l√≠mite agotado. Usando solo RSS y Discord.")

            if not all_news:
                print("‚ÑπÔ∏è No hay noticias nuevas para procesar en este ciclo.")
                logging.info("No hay noticias nuevas para procesar en este ciclo.")
                return

            # Puntuar y ordenar noticias
            scored_items = []
            for entry in all_news:
                score = await score_news(entry, trends)
                if score > 0:
                    scored_items.append((entry, score))

            scored_items.sort(key=lambda x: (
                x[0].get("is_discord", False),
                not x[0].get("is_tweet", False),
                x[1]
            ), reverse=True)

            # Procesar noticias
            processed_count = 0
            for entry, score in scored_items:
                if await process_single_news(session, entry, entry["hash"], trends):
                    processed_count += 1
                    print(f"‚úÖ Noticia procesada: {entry['title'][:50]} (Puntuaci√≥n: {score:.1f})")
                    await asyncio.sleep(CONFIG["INTERVALS"]["API_REQUEST_DELAY_SECONDS"])

            bot_status["recent_processed_news"] = processed_count
            logging.info(f"Ciclo de procesamiento de noticias completado. Elementos procesados para publicaci√≥n: {processed_count}")
            save_bot_state_sync(cursor, conn)

    except Exception as e:
        logging.error(f"‚ùå Error en process_news: {e}", exc_info=True)
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)
    finally:
        bot_status["tasks_running"] -= 1

async def discord_news_processor():
    while True:
        try:
            await asyncio.sleep(CONFIG["INTERVALS"]["DISCORD_PROCESSING_SECONDS"])
            bot_status["last_task"] = "Procesando cola de Discord"
            await process_news()
        except Exception as e:
            logging.error(f"Error en discord_news_processor: {e}", exc_info=True)
            bot_status["errors"] += 1
            save_bot_state_sync(cursor, conn)
            await asyncio.sleep(5)
            

async def heartbeat():
    while True:
        try:
            # Calcular tiempo de actividad
            uptime = datetime.now() - bot_status["uptime"]
            uptime_str = f"{int(uptime.total_seconds() // 86400)}d {int((uptime.total_seconds() % 86400) // 3600)}h {int((uptime.total_seconds() % 3600) // 60)}m"

            # Imprimir encabezado
            print_section_header("Estado del Sistema", color="cyan")  # Cambi√© a cian para un look m√°s vibrante

            # Informaci√≥n general
            print(f"\033[1;36müõ†Ô∏è  Sistema Activo: \033[0m{uptime_str}")
            print(f"\033[1;36müîÑ Tarea Actual: \033[0m{bot_status.get('last_task', 'N/A')}")

            # Estado de conexiones
            print(f"\033[1;36müîó Conexiones:\033[0m")
            connections = [
                ("Twitter EN", bot_status["twitter_connected_en"]),
                ("Twitter ES", bot_status["twitter_connected_es"]),
                ("SQLite", bot_status["sqlite_connected"]),
                ("Discord", bot_status["discord_connected"])
            ]
            for name, status in connections:
                status_icon = "\033[32m‚úÖ\033[0m" if status else "\033[31m‚ùå\033[0m"
                print(f"  {name:<12}: {status_icon}")

            # Estad√≠sticas de tweets
            print(f"\033[1;36müì¢ Tweets Publicados:\033[0m")
            print(f"  EN: {bot_status['posted_tweets_en']:<4} | ES: {bot_status['posted_tweets_es']}")
            print_progress_bar(
                bot_status["daily_tweets_total"],
                CONFIG["API_LIMITS"]["TWEETS_PER_DAY"],
                "Uso Diario",
                critical_threshold=80
            )

            # Uso de API
            print(f"\033[1;36müìä API X:\033[0m")
            api_reads_percent = (bot_status["x_api_reads_remaining"] / CONFIG["API_LIMITS"]["MONTHLY_READS"]) * 100
            print_progress_bar(
                bot_status["x_api_reads_remaining"],
                CONFIG["API_LIMITS"]["MONTHLY_READS"],
                "Lecturas Restantes",
                critical_threshold=20
            )

            # Errores
            error_color = "31" if bot_status["errors"] > 0 else "32"  # Rojo si hay errores, verde si no
            print(f"\033[1;36m‚ö†Ô∏è  Errores: \033[{error_color}m{bot_status['errors']}\033[0m")

            # Tiempos de espera
            print(f"\033[1;36m‚è≥ Restricciones:\033[0m")
            current_time = time.time()
            any_restrictions = False
            for key, wait_until in bot_status["twitter_wait_until"].items():
                if current_time < wait_until:
                    wait_time = wait_until - current_time
                    mode = {"read": "Lectura", "write_en": "Escritura EN", "write_es": "Escritura ES"}.get(key, key)
                    print(f"  {mode:<12}: Pausada por {wait_time:.0f}s")
                    any_restrictions = True
            if not any_restrictions:
                print("  Ninguna activa")

            # Separador final
            print(f"\033[1;34m{'‚îÄ' * 60}\033[0m\n")

            # Esperar hasta el pr√≥ximo heartbeat
            await asyncio.sleep(CONFIG["INTERVALS"]["HEARTBEAT_SECONDS"])
        except Exception as e:
            logging.error(f"\033[31m‚ùå Error en heartbeat: {e}\033[0m", exc_info=True)
            bot_status["errors"] += 1
            save_bot_state_sync(cursor, conn)
            await asyncio.sleep(5)

async def run_discord():
    try:
        await discord_bot.start(DISCORD_TOKEN)
    except Exception as e:
        logging.error(f"Error iniciando Discord: {e}", exc_info=True)
        bot_status["discord_connected"] = False
        bot_status["errors"] += 1
        save_bot_state_sync(cursor, conn)

async def main():
    global bot_status
    print_section_header("Iniciando Bot")
    logging.info("Iniciando bot...")
    bot_status["last_task"] = "Iniciando bot"
    
    # Limpiar im√°genes temporales antiguas
    temp_dir = CONFIG["PATHS"]["TEMP_IMAGE_DIR"]
    for filename in os.listdir(temp_dir):
        file_path = os.path.join(temp_dir, filename)
        try:
            if os.path.isfile(file_path):
                os.unlink(file_path)
                logging.debug(f"Eliminado archivo temporal antiguo: {file_path}")
        except Exception as e:
            logging.warning(f"Error eliminando {file_path}: {e}")
    
    scheduler = AsyncIOScheduler()
    scheduler.add_job(process_news, IntervalTrigger(minutes=CONFIG["INTERVALS"]["PROCESS_NEWS_MINUTES"]), misfire_grace_time=300)
    scheduler.add_job(update_twitter_feeds, IntervalTrigger(minutes=CONFIG["INTERVALS"]["UPDATE_TWITTER_FEEDS_MINUTES"]), misfire_grace_time=300)
    scheduler.add_job(process_queue, IntervalTrigger(minutes=CONFIG["INTERVALS"]["QUEUE_PROCESSING_MINUTES"]), misfire_grace_time=300)
    scheduler.add_job(review_queue, IntervalTrigger(minutes=CONFIG["INTERVALS"]["QUEUE_PROCESSING_MINUTES"]), misfire_grace_time=300)
    scheduler.add_job(fetch_and_post_weather,CronTrigger(hour=22, minute=0, second=0, timezone="UTC"),misfire_grace_time=600)
    scheduler.add_job(fetch_and_post_carbon_intensity,CronTrigger(hour=10, minute=0, second=0, timezone="UTC"),misfire_grace_time=600)

    loop = asyncio.get_event_loop()
    loop.create_task(run_discord())
    loop.create_task(heartbeat())
    loop.create_task(discord_news_processor())
    scheduler.start()
    
    logging.info("Esperando 10 segundos para estabilizar conexiones iniciales...")
    await asyncio.sleep(10)
    
    while True:
        try:
            await asyncio.sleep(3600)
            bot_status["last_task"] = "Esperando en bucle principal"
            save_bot_state_sync(cursor, conn)
        except Exception as e:
            logging.error(f"Error en bucle principal: {e}")
            bot_status["errors"] += 1
            save_bot_state_sync(cursor, conn)
            await asyncio.sleep(5)

if __name__ == "__main__":
    asyncio.run(main())
